text,category
"- Decision: Workspace ready, boards & sites created. - Next: Build embedding notebook tomorrow.",
"This repository contains the core components of **RepoSage**, a hybrid AI-powered study assistant that routes student queries to the right documentation sections and delivers precise answers using a blend of classification, retrieval, Bayesian scoring, and generative fallback.",
---,
1. **Corpus Preparation**,
"* Recursively scan **all** project markdowns (`reposage/README.md`, other `*.md`). * Split each file into paragraphs and export to `data/classification.csv` with an empty `category` column for manual labeling.",
2. **H2O Classifier (Routing Layer)**,
"* After labeling paragraphs (e.g. `installation`, `usage`, `configuration`, `api_reference`, `troubleshooting`), train an H2O AutoML model to predict the category of any incoming question. * This routing step ensures each query is answered by the most relevant section of your docs.",
3. **DeepSeek + Bayesian + Transformer Q\&A**,
* Build a FAISS index over all paragraphs (using sentence-transformer embeddings). * On query:,
"1. **Route** via the H2O classifier to select a subset of paragraphs. 2. **Retrieve** top‚Äëk relevant passages with FAISS. 3. **Rescore** with a Bayesian posterior to boost high-confidence results. 4. **Fallback**: if the posterior confidence is below threshold, generate an answer using a compact Flan‚ÄëT5 model.",
4. **Streamlit UI / Hugging Face Space**,
"* The entire stack is orchestrated in `app.py` (or Gradio) as a Streamlit application. * Deploy on Hugging Face Spaces for instant access. * Users type a question, see ‚ÄúRouted to category:¬†X,‚Äù and receive a clear, sourced answer.",
---,
```bash python scripts/prepare_corpus.py \ --input-dir reposage/ \ --output data/classification.csv ```,
Manually open `data/classification.csv` and assign each paragraph a category label.,
```bash python scripts/train_classifier.py \ --data data/classification.csv \ --model-out models/routing_model.zip ```,
```bash python scripts/build_index.py \ --data data/classification.csv \ --index-out models/faiss_index.bin ```,
"In `app.py`, configure paths to:",
* `models/routing_model.zip` * `models/faiss_index.bin` * Embedding & Flan‚ÄëT5 models via your settings or environment variables.,
Run locally:,
```bash streamlit run app.py ```,
---,
``` ‚îú‚îÄ data/ ‚îÇ  ‚îî‚îÄ classification.csv       # Paragraphs + category column ‚îú‚îÄ models/ ‚îÇ  ‚îú‚îÄ routing_model.zip       # Trained H2O classifier ‚îÇ  ‚îî‚îÄ faiss_index.bin         # FAISS index of embeddings ‚îú‚îÄ scripts/ ‚îÇ  ‚îú‚îÄ prepare_corpus.py       # Splits & exports paragraphs ‚îÇ  ‚îú‚îÄ train_classifier.py     # H2O AutoML training ‚îÇ  ‚îî‚îÄ build_index.py          # Embedding + FAISS index builder ‚îú‚îÄ app.py                     # Streamlit (or Gradio) Q&A app ‚îú‚îÄ requirements.txt           # Python dependencies ‚îî‚îÄ README.md                  # (This file) ```,
---,
"1. Label paragraphs in `data/classification.csv`. 2. Train or fine-tune the H2O classifier (`scripts/train_classifier.py`). 3. Integrate the routing model into your Streamlit UI so that every query is first classified, then answered from the right subset.",
---,
"1. Fork and create a feature branch. 2. Add or update scripts/tests as needed. 3. Ensure style checks (`flake8`, `black`) pass. 4. Submit a pull request detailing your enhancements.",
---,
Licensed under MIT. See [LICENSE](LICENSE) for details.,
"--- title: RepoSage Chatbot emoji: ü§ñ colorFrom: indigo colorTo: blue sdk: streamlit sdk_version: ""1.46.0"" app_file: app.py pinned: true ---",
"An MVP AI chatbot built in AML-3304 using Bayesian embeddings, a simple transformer block, and DeepSeek-R1 integration ‚Äî all wired up with a GitHub-driven CI/CD pipeline to Hugging Face Spaces.",
---,
Try it out live: üëâ https://huggingface.co/spaces/rohitrajpoot/reposage-chatbot,
---,
**What it is:** - A command-line & web demo (via Streamlit) that shows: 1. **Embedding Q&A**: nearest‚Äêneighbor lookup in a trained token embedding (`assist/chat.py`) 2. **Bayesian Q&A**: frequency‚Äêbased ‚Äúco-occurrence‚Äù embedding lookup (`assist/bayes_chat.py`) 3. **Transformer Demo**: single‚Äêblock transformer next‚Äêtoken prediction (`assist/transformer_demo.py`) 4. **DeepSeek-R1**: calls to a 1.3B-parameter model for generative Q&A (wrapped to skip gracefully in Colab),
**Why it matters:** - Demonstrates core GPT ‚Äúatoms‚Äù (token ‚Üí embedding ‚Üí attention ‚Üí generation) - Shows an end-to-end MLOps flow: local dev ‚Üí GitHub Actions ‚Üí Docker ‚Üí Hugging Face Spaces,
---,
```bash git clone https://github.com/rohitrajpoot/reposage.git cd reposage,
python3 -m venv .venv source .venv/bin/activate,
pip install --upgrade pip setuptools wheel pip install -r requirements.txt,
"python -m assist.main chat ""hello world""",
streamlit run app.py,
reposage index,
"reposage query ""What is DeepSeek?"" --threshold 0.3",
"curl -X POST https://<your-space>.hf.space/query \ -d '{""question"":""What is DeepSeek?""}'",
"RepoSage is an AI-centric study assistant that uses semantic search (DeepSeek), a Bayesian Q&A layer, and a transformer fallback to answer student queries with traceability and transparency.",
"- **DeepSeek Index** of lecture slides, code, and readings - **Bayesian Posterior Scoring** for precise answer selection - **Transformer Fallback** via HF text2text when confidence is low - **CLI & REST API** for local and web deployment - **CI/CD** via GitHub Actions ‚Üí Hugging Face Spaces",
"- Python 3.8+ - `faiss`, `sentence-transformers`, `transformers`, `openai-whisper`",
```bash git clone https://github.com/RohitRajpoot/reposage.git cd reposage pip install -r requirements.txt,
reposage index,
"reposage query ""What is DeepSeek?"" --threshold 0.3",
```markdown,
"- Guide the transformer fallback to produce concise, context-aware answers. - Maintain consistency with our domain (AML3304 materials).",
"1. **System prompt**: > You are RepoSage, an AI tutor. Use the indexed materials to answer student questions.",
2. **User prompt template**:,
"| Team Member | Role(s)                 | Responsibilities                             | |-------------|-------------------------|----------------------------------------------| | Rohit       | CEO / Product Lead,     | ‚Ä¢ Define product vision & roadmap<br>‚Ä¢ Manage scope & stakeholder communication<br>‚Ä¢ Write technical documentation | | Kishan      | CTO / DevOps Lead,      | ‚Ä¢ Develop DeepSeek index & CLI scaffold<br>‚Ä¢ Set up CI/CD pipelines<br>‚Ä¢ Manage Hugging Face Spaces deployments   | | Maitri      | Data Scientist / AI Lead| ‚Ä¢ Implement Bayesian Q&A layer<br>‚Ä¢ Design & tune priors<br>‚Ä¢",
Oversee transformer fallback prompts          |,
---,
{{ card_data }} ---,
"This is a [sentence-transformers](https://www.SBERT.net) model{% if base_model %} finetuned from [{{ base_model }}](https://huggingface.co/{{ base_model }}){% else %} trained{% endif %}{% if train_datasets | selectattr(""name"") | list %} on {% if train_datasets | selectattr(""name"") | map(attribute=""name"") | join("", "") | length > 200 %}{{ train_datasets | length }}{% else %}the {% for dataset in (train_datasets | selectattr(""name"")) %}{% if dataset.id %}[{{ dataset.name if dataset.name else dataset.id }}](https://huggingface.co/datasets/{{ dataset.id }}){% else %}{{ dataset.name }}{% endif %}{%",
"if not loop.last %}{% if loop.index == (train_datasets | selectattr(""name"") | list | length - 1) %} and {% else %}, {% endif %}{% endif %}{% endfor %}{% endif %} dataset{{""s"" if train_datasets | selectattr(""name"") | list | length > 1 else """"}}{% endif %}. It maps sentences & paragraphs to a {{ output_dimensionality }}-dimensional dense vector space and can be used for {{ task_name }}.",
- **Model Type:** Sentence Transformer {% if base_model -%} {%- if base_model_revision -%} - **Base model:** [{{ base_model }}](https://huggingface.co/{{ base_model }}) <!-- at revision {{ base_model_revision }} --> {%- else -%} - **Base model:** [{{ base_model }}](https://huggingface.co/{{ base_model }}) {%- endif -%} {%- else -%} <!-- - **Base model:** [Unknown](https://huggingface.co/unknown) --> {%- endif %} - **Maximum Sequence Length:** {{ model_max_length }} tokens - **Output Dimensionality:** {{ output_dimensionality }} dimensions - **Similarity Function:** {{ similarity_fn_name }} {%,
"if train_datasets | selectattr(""name"") | list -%} - **Training Dataset{{""s"" if train_datasets | selectattr(""name"") | list | length > 1 else """"}}:** {%- for dataset in (train_datasets | selectattr(""name"")) %} {%- if dataset.id %} - [{{ dataset.name if dataset.name else dataset.id }}](https://huggingface.co/datasets/{{ dataset.id }}) {%- else %} - {{ dataset.name }} {%- endif %} {%- endfor %} {%- else -%} <!-- - **Training Dataset:** Unknown --> {%- endif %} {% if language -%} - **Language{{""s"" if language is not string and language | length > 1 else """"}}:** {%- if language is string %} {{",
"language }} {%- else %} {% for lang in language -%} {{ lang }}{{ "", "" if not loop.last else """" }} {%- endfor %} {%- endif %} {%- else -%} <!-- - **Language:** Unknown --> {%- endif %} {% if license -%} - **License:** {{ license }} {%- else -%} <!-- - **License:** Unknown --> {%- endif %}",
- **Documentation:** [Sentence Transformers Documentation](https://sbert.net) - **Repository:** [Sentence Transformers on GitHub](https://github.com/UKPLab/sentence-transformers) - **Hugging Face:** [Sentence Transformers on Hugging Face](https://huggingface.co/models?library=sentence-transformers),
``` {{ model_string }} ```,
First install the Sentence Transformers library:,
```bash pip install -U sentence-transformers ```,
Then you can load this model and run inference. ```python from sentence_transformers import SentenceTransformer,
"model = SentenceTransformer(""{{ model_id | default('sentence_transformers_model_id', true) }}"")",
"sentences = [ {%- for text in (predict_example or [""The weather is lovely today."", ""It's so sunny outside!"", ""He drove to the stadium.""]) %} {{ ""%r"" | format(text) }}, {%- endfor %} ] embeddings = model.encode(sentences) print(embeddings.shape)",
"similarities = model.similarity(embeddings, embeddings) print(similarities.shape)",
```,
<!--,
<details><summary>Click to see the direct usage in Transformers</summary>,
</details> -->,
<!--,
You can finetune this model on your own dataset.,
<details><summary>Click to expand</summary>,
</details> -->,
<!--,
*List how the model may foreseeably be misused and address what users ought not to do with the model.* --> {% if eval_metrics %},
"{% for metrics in eval_metrics %} #### {{ metrics.description }} {% if metrics.dataset_name %} * Dataset{% if metrics.dataset_name is not string and metrics.dataset_name | length > 1 %}s{% endif %}: {% if metrics.dataset_name is string -%} `{{ metrics.dataset_name }}` {%- else -%} {%- for name in metrics.dataset_name -%} `{{ name }}` {%- if not loop.last -%} {%- if loop.index == metrics.dataset_name | length - 1 %} and {% else -%}, {% endif -%} {%- endif -%} {%- endfor -%} {%- endif -%} {%- endif %} * Evaluated with {% if metrics.class_name.startswith(""sentence_transformers."") %}[<code>{{",
"metrics.class_name.split(""."")[-1] }}</code>](https://sbert.net/docs/package_reference/sentence_transformer/evaluation.html#sentence_transformers.evaluation.{{ metrics.class_name.split(""."")[-1] }}){% else %}<code>{{ metrics.class_name }}</code>{% endif %}{% if metrics.config_code %} with these parameters: {{ metrics.config_code }}{% endif %}",
{{ metrics.table }} {%- endfor %}{% endif %} <!--,
*What are the known or foreseeable issues stemming from this model? You could also flag here known failure cases or weaknesses of the model.* -->,
<!--,
"*What are recommendations with respect to the foreseeable issues? For example, filtering explicit content.* -->",
"{% for dataset_type, dataset_list in [(""training"", train_datasets), (""evaluation"", eval_datasets)] %}{% if dataset_list %}",
{% for dataset in dataset_list %}{% if dataset_list | length > 3 %}<details><summary>{{ dataset['name'] or 'Unnamed Dataset' }}</summary> {% endif %} #### {{ dataset['name'] or 'Unnamed Dataset' }} {% if dataset['name'] %} * Dataset: {% if 'id' in dataset %}[{{ dataset['name'] }}](https://huggingface.co/datasets/{{ dataset['id'] }}){% else %}{{ dataset['name'] }}{% endif %} {%- if 'revision' in dataset and 'id' in dataset %} at [{{ dataset['revision'][:7] }}](https://huggingface.co/datasets/{{ dataset['id'] }}/tree/{{ dataset['revision'] }}){% endif %}{% endif %} {% if dataset['size'] %}*,
"Size: {{ ""{:,}"".format(dataset['size']) }} {{ dataset_type }} samples {% endif %}* Columns: {% if dataset['columns'] | length == 1 %}{{ dataset['columns'][0] }}{% elif dataset['columns'] | length == 2 %}{{ dataset['columns'][0] }} and {{ dataset['columns'][1] }}{% else %}{{ dataset['columns'][:-1] | join(', ') }}, and {{ dataset['columns'][-1] }}{% endif %} {% if dataset['stats_table'] %}* Approximate statistics based on the first {{ [dataset['size'], 1000] | min }} samples: {{ dataset['stats_table'] }}{% endif %}{% if dataset['examples_table'] %}* Samples: {{ dataset['examples_table'] }}{%",
"endif %}* Loss: {% if dataset[""loss""][""fullname""].startswith(""sentence_transformers."") %}[<code>{{ dataset[""loss""][""fullname""].split(""."")[-1] }}</code>](https://sbert.net/docs/package_reference/sentence_transformer/losses.html#{{ dataset[""loss""][""fullname""].split(""."")[-1].lower() }}){% else %}<code>{{ dataset[""loss""][""fullname""] }}</code>{% endif %}{% if ""config_code"" in dataset[""loss""] %} with these parameters: {{ dataset[""loss""][""config_code""] }}{% endif %} {% if dataset_list | length > 3 %}</details> {% endif %}{% endfor %}{% endif %}{% endfor -%}",
{% if all_hyperparameters %},
{% if non_default_hyperparameters -%} #### Non-Default Hyperparameters,
"{% for name, value in non_default_hyperparameters.items() %}- `{{ name }}`: {{ value }} {% endfor %}{%- endif %} #### All Hyperparameters <details><summary>Click to expand</summary>",
"{% for name, value in all_hyperparameters.items() %}- `{{ name }}`: {{ value }} {% endfor %} </details> {% endif %}",
{%- if eval_lines %},
{% if hide_eval_lines %}<details><summary>Click to expand</summary>,
{% endif -%} {{ eval_lines }}{% if explain_bold_in_eval %} * The bold row denotes the saved checkpoint.{% endif %} {%- if hide_eval_lines %} </details>{% endif %} {% endif %},
{%- if co2_eq_emissions %},
"Carbon emissions were measured using [CodeCarbon](https://github.com/mlco2/codecarbon). - **Energy Consumed**: {{ ""%.3f""|format(co2_eq_emissions[""energy_consumed""]) }} kWh - **Carbon Emitted**: {{ ""%.3f""|format(co2_eq_emissions[""emissions""] / 1000) }} kg of CO2 - **Hours Used**: {{ co2_eq_emissions[""hours_used""] }} hours",
"- **On Cloud**: {{ ""Yes"" if co2_eq_emissions[""on_cloud""] else ""No"" }} - **GPU Model**: {{ co2_eq_emissions[""hardware_used""] or ""No GPU used"" }} - **CPU Model**: {{ co2_eq_emissions[""cpu_model""] }} - **RAM Size**: {{ ""%.2f""|format(co2_eq_emissions[""ram_total_size""]) }} GB {% endif %}",
"- Python: {{ version[""python""] }} - Sentence Transformers: {{ version[""sentence_transformers""] }} - Transformers: {{ version[""transformers""] }} - PyTorch: {{ version[""torch""] }} - Accelerate: {{ version[""accelerate""] }} - Datasets: {{ version[""datasets""] }} - Tokenizers: {{ version[""tokenizers""] }}",
"{% for loss_name, citation in citations.items() %} #### {{ loss_name }} ```bibtex {{ citation | trim }} ``` {% endfor %} <!--",
*Clearly define terms in order to be accessible across audiences.* -->,
<!--,
"*Lists the people who create the model card, providing recognition and accountability for the detailed work that goes into its construction.* -->",
<!--,
"*Provides a way for people who have updates to the Model Card, suggestions, or questions, to contact the Model Card authors.* -->",
---,
{{ card_data }} ---,
"This is a [Cross Encoder](https://www.sbert.net/docs/cross_encoder/usage/usage.html) model{% if base_model %} finetuned from [{{ base_model }}](https://huggingface.co/{{ base_model }}){% else %} trained{% endif %}{% if train_datasets | selectattr(""name"") | list %} on {% if train_datasets | selectattr(""name"") | map(attribute=""name"") | join("", "") | length > 200 %}{{ train_datasets | length }}{% else %}the {% for dataset in (train_datasets | selectattr(""name"")) %}{% if dataset.id %}[{{ dataset.name if dataset.name else dataset.id }}](https://huggingface.co/datasets/{{ dataset.id }}){% else %}{{",
"dataset.name }}{% endif %}{% if not loop.last %}{% if loop.index == (train_datasets | selectattr(""name"") | list | length - 1) %} and {% else %}, {% endif %}{% endif %}{% endfor %}{% endif %} dataset{{""s"" if train_datasets | selectattr(""name"") | list | length > 1 else """"}}{% endif %} using the [sentence-transformers](https://www.SBERT.net) library. It computes scores for pairs of texts, which can be used for {{ task_name }}.",
"- **Model Type:** Cross Encoder {% if base_model -%} {%- if base_model_revision -%} - **Base model:** [{{ base_model }}](https://huggingface.co/{{ base_model }}) <!-- at revision {{ base_model_revision }} --> {%- else -%} - **Base model:** [{{ base_model }}](https://huggingface.co/{{ base_model }}) {%- endif -%} {%- else -%} <!-- - **Base model:** [Unknown](https://huggingface.co/unknown) --> {%- endif %} - **Maximum Sequence Length:** {{ model_max_length }} tokens - **Number of Output Labels:** {{ model_num_labels }} label{{ ""s"" if model_num_labels > 1 else """" }} {% if train_datasets |",
"selectattr(""name"") | list -%} - **Training Dataset{{""s"" if train_datasets | selectattr(""name"") | list | length > 1 else """"}}:** {%- for dataset in (train_datasets | selectattr(""name"")) %} {%- if dataset.id %} - [{{ dataset.name if dataset.name else dataset.id }}](https://huggingface.co/datasets/{{ dataset.id }}) {%- else %} - {{ dataset.name }} {%- endif %} {%- endfor %} {%- else -%} <!-- - **Training Dataset:** Unknown --> {%- endif %} {% if language -%} - **Language{{""s"" if language is not string and language | length > 1 else """"}}:** {%- if language is string %} {{ language }} {%- else %}",
"{% for lang in language -%} {{ lang }}{{ "", "" if not loop.last else """" }} {%- endfor %} {%- endif %} {%- else -%} <!-- - **Language:** Unknown --> {%- endif %} {% if license -%} - **License:** {{ license }} {%- else -%} <!-- - **License:** Unknown --> {%- endif %}",
- **Documentation:** [Sentence Transformers Documentation](https://sbert.net) - **Documentation:** [Cross Encoder Documentation](https://www.sbert.net/docs/cross_encoder/usage/usage.html) - **Repository:** [Sentence Transformers on GitHub](https://github.com/UKPLab/sentence-transformers) - **Hugging Face:** [Cross Encoders on Hugging Face](https://huggingface.co/models?library=sentence-transformers&other=cross-encoder),
First install the Sentence Transformers library:,
```bash pip install -U sentence-transformers ```,
Then you can load this model and run inference. ```python from sentence_transformers import CrossEncoder,
"model = CrossEncoder(""{{ model_id | default('cross_encoder_model_id', true) }}"")",
"pairs = [ {%- for text in (predict_example or [[""How many calories in an egg"", ""There are on average between 55 and 80 calories in an egg depending on its size.""], [""How many calories in an egg"", ""Egg whites are very low in calories, have no fat, no cholesterol, and are loaded with protein.""], [""How many calories in an egg"", ""Most of the calories in an egg come from the yellow yolk in the center.""]]) %} {{ ""%r"" | format(text) }}, {%- endfor %} ] scores = model.predict(pairs) print(scores.shape)",
"ranks = model.rank( {{ ""%r"" | format(predict_example[0][0] if predict_example else ""How many calories in an egg"") }}, [ {%- for pair in (predict_example or [[""How many calories in an egg"", ""There are on average between 55 and 80 calories in an egg depending on its size.""], [""How many calories in an egg"", ""Egg whites are very low in calories, have no fat, no cholesterol, and are loaded with protein.""], [""How many calories in an egg"", ""Most of the calories in an egg come from the yellow yolk in the center.""]]) %} {{ ""%r"" | format(pair[1]) }}, {%- endfor %} ] )",
```,
<!--,
<details><summary>Click to see the direct usage in Transformers</summary>,
</details> -->,
<!--,
You can finetune this model on your own dataset.,
<details><summary>Click to expand</summary>,
</details> -->,
<!--,
*List how the model may foreseeably be misused and address what users ought not to do with the model.* --> {% if eval_metrics %},
"{% for metrics in eval_metrics %} #### {{ metrics.description }} {% if metrics.dataset_name %} * Dataset{% if metrics.dataset_name is not string and metrics.dataset_name | length > 1 %}s{% endif %}: {% if metrics.dataset_name is string -%} `{{ metrics.dataset_name }}` {%- else -%} {%- for name in metrics.dataset_name -%} `{{ name }}` {%- if not loop.last -%} {%- if loop.index == metrics.dataset_name | length - 1 %} and {% else -%}, {% endif -%} {%- endif -%} {%- endfor -%} {%- endif -%} {%- endif %} * Evaluated with {% if metrics.class_name.startswith(""sentence_transformers."") %}[<code>{{",
"metrics.class_name.split(""."")[-1] }}</code>](https://sbert.net/docs/package_reference/cross_encoder/evaluation.html#sentence_transformers.cross_encoder.evaluation.{{ metrics.class_name.split(""."")[-1] }}){% else %}<code>{{ metrics.class_name }}</code>{% endif %}{% if metrics.config_code %} with these parameters: {{ metrics.config_code }}{% endif %}",
{{ metrics.table }} {%- endfor %}{% endif %} <!--,
*What are the known or foreseeable issues stemming from this model? You could also flag here known failure cases or weaknesses of the model.* -->,
<!--,
"*What are recommendations with respect to the foreseeable issues? For example, filtering explicit content.* -->",
"{% for dataset_type, dataset_list in [(""training"", train_datasets), (""evaluation"", eval_datasets)] %}{% if dataset_list %}",
{% for dataset in dataset_list %}{% if dataset_list | length > 3 %}<details><summary>{{ dataset['name'] or 'Unnamed Dataset' }}</summary> {% endif %} #### {{ dataset['name'] or 'Unnamed Dataset' }} {% if dataset['name'] %} * Dataset: {% if 'id' in dataset %}[{{ dataset['name'] }}](https://huggingface.co/datasets/{{ dataset['id'] }}){% else %}{{ dataset['name'] }}{% endif %} {%- if 'revision' in dataset and 'id' in dataset %} at [{{ dataset['revision'][:7] }}](https://huggingface.co/datasets/{{ dataset['id'] }}/tree/{{ dataset['revision'] }}){% endif %}{% endif %} {% if dataset['size'] %}*,
"Size: {{ ""{:,}"".format(dataset['size']) }} {{ dataset_type }} samples {% endif %}* Columns: {% if dataset['columns'] | length == 1 %}{{ dataset['columns'][0] }}{% elif dataset['columns'] | length == 2 %}{{ dataset['columns'][0] }} and {{ dataset['columns'][1] }}{% else %}{{ dataset['columns'][:-1] | join(', ') }}, and {{ dataset['columns'][-1] }}{% endif %} {% if dataset['stats_table'] %}* Approximate statistics based on the first {{ [dataset['size'], 1000] | min }} samples: {{ dataset['stats_table'] }}{% endif %}{% if dataset['examples_table'] %}* Samples: {{ dataset['examples_table'] }}{%",
"endif %}* Loss: {% if dataset[""loss""][""fullname""].startswith(""sentence_transformers."") %}[<code>{{ dataset[""loss""][""fullname""].split(""."")[-1] }}</code>](https://sbert.net/docs/package_reference/cross_encoder/losses.html#{{ dataset[""loss""][""fullname""].split(""."")[-1].lower() }}){% else %}<code>{{ dataset[""loss""][""fullname""] }}</code>{% endif %}{% if ""config_code"" in dataset[""loss""] %} with these parameters: {{ dataset[""loss""][""config_code""] }}{% endif %} {% if dataset_list | length > 3 %}</details> {% endif %}{% endfor %}{% endif %}{% endfor -%}",
{% if all_hyperparameters %},
{% if non_default_hyperparameters -%} #### Non-Default Hyperparameters,
"{% for name, value in non_default_hyperparameters.items() %}- `{{ name }}`: {{ value }} {% endfor %}{%- endif %} #### All Hyperparameters <details><summary>Click to expand</summary>",
"{% for name, value in all_hyperparameters.items() %}- `{{ name }}`: {{ value }} {% endfor %} </details> {% endif %}",
{%- if eval_lines %},
{% if hide_eval_lines %}<details><summary>Click to expand</summary>,
{% endif -%} {{ eval_lines }}{% if explain_bold_in_eval %} * The bold row denotes the saved checkpoint.{% endif %} {%- if hide_eval_lines %} </details>{% endif %} {% endif %},
{%- if co2_eq_emissions %},
"Carbon emissions were measured using [CodeCarbon](https://github.com/mlco2/codecarbon). - **Energy Consumed**: {{ ""%.3f""|format(co2_eq_emissions[""energy_consumed""]) }} kWh - **Carbon Emitted**: {{ ""%.3f""|format(co2_eq_emissions[""emissions""] / 1000) }} kg of CO2 - **Hours Used**: {{ co2_eq_emissions[""hours_used""] }} hours",
"- **On Cloud**: {{ ""Yes"" if co2_eq_emissions[""on_cloud""] else ""No"" }} - **GPU Model**: {{ co2_eq_emissions[""hardware_used""] or ""No GPU used"" }} - **CPU Model**: {{ co2_eq_emissions[""cpu_model""] }} - **RAM Size**: {{ ""%.2f""|format(co2_eq_emissions[""ram_total_size""]) }} GB {% endif %}",
"- Python: {{ version[""python""] }} - Sentence Transformers: {{ version[""sentence_transformers""] }} - Transformers: {{ version[""transformers""] }} - PyTorch: {{ version[""torch""] }} - Accelerate: {{ version[""accelerate""] }} - Datasets: {{ version[""datasets""] }} - Tokenizers: {{ version[""tokenizers""] }}",
"{% for loss_name, citation in citations.items() %} #### {{ loss_name }} ```bibtex {{ citation | trim }} ``` {% endfor %} <!--",
*Clearly define terms in order to be accessible across audiences.* -->,
<!--,
"*Lists the people who create the model card, providing recognition and accountability for the detailed work that goes into its construction.* -->",
<!--,
"*Provides a way for people who have updates to the Model Card, suggestions, or questions, to contact the Model Card authors.* -->",
BSD 3-Clause License,
"Copyright (c) 2013-2024, Kim Davies and contributors. All rights reserved.",
"Redistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met:",
"1. Redistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer.",
"2. Redistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution.",
3. Neither the name of the copyright holder nor the names of its contributors may be used to endorse or promote products derived from this software without specific prior written permission.,
"THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS"" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT",
"LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.",
- Steven Bird <stevenbird1@gmail.com> - Edward Loper <edloper@gmail.com> - Ewan Klein <ewan@inf.ed.ac.uk>,
- Tom Aarsen - Rami Al-Rfou' - Mark Amery - Greg Aumann - Ivan Barria - Ingolf Becker - Yonatan Becker - Paul Bedaride - Steven Bethard - Robert Berwick - Dan Blanchard - Nathan Bodenstab - Alexander B√∂hm - Francis Bond - Paul Bone - Jordan Boyd-Graber - Daniel Blanchard - Phil Blunsom - Lars Buitinck - Cristian Capdevila - Steve Cassidy - Chen-Fu Chiang - Dmitry Chichkov - Jinyoung Choi - Andrew Clausen - Lucas Champollion - Graham Christensen - Trevor Cohn - David Coles - Tom Conroy <https://github.com/tconroy> - Claude Coulombe - Lucas Cooper - Robin Cooper - Chris Crowner - James Curran -,
Arthur Darcet - Dariel Dato-on - Selina Dennis - Leon Derczynski - Alexis Dimitriadis - Nikhil Dinesh - Liang Dong - David Doukhan - Rebecca Dridan - Pablo Duboue - Long Duong - Christian Federmann - Campion Fellin - Michelle Fullwood - Dan Garrette - Maciej Gawinecki - Jean Mark Gawron - Sumukh Ghodke - Yoav Goldberg - Michael Wayne Goodman - Dougal Graham - Brent Gray - Simon Greenhill - Clark Grubb - Eduardo Pereira Habkost - Masato Hagiwara - Lauri Hallila - Michael Hansen - Yurie Hara - Will Hardy - Tyler Hartley - Peter Hawkins - Saimadhav Heblikar - Fredrik Hedman - Helder - Michael,
Heilman - Ofer Helman - Christopher Hench - Bruce Hill - Amy Holland - Kristy Hollingshead - Marcus Huderle - Baden Hughes - Nancy Ide - Rebecca Ingram - Edward Ivanovic - Thomas Jakobsen - Nick Johnson - Eric Kafe - Piotr Kasprzyk - Angelos Katharopoulos - Sudharshan Kaushik - Chris Koenig - Mikhail Korobov - Denis Krusko - Ilia Kurenkov - Stefano Lattarini - Pierre-Fran√ßois Laquerre - Stefano Lattarini - Haejoong Lee - Jackson Lee - Max Leonov - Chris Liechti - Hyuckin David Lim - Tom Lippincott - Peter Ljungl√∂f - Alex Louden - David Luke≈° - Joseph Lynch - Nitin Madnani - Felipe Madrigal -,
Bj√∏rn M√¶land - Dean Malmgren - Christopher Maloof - Rob Malouf - Iker Manterola - Carl de Marcken - Mitch Marcus - Torsten Marek - Robert Marshall - Marius Mather - Duncan McGreggor - David McClosky - Xinfan Meng - Dmitrijs Milajevs - Matt Miller - Margaret Mitchell - Tomonori Nagano - Jason Narad - Shari A‚Äôaidil Nasruddin - Lance Nathan - Morten Neergaard - David Nemeskey - Eric Nichols - Joel Nothman - Alireza Nourian - Alexander Oleynikov - Pierpaolo Pantone - Ted Pedersen - Jacob Perkins - Alberto Planas - Ondrej Platek - Alessandro Presta - Qi Liu - Martin Thorsen Ranang - Michael,
Recachinas - Brandon Rhodes - Joshua Ritterman - Will Roberts - Stuart Robinson - Carlos Rodriguez - Lorenzo Rubio - Alex Rudnick - Jussi Salmela - Geoffrey Sampson - Kepa Sarasola - Kevin Scannell - Nathan Schneider - Rico Sennrich - Thomas Skardal - Eric Smith - Lynn Soe - Rob Speer - Peter Spiller - Richard Sproat - Ceri Stagg - Peter Stahl - Oliver Steele - Thomas Stieglmaier - Jan Strunk - Liling Tan - Claire Taylor - Louis Tiao - Steven Tomcavage - Tiago Tresoldi - Marcus Uneson - Yu Usami - Petro Verkhogliad - Peter Wang - Zhe Wang - Charlotte Wilson - Chuck Wooters - Steven Xu -,
Beracah Yankama - Lei Ye (Âè∂Á£ä) - Patrick Ye - Geraldine Sim Wei Ying - Jason Yoder - Thomas Zieglier - 0ssifrage - ducki13 - kiwipi - lade - isnowfy - onesandzeros - pquentin - wvanlint - √Ålvaro Justen <https://github.com/turicas> - bjut-hz - Sergio Oller - Izam Mohammed <https://github.com/izam-mohammed> - Will Monroe - Elijah Rippeth - Emil Manukyan - Casper Lehmann-Str√∏m - Andrew Giel - Tanin Na Nakorn - Linghao Zhang - Colin Carroll - Heguang Miao - Hannah Aizenman (story645) - George Berry - Adam Nelson - J Richard Snape - Alex Constantin <alex@keyworder.ch> - Tsolak Ghukasyan - Prasasto,
Adi - Safwan Kamarrudin - Arthur Tilley - Vilhjalmur Thorsteinsson - Jaehoon Hwang <https://github.com/jaehoonhwang> - Chintan Shah <https://github.com/chintanshah24> - sbagan - Zicheng Xu - Albert Au Yeung <https://github.com/albertauyeung> - Shenjian Zhao - Deng Wang <https://github.com/lmatt-bit> - Ali Abdullah - Stoytcho Stoytchev - Lakhdar Benzahia - Kheireddine Abainia <https://github.com/xprogramer> - Yibin Lin <https://github.com/yibinlin> - Artiem Krinitsyn - Bj√∂rn Mattsson - Oleg Chislov - Pavan Gururaj Joshi <https://github.com/PavanGJ> - Ethan Hill <https://github.com/hill1303> -,
Vivek Lakshmanan - Somnath Rakshit <https://github.com/somnathrakshit> - Anlan Du - Pulkit Maloo <https://github.com/pulkitmaloo> - Brandon M. Burroughs <https://github.com/brandonmburroughs> - John Stewart <https://github.com/free-variation> - Iaroslav Tymchenko <https://github.com/myproblemchild> - Ale≈° Tamchyna - Tim Gianitsos <https://github.com/timgianitsos> - Philippe Partarrieu <https://github.com/ppartarr> - Andrew Owen Martin - Adrian Ellis <https://github.com/adrianjellis> - Nat Quayle Nelson <https://github.com/nqnstudios> - Yanpeng Zhao <https://github.com/zhaoyanpeng> - Matan Rak,
<https://github.com/matanrak> - Nick Ulle <https://github.com/nick-ulle> - Uday Krishna <https://github.com/udaykrishna> - Osman Zubair <https://github.com/okz12> - Viresh Gupta <https://github.com/virresh> - Ond≈ôej C√≠fka <https://github.com/cifkao> - Iris X. Zhou <https://github.com/irisxzhou> - Devashish Lal <https://github.com/BLaZeKiLL> - Gerhard Kremer <https://github.com/GerhardKa> - Nicolas Darr <https://github.com/ndarr> - Herv√© Nicol <https://github.com/hervenicol> - Alexandre H. T. Dias <https://github.com/alexandredias3d> - Daksh Shah <https://github.com/Daksh> - Jacob Weightman,
<https://github.com/jacobdweightman> - Bonifacio de Oliveira <https://github.com/Bonifacio2> - Armins Bagrats Stepanjans <https://github.com/ab-10> - Vassilis Palassopoulos <https://github.com/palasso> - Ram Rachum <https://github.com/cool-RR> - Or Sharir <https://github.com/orsharir> - Denali Molitor <https://github.com/dmmolitor> - Jacob Moorman <https://github.com/jdmoorman> - Cory Nezin <https://github.com/corynezin> - Matt Chaput - Danny Sepler <https://github.com/dannysepler> - Akshita Bhagia <https://github.com/AkshitaB> - Pratap Yadav <https://github.com/prtpydv> - Hiroki Teranishi,
<https://github.com/chantera> - Ruben Cartuyvels <https://github.com/rubencart> - Dalton Pearson <https://github.com/daltonpearson> - Robby Horvath <https://github.com/robbyhorvath> - Gavish Poddar <https://github.com/gavishpoddar> - Saibo Geng <https://github.com/Saibo-creator> - Ahmet Yildirim <https://github.com/RnDevelover> - Yuta Nakamura <https://github.com/yutanakamura-tky> - Adam Hawley <https://github.com/adamjhawley> - Panagiotis Simakis <https://github.com/sp1thas> - Richard Wang <https://github.com/richarddwang> - Alexandre Perez-Lebel <https://github.com/aperezlebel> - Fernando,
Carranza <https://github.com/fernandocar86> - Martin Kondratzky <https://github.com/martinkondra> - Heungson Lee <https://github.com/heungson> - M.K. Pawelkiewicz <https://github.com/hamiltonianflow> - Steven Thomas Smith <https://github.com/essandess> - Jan Lennartz <https://github.com/Madnex> - Tim Sockel <https://github.com/TiMauzi> - Akihiro Yamazaki <https://github.com/zakkie> - Ron Urbach <https://github.com/sharpblade4> - Vivek Kalyan <https://github.com/vivekkalyan> - Tom Strange https://github.com/strangetom,
- Martin Porter - Vivake Gupta - Barry Wilkins - Hiranmay Ghosh - Chris Emerson,
- Assem Chelli - Abdelkrim Aries - Lakhdar Benzahia,
[![PyPI](https://img.shields.io/pypi/v/nltk.svg)](https://pypi.python.org/pypi/nltk) ![CI](https://github.com/nltk/nltk/actions/workflows/ci.yaml/badge.svg?branch=develop),
"NLTK -- the Natural Language Toolkit -- is a suite of open source Python modules, data sets, and tutorials supporting research and development in Natural Language Processing. NLTK requires Python version 3.8, 3.9, 3.10, 3.11 or 3.12.",
"For documentation, please visit [nltk.org](https://www.nltk.org/).",
Do you want to contribute to NLTK development? Great! Please read [CONTRIBUTING.md](CONTRIBUTING.md) for more details.,
See also [how to contribute to NLTK](https://www.nltk.org/contribute.html).,
"Have you found the toolkit helpful?  Please support NLTK development by donating to the project via PayPal, using the link on the NLTK homepage.",
"If you publish work that uses NLTK, please cite the NLTK book, as follows:",
"Bird, Steven, Edward Loper and Ewan Klein (2009). Natural Language Processing with Python.  O'Reilly Media Inc.",
Copyright (C) 2001-2024 NLTK Project,
"For license information, see [LICENSE.txt](LICENSE.txt).",
[AUTHORS.md](AUTHORS.md) contains a list of everyone who has contributed to NLTK.,
"- NLTK source code is distributed under the Apache 2.0 License. - NLTK documentation is distributed under the Creative Commons Attribution-Noncommercial-No Derivative Works 3.0 United States license. - NLTK corpora are provided under the terms given in the README file for each corpus; all are redistributable and available for non-commercial use. - NLTK may be freely redistributed, subject to the provisions of these licenses.",
---,
{{ card_data }} ---,
<!-- Provide a quick summary of the dataset. -->,
"{{ dataset_summary | default("""", true) }}",
<!-- Provide a longer summary of what this dataset is. -->,
"{{ dataset_description | default("""", true) }}",
"- **Curated by:** {{ curators | default(""[More Information Needed]"", true)}} - **Funded by [optional]:** {{ funded_by | default(""[More Information Needed]"", true)}} - **Shared by [optional]:** {{ shared_by | default(""[More Information Needed]"", true)}} - **Language(s) (NLP):** {{ language | default(""[More Information Needed]"", true)}} - **License:** {{ license | default(""[More Information Needed]"", true)}}",
<!-- Provide the basic links for the dataset. -->,
"- **Repository:** {{ repo | default(""[More Information Needed]"", true)}} - **Paper [optional]:** {{ paper | default(""[More Information Needed]"", true)}} - **Demo [optional]:** {{ demo | default(""[More Information Needed]"", true)}}",
<!-- Address questions around how the dataset is intended to be used. -->,
<!-- This section describes suitable use cases for the dataset. -->,
"{{ direct_use | default(""[More Information Needed]"", true)}}",
"<!-- This section addresses misuse, malicious use, and uses that the dataset will not work well for. -->",
"{{ out_of_scope_use | default(""[More Information Needed]"", true)}}",
"<!-- This section provides a description of the dataset fields, and additional information about the dataset structure such as criteria used to create the splits, relationships between data points, etc. -->",
"{{ dataset_structure | default(""[More Information Needed]"", true)}}",
<!-- Motivation for the creation of this dataset. -->,
"{{ curation_rationale_section | default(""[More Information Needed]"", true)}}",
"<!-- This section describes the source data (e.g. news text and headlines, social media posts, translated sentences, ...). -->",
#### Data Collection and Processing,
"<!-- This section describes the data collection and processing process such as data selection criteria, filtering and normalization methods, tools and libraries used, etc. -->",
"{{ data_collection_and_processing_section | default(""[More Information Needed]"", true)}}",
#### Who are the source data producers?,
<!-- This section describes the people or systems who originally created the data. It should also include self-reported demographic or identity information for the source data creators if this information is available. -->,
"{{ source_data_producers_section | default(""[More Information Needed]"", true)}}",
"<!-- If the dataset contains annotations which are not part of the initial data collection, use this section to describe them. -->",
#### Annotation process,
"<!-- This section describes the annotation process such as annotation tools used in the process, the amount of data annotated, annotation guidelines provided to the annotators, interannotator statistics, annotation validation, etc. -->",
"{{ annotation_process_section | default(""[More Information Needed]"", true)}}",
#### Who are the annotators?,
<!-- This section describes the people or systems who created the annotations. -->,
"{{ who_are_annotators_section | default(""[More Information Needed]"", true)}}",
#### Personal and Sensitive Information,
"<!-- State whether the dataset contains data that might be considered personal, sensitive, or private (e.g., data that reveals addresses, uniquely identifiable names or aliases, racial or ethnic origins, sexual orientations, religious beliefs, political opinions, financial or health data, etc.). If efforts were made to anonymize the data, describe the anonymization process. -->",
"{{ personal_and_sensitive_information | default(""[More Information Needed]"", true)}}",
<!-- This section is meant to convey both technical and sociotechnical limitations. -->,
"{{ bias_risks_limitations | default(""[More Information Needed]"", true)}}",
"<!-- This section is meant to convey recommendations with respect to the bias, risk, and technical limitations. -->",
"{{ bias_recommendations | default(""Users should be made aware of the risks, biases and limitations of the dataset. More information needed for further recommendations."", true)}}",
"<!-- If there is a paper or blog post introducing the dataset, the APA and Bibtex information for that should go in this section. -->",
**BibTeX:**,
"{{ citation_bibtex | default(""[More Information Needed]"", true)}}",
**APA:**,
"{{ citation_apa | default(""[More Information Needed]"", true)}}",
"<!-- If relevant, include terms and calculations in this section that can help readers understand the dataset or dataset card. -->",
"{{ glossary | default(""[More Information Needed]"", true)}}",
"{{ more_information | default(""[More Information Needed]"", true)}}",
"{{ dataset_card_authors | default(""[More Information Needed]"", true)}}",
"{{ dataset_card_contact | default(""[More Information Needed]"", true)}}",
---,
{{ card_data }} ---,
<!-- Provide a quick summary of what the model is/does. -->,
"{{ model_summary | default("""", true) }}",
<!-- Provide a longer summary of what this model is. -->,
"{{ model_description | default("""", true) }}",
"- **Developed by:** {{ developers | default(""[More Information Needed]"", true)}} - **Funded by [optional]:** {{ funded_by | default(""[More Information Needed]"", true)}} - **Shared by [optional]:** {{ shared_by | default(""[More Information Needed]"", true)}} - **Model type:** {{ model_type | default(""[More Information Needed]"", true)}} - **Language(s) (NLP):** {{ language | default(""[More Information Needed]"", true)}} - **License:** {{ license | default(""[More Information Needed]"", true)}} - **Finetuned from model [optional]:** {{ base_model | default(""[More Information Needed]"", true)}}",
<!-- Provide the basic links for the model. -->,
"- **Repository:** {{ repo | default(""[More Information Needed]"", true)}} - **Paper [optional]:** {{ paper | default(""[More Information Needed]"", true)}} - **Demo [optional]:** {{ demo | default(""[More Information Needed]"", true)}}",
"<!-- Address questions around how the model is intended to be used, including the foreseeable users of the model and those affected by the model. -->",
<!-- This section is for the model use without fine-tuning or plugging into a larger ecosystem/app. -->,
"{{ direct_use | default(""[More Information Needed]"", true)}}",
"<!-- This section is for the model use when fine-tuned for a task, or when plugged into a larger ecosystem/app -->",
"{{ downstream_use | default(""[More Information Needed]"", true)}}",
"<!-- This section addresses misuse, malicious use, and uses that the model will not work well for. -->",
"{{ out_of_scope_use | default(""[More Information Needed]"", true)}}",
<!-- This section is meant to convey both technical and sociotechnical limitations. -->,
"{{ bias_risks_limitations | default(""[More Information Needed]"", true)}}",
"<!-- This section is meant to convey recommendations with respect to the bias, risk, and technical limitations. -->",
"{{ bias_recommendations | default(""Users (both direct and downstream) should be made aware of the risks, biases and limitations of the model. More information needed for further recommendations."", true)}}",
Use the code below to get started with the model.,
"{{ get_started_code | default(""[More Information Needed]"", true)}}",
"<!-- This should link to a Dataset Card, perhaps with a short stub of information on what the training data is all about as well as documentation related to data pre-processing or additional filtering. -->",
"{{ training_data | default(""[More Information Needed]"", true)}}",
<!-- This relates heavily to the Technical Specifications. Content here should link to that section when it is relevant to the training procedure. -->,
#### Preprocessing [optional],
"{{ preprocessing | default(""[More Information Needed]"", true)}}",
#### Training Hyperparameters,
"- **Training regime:** {{ training_regime | default(""[More Information Needed]"", true)}} <!--fp32, fp16 mixed precision, bf16 mixed precision, bf16 non-mixed precision, fp16 non-mixed precision, fp8 mixed precision -->",
"#### Speeds, Sizes, Times [optional]",
"<!-- This section provides information about throughput, start/end time, checkpoint size if relevant, etc. -->",
"{{ speeds_sizes_times | default(""[More Information Needed]"", true)}}",
<!-- This section describes the evaluation protocols and provides the results. -->,
#### Testing Data,
<!-- This should link to a Dataset Card if possible. -->,
"{{ testing_data | default(""[More Information Needed]"", true)}}",
#### Factors,
"<!-- These are the things the evaluation is disaggregating by, e.g., subpopulations or domains. -->",
"{{ testing_factors | default(""[More Information Needed]"", true)}}",
#### Metrics,
"<!-- These are the evaluation metrics being used, ideally with a description of why. -->",
"{{ testing_metrics | default(""[More Information Needed]"", true)}}",
"{{ results | default(""[More Information Needed]"", true)}}",
#### Summary,
"{{ results_summary | default("""", true) }}",
<!-- Relevant interpretability work for the model goes here -->,
"{{ model_examination | default(""[More Information Needed]"", true)}}",
"<!-- Total emissions (in grams of CO2eq) and additional considerations, such as electricity usage, go here. Edit the suggested text below accordingly -->",
Carbon emissions can be estimated using the [Machine Learning Impact calculator](https://mlco2.github.io/impact#compute) presented in [Lacoste et al. (2019)](https://arxiv.org/abs/1910.09700).,
"- **Hardware Type:** {{ hardware_type | default(""[More Information Needed]"", true)}} - **Hours used:** {{ hours_used | default(""[More Information Needed]"", true)}} - **Cloud Provider:** {{ cloud_provider | default(""[More Information Needed]"", true)}} - **Compute Region:** {{ cloud_region | default(""[More Information Needed]"", true)}} - **Carbon Emitted:** {{ co2_emitted | default(""[More Information Needed]"", true)}}",
"{{ model_specs | default(""[More Information Needed]"", true)}}",
"{{ compute_infrastructure | default(""[More Information Needed]"", true)}}",
#### Hardware,
"{{ hardware_requirements | default(""[More Information Needed]"", true)}}",
#### Software,
"{{ software | default(""[More Information Needed]"", true)}}",
"<!-- If there is a paper or blog post introducing the model, the APA and Bibtex information for that should go in this section. -->",
**BibTeX:**,
"{{ citation_bibtex | default(""[More Information Needed]"", true)}}",
**APA:**,
"{{ citation_apa | default(""[More Information Needed]"", true)}}",
"<!-- If relevant, include terms and calculations in this section that can help readers understand the model or model card. -->",
"{{ glossary | default(""[More Information Needed]"", true)}}",
"{{ more_information | default(""[More Information Needed]"", true)}}",
"{{ model_card_authors | default(""[More Information Needed]"", true)}}",
"{{ model_card_contact | default(""[More Information Needed]"", true)}}",
This directory contains the JavaScript portion of the Altair `JupyterChart`. The `JupyterChart` is based on the [AnyWidget](https://anywidget.dev/) project.,
**This software is dual-licensed under the The University of Illinois/NCSA Open Source License (NCSA) and The 3-Clause BSD License**,
**Copyright (c) 2019 Kevin Sheppard. All rights reserved.**,
"Developed by: Kevin Sheppard (<kevin.sheppard@economics.ox.ac.uk>, <kevin.k.sheppard@gmail.com>) [http://www.kevinsheppard.com](http://www.kevinsheppard.com)",
"Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the ""Software""), to deal with the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:",
"Redistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimers.",
"Redistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimers in the documentation and/or other materials provided with the distribution.",
"Neither the names of Kevin Sheppard, nor the names of any contributors may be used to endorse or promote products derived from this Software without specific prior written permission.",
"**THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE CONTRIBUTORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS WITH THE SOFTWARE.**",
**Copyright (c) 2019 Kevin Sheppard. All rights reserved.**,
"Redistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met:",
"1. Redistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer.",
"2. Redistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution.",
3. Neither the name of the copyright holder nor the names of its contributors may be used to endorse or promote products derived from this software without specific prior written permission.,
"**THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS"" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT",
"LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.**",
"Many parts of this module have been derived from original sources, often the algorithm's designer. Component licenses are located with the component code.",
Update this directory using maint_tools/vendor_array_api_compat.sh,
Update this directory using maint_tools/vendor_array_api_extra.sh,
"If you add a file to this directory, you **MUST** update `torch/CMakeLists.txt` and add the file as a dependency to the `add_custom_command` call.",
Copyright (C) 2010-2019 Max-Planck-Society All rights reserved.,
"Redistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met:",
"* Redistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer. * Redistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution. * Neither the name of the copyright holder nor the names of its contributors may be used to endorse or promote products derived from this software without specific prior written permission.",
"THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS"" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT",
"LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.",
MIT License,
"Copyright (c) 2024, Marco Gorelli",
"Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the ""Software""), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:",
The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.,
"THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.",
"<!--- Licensed to the Apache Software Foundation (ASF) under one or more contributor license agreements.  See the NOTICE file distributed with this work for additional information regarding copyright ownership.  The ASF licenses this file to you under the Apache License, Version 2.0 (the ""License""); you may not use this file except in compliance with the License.  You may obtain a copy of the License at",
http://www.apache.org/licenses/LICENSE-2.0,
"Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.  See the License for the specific language governing permissions and limitations under the License. -->",
The ORC and JSON files come from the `examples` directory in the Apache ORC source tree: https://github.com/apache/orc/tree/main/examples,
- Simulates AI-centric product pipeline - DeepSeek + Bayesian Q&A + Transformer fallback,
"- Traceable, transparent AI engineering - Real-world deployment to Hugging Face Spaces - Continuous integration & sprint metrics",
"- PM, Dev, AI Lead, DevOps",
- User ‚Üí RepoSage CLI ‚Üí index/query modules,
- Embedding engine: all-MiniLM-L6-v2 - FAISS vector store,
- Cosine-likelihood softmax ‚Üí posterior,
- Flan-T5 small for low-confidence queries,
- GitHub Actions ‚Üí HF Spaces deploy,
