text,category
"### Training Hardware
- **On Cloud**: {{ ""Yes"" if co2_eq_emissions[""on_cloud""] else ""No"" }}
- **GPU Model**: {{ co2_eq_emissions[""hardware_used""] or ""No GPU used"" }}
- **CPU Model**: {{ co2_eq_emissions[""cpu_model""] }}
- **RAM Size**: {{ ""%.2f""|format(co2_eq_emissions[""ram_total_size""]) }} GB
{% endif %}
### Framework Versions
- Python: {{ version[""python""] }}
- Sentence Transformers: {{ version[""sentence_transformers""] }}
- Transformers: {{ version[""transformers""] }}
- PyTorch: {{ version[""torch""] }}
- Accelerate: {{ version[""accelerate""] }}
- Datasets: {{ version[""datasets""] }}
- Tokenizers: {{ version[""tokenizers""] }}",other
"```bash
pip install -U sentence-transformers
```",installation
"## Features
- **DeepSeek Index** of lecture slides, code, and readings  
- **Bayesian Posterior Scoring** for precise answer selection  
- **Transformer Fallback** via HF text2text when confidence is low  
- **CLI & REST API** for local and web deployment  
- **CI/CD** via GitHub Actions → Hugging Face Spaces",usage
"The ORC and JSON files come from the `examples` directory in the Apache ORC
source tree:
https://github.com/apache/orc/tree/main/examples",other
"<!-- If there is a paper or blog post introducing the dataset, the APA and Bibtex information for that should go in this section. -->",reference
"{%- if eval_lines %}
### Training Logs
{% if hide_eval_lines %}<details><summary>Click to expand</summary>",usage
"### Installation
```bash
git clone https://github.com/RohitRajpoot/reposage.git
cd reposage
pip install -r requirements.txt",installation
"{%- if eval_lines %}
### Training Logs
{% if hide_eval_lines %}<details><summary>Click to expand</summary>",usage
"Then you can load this model and run inference.
```python
from sentence_transformers import SentenceTransformer",other
"### Usage
# Build the index
reposage index",usage
"1. Redistributions of source code must retain the above copyright notice,
   this list of conditions and the following disclaimer.",other
Update this directory using maint_tools/vendor_array_api_extra.sh,reference
"### Installation
```bash
git clone https://github.com/RohitRajpoot/reposage.git
cd reposage
pip install -r requirements.txt",installation
"# Slide 3: Bayesian Q&A Layer
- Cosine-likelihood softmax → posterior",reference
**APA:**,other
First install the Sentence Transformers library:,installation
"{{ citation_bibtex | default(""[More Information Needed]"", true)}}",other
"<!--
### Out-of-Scope Use",other
#### Training Hyperparameters,other
"## Overview
RepoSage is an AI-centric study assistant that uses semantic search (DeepSeek), a Bayesian Q&A layer, and a transformer fallback to answer student queries with traceability and transparency.",other
"```
{{ model_string }}
```",other
<details><summary>Click to expand</summary>,usage
"# 2) Install dependencies
pip install --upgrade pip setuptools wheel
pip install -r requirements.txt",installation
"# Query
reposage query ""What is DeepSeek?"" --threshold 0.3",usage
You can finetune this model on your own dataset.,other
"Copyright (c) 2024, Marco Gorelli",other
"{{ out_of_scope_use | default(""[More Information Needed]"", true)}}",other
"```bash
pip install -U sentence-transformers
```",installation
First install the Sentence Transformers library:,installation
"1. **Route** via the H2O classifier to select a subset of paragraphs.
     2. **Retrieve** top‑k relevant passages with FAISS.
     3. **Rescore** with a Bayesian posterior to boost high-confidence results.
     4. **Fallback**: if the posterior confidence is below threshold, generate an answer using a compact Flan‑T5 model.",reference
### Redistributing,other
"```bash
pip install -U sentence-transformers
```",installation
First install the Sentence Transformers library:,installation
"### Metrics
{% for metrics in eval_metrics %}
#### {{ metrics.description }}
{% if metrics.dataset_name %}
* Dataset{% if metrics.dataset_name is not string and metrics.dataset_name | length > 1 %}s{% endif %}: {% if metrics.dataset_name is string -%}
        `{{ metrics.dataset_name }}`
    {%- else -%}
        {%- for name in metrics.dataset_name -%}
            `{{ name }}`
            {%- if not loop.last -%}
                {%- if loop.index == metrics.dataset_name | length - 1 %} and {% else -%}, {% endif -%}
            {%- endif -%}
        {%- endfor -%}
    {%- endif -%}
{%- endif %}
* Evaluated with {% if metrics.class_name.startswith(""sentence_transformers."") %}[<code>{{ metrics.class_name.split(""."")[-1] }}</code>](https://sbert.net/docs/package_reference/cross_encoder/evaluation.html#sentence_transformers.cross_encoder.evaluation.{{ metrics.class_name.split(""."")[-1] }}){% else %}<code>{{ metrics.class_name }}</code>{% endif %}{% if metrics.config_code %} with these parameters:
{{ metrics.config_code }}{% endif %}",reference
"<!-- If there is a paper or blog post introducing the dataset, the APA and Bibtex information for that should go in this section. -->",reference
"<!-- If there is a paper or blog post introducing the model, the APA and Bibtex information for that should go in this section. -->",reference
"**Why it matters:**  
- Demonstrates core GPT “atoms” (token → embedding → attention → generation)  
- Shows an end-to-end MLOps flow: local dev → GitHub Actions → Docker → Hugging Face Spaces",other
"{{ source_data_producers_section | default(""[More Information Needed]"", true)}}",other
"```bash
pip install -U sentence-transformers
```",installation
First install the Sentence Transformers library:,installation
## ⚙️ Installation,installation
"* After labeling paragraphs (e.g. `installation`, `usage`, `configuration`, `api_reference`, `troubleshooting`), train an H2O AutoML model to predict the category of any incoming question.
   * This routing step ensures each query is answered by the most relevant section of your docs.",installation
"```bash
pip install -U sentence-transformers
```",installation
"### Installation
```bash
git clone https://github.com/RohitRajpoot/reposage.git
cd reposage
pip install -r requirements.txt",installation
"* After labeling paragraphs (e.g. `installation`, `usage`, `configuration`, `api_reference`, `troubleshooting`), train an H2O AutoML model to predict the category of any incoming question.
   * This routing step ensures each query is answered by the most relevant section of your docs.",installation
"Copyright (C) 2010-2019 Max-Planck-Society
All rights reserved.",other
## 🛠️ Next Steps,other
"## Training Details
{% for dataset_type, dataset_list in [(""training"", train_datasets), (""evaluation"", eval_datasets)] %}{% if dataset_list %}
### {{ dataset_type.title() }} Dataset{{""s"" if dataset_list | length > 1 else """"}}
{% for dataset in dataset_list %}{% if dataset_list | length > 3 %}<details><summary>{{ dataset['name'] or 'Unnamed Dataset' }}</summary>
{% endif %}
#### {{ dataset['name'] or 'Unnamed Dataset' }}
{% if dataset['name'] %}
* Dataset: {% if 'id' in dataset %}[{{ dataset['name'] }}](https://huggingface.co/datasets/{{ dataset['id'] }}){% else %}{{ dataset['name'] }}{% endif %}
{%- if 'revision' in dataset and 'id' in dataset %} at [{{ dataset['revision'][:7] }}](https://huggingface.co/datasets/{{ dataset['id'] }}/tree/{{ dataset['revision'] }}){% endif %}{% endif %}
{% if dataset['size'] %}* Size: {{ ""{:,}"".format(dataset['size']) }} {{ dataset_type }} samples
{% endif %}* Columns: {% if dataset['columns'] | length == 1 %}{{ dataset['columns'][0] }}{% elif dataset['columns'] | length == 2 %}{{ dataset['columns'][0] }} and {{ dataset['columns'][1] }}{% else %}{{ dataset['columns'][:-1] | join(', ') }}, and {{ dataset['columns'][-1] }}{% endif %}
{% if dataset['stats_table'] %}* Approximate statistics based on the first {{ [dataset['size'], 1000] | min }} samples:
{{ dataset['stats_table'] }}{% endif %}{% if dataset['examples_table'] %}* Samples:
{{ dataset['examples_table'] }}{% endif %}* Loss: {% if dataset[""loss""][""fullname""].startswith(""sentence_transformers."") %}[<code>{{ dataset[""loss""][""fullname""].split(""."")[-1] }}</code>](https://sbert.net/docs/package_reference/sentence_transformer/losses.html#{{ dataset[""loss""][""fullname""].split(""."")[-1].lower() }}){% else %}<code>{{ dataset[""loss""][""fullname""] }}</code>{% endif %}{% if ""config_code"" in dataset[""loss""] %} with these parameters:
{{ dataset[""loss""][""config_code""] }}{% endif %}
{% if dataset_list | length > 3 %}</details>
{% endif %}{% endfor %}{% endif %}{% endfor -%}",reference
"## Training Details
{% for dataset_type, dataset_list in [(""training"", train_datasets), (""evaluation"", eval_datasets)] %}{% if dataset_list %}
### {{ dataset_type.title() }} Dataset{{""s"" if dataset_list | length > 1 else """"}}
{% for dataset in dataset_list %}{% if dataset_list | length > 3 %}<details><summary>{{ dataset['name'] or 'Unnamed Dataset' }}</summary>
{% endif %}
#### {{ dataset['name'] or 'Unnamed Dataset' }}
{% if dataset['name'] %}
* Dataset: {% if 'id' in dataset %}[{{ dataset['name'] }}](https://huggingface.co/datasets/{{ dataset['id'] }}){% else %}{{ dataset['name'] }}{% endif %}
{%- if 'revision' in dataset and 'id' in dataset %} at [{{ dataset['revision'][:7] }}](https://huggingface.co/datasets/{{ dataset['id'] }}/tree/{{ dataset['revision'] }}){% endif %}{% endif %}
{% if dataset['size'] %}* Size: {{ ""{:,}"".format(dataset['size']) }} {{ dataset_type }} samples
{% endif %}* Columns: {% if dataset['columns'] | length == 1 %}{{ dataset['columns'][0] }}{% elif dataset['columns'] | length == 2 %}{{ dataset['columns'][0] }} and {{ dataset['columns'][1] }}{% else %}{{ dataset['columns'][:-1] | join(', ') }}, and {{ dataset['columns'][-1] }}{% endif %}
{% if dataset['stats_table'] %}* Approximate statistics based on the first {{ [dataset['size'], 1000] | min }} samples:
{{ dataset['stats_table'] }}{% endif %}{% if dataset['examples_table'] %}* Samples:
{{ dataset['examples_table'] }}{% endif %}* Loss: {% if dataset[""loss""][""fullname""].startswith(""sentence_transformers."") %}[<code>{{ dataset[""loss""][""fullname""].split(""."")[-1] }}</code>](https://sbert.net/docs/package_reference/sentence_transformer/losses.html#{{ dataset[""loss""][""fullname""].split(""."")[-1].lower() }}){% else %}<code>{{ dataset[""loss""][""fullname""] }}</code>{% endif %}{% if ""config_code"" in dataset[""loss""] %} with these parameters:
{{ dataset[""loss""][""config_code""] }}{% endif %}
{% if dataset_list | length > 3 %}</details>
{% endif %}{% endfor %}{% endif %}{% endfor -%}",reference
"| Team Member | Role(s)                 | Responsibilities                             |
|-------------|-------------------------|----------------------------------------------|
| Rohit       | CEO / Product Lead,     | • Define product vision & roadmap<br>• Manage scope & stakeholder communication<br>• Write technical documentation |
| Kishan      | CTO / DevOps Lead,      | • Develop DeepSeek index & CLI scaffold<br>• Set up CI/CD pipelines<br>• Manage Hugging Face Spaces deployments   |
| Maitri      | Data Scientist / AI Lead| • Implement Bayesian Q&A layer<br>• Design & tune priors<br>• Oversee transformer fallback prompts          |",usage
"{% for name, value in non_default_hyperparameters.items() %}- `{{ name }}`: {{ value }}
{% endfor %}{%- endif %}
#### All Hyperparameters
<details><summary>Click to expand</summary>",usage
"# 4) Run Streamlit demo
streamlit run app.py",other
"```bash
pip install -U sentence-transformers
```",installation
"# Slide 1: Architecture Overview
- User → RepoSage CLI → index/query modules",usage
"## Training Details
{% for dataset_type, dataset_list in [(""training"", train_datasets), (""evaluation"", eval_datasets)] %}{% if dataset_list %}
### {{ dataset_type.title() }} Dataset{{""s"" if dataset_list | length > 1 else """"}}
{% for dataset in dataset_list %}{% if dataset_list | length > 3 %}<details><summary>{{ dataset['name'] or 'Unnamed Dataset' }}</summary>
{% endif %}
#### {{ dataset['name'] or 'Unnamed Dataset' }}
{% if dataset['name'] %}
* Dataset: {% if 'id' in dataset %}[{{ dataset['name'] }}](https://huggingface.co/datasets/{{ dataset['id'] }}){% else %}{{ dataset['name'] }}{% endif %}
{%- if 'revision' in dataset and 'id' in dataset %} at [{{ dataset['revision'][:7] }}](https://huggingface.co/datasets/{{ dataset['id'] }}/tree/{{ dataset['revision'] }}){% endif %}{% endif %}
{% if dataset['size'] %}* Size: {{ ""{:,}"".format(dataset['size']) }} {{ dataset_type }} samples
{% endif %}* Columns: {% if dataset['columns'] | length == 1 %}{{ dataset['columns'][0] }}{% elif dataset['columns'] | length == 2 %}{{ dataset['columns'][0] }} and {{ dataset['columns'][1] }}{% else %}{{ dataset['columns'][:-1] | join(', ') }}, and {{ dataset['columns'][-1] }}{% endif %}
{% if dataset['stats_table'] %}* Approximate statistics based on the first {{ [dataset['size'], 1000] | min }} samples:
{{ dataset['stats_table'] }}{% endif %}{% if dataset['examples_table'] %}* Samples:
{{ dataset['examples_table'] }}{% endif %}* Loss: {% if dataset[""loss""][""fullname""].startswith(""sentence_transformers."") %}[<code>{{ dataset[""loss""][""fullname""].split(""."")[-1] }}</code>](https://sbert.net/docs/package_reference/cross_encoder/losses.html#{{ dataset[""loss""][""fullname""].split(""."")[-1].lower() }}){% else %}<code>{{ dataset[""loss""][""fullname""] }}</code>{% endif %}{% if ""config_code"" in dataset[""loss""] %} with these parameters:
{{ dataset[""loss""][""config_code""] }}{% endif %}
{% if dataset_list | length > 3 %}</details>
{% endif %}{% endfor %}{% endif %}{% endfor -%}",reference
"<!-- If there is a paper or blog post introducing the dataset, the APA and Bibtex information for that should go in this section. -->",reference
"{%- if eval_lines %}
### Training Logs
{% if hide_eval_lines %}<details><summary>Click to expand</summary>",usage
"```bash
pip install -U sentence-transformers
```",installation
"## Training Details
{% for dataset_type, dataset_list in [(""training"", train_datasets), (""evaluation"", eval_datasets)] %}{% if dataset_list %}
### {{ dataset_type.title() }} Dataset{{""s"" if dataset_list | length > 1 else """"}}
{% for dataset in dataset_list %}{% if dataset_list | length > 3 %}<details><summary>{{ dataset['name'] or 'Unnamed Dataset' }}</summary>
{% endif %}
#### {{ dataset['name'] or 'Unnamed Dataset' }}
{% if dataset['name'] %}
* Dataset: {% if 'id' in dataset %}[{{ dataset['name'] }}](https://huggingface.co/datasets/{{ dataset['id'] }}){% else %}{{ dataset['name'] }}{% endif %}
{%- if 'revision' in dataset and 'id' in dataset %} at [{{ dataset['revision'][:7] }}](https://huggingface.co/datasets/{{ dataset['id'] }}/tree/{{ dataset['revision'] }}){% endif %}{% endif %}
{% if dataset['size'] %}* Size: {{ ""{:,}"".format(dataset['size']) }} {{ dataset_type }} samples
{% endif %}* Columns: {% if dataset['columns'] | length == 1 %}{{ dataset['columns'][0] }}{% elif dataset['columns'] | length == 2 %}{{ dataset['columns'][0] }} and {{ dataset['columns'][1] }}{% else %}{{ dataset['columns'][:-1] | join(', ') }}, and {{ dataset['columns'][-1] }}{% endif %}
{% if dataset['stats_table'] %}* Approximate statistics based on the first {{ [dataset['size'], 1000] | min }} samples:
{{ dataset['stats_table'] }}{% endif %}{% if dataset['examples_table'] %}* Samples:
{{ dataset['examples_table'] }}{% endif %}* Loss: {% if dataset[""loss""][""fullname""].startswith(""sentence_transformers."") %}[<code>{{ dataset[""loss""][""fullname""].split(""."")[-1] }}</code>](https://sbert.net/docs/package_reference/cross_encoder/losses.html#{{ dataset[""loss""][""fullname""].split(""."")[-1].lower() }}){% else %}<code>{{ dataset[""loss""][""fullname""] }}</code>{% endif %}{% if ""config_code"" in dataset[""loss""] %} with these parameters:
{{ dataset[""loss""][""config_code""] }}{% endif %}
{% if dataset_list | length > 3 %}</details>
{% endif %}{% endfor %}{% endif %}{% endfor -%}",reference
"## Training Details
{% for dataset_type, dataset_list in [(""training"", train_datasets), (""evaluation"", eval_datasets)] %}{% if dataset_list %}
### {{ dataset_type.title() }} Dataset{{""s"" if dataset_list | length > 1 else """"}}
{% for dataset in dataset_list %}{% if dataset_list | length > 3 %}<details><summary>{{ dataset['name'] or 'Unnamed Dataset' }}</summary>
{% endif %}
#### {{ dataset['name'] or 'Unnamed Dataset' }}
{% if dataset['name'] %}
* Dataset: {% if 'id' in dataset %}[{{ dataset['name'] }}](https://huggingface.co/datasets/{{ dataset['id'] }}){% else %}{{ dataset['name'] }}{% endif %}
{%- if 'revision' in dataset and 'id' in dataset %} at [{{ dataset['revision'][:7] }}](https://huggingface.co/datasets/{{ dataset['id'] }}/tree/{{ dataset['revision'] }}){% endif %}{% endif %}
{% if dataset['size'] %}* Size: {{ ""{:,}"".format(dataset['size']) }} {{ dataset_type }} samples
{% endif %}* Columns: {% if dataset['columns'] | length == 1 %}{{ dataset['columns'][0] }}{% elif dataset['columns'] | length == 2 %}{{ dataset['columns'][0] }} and {{ dataset['columns'][1] }}{% else %}{{ dataset['columns'][:-1] | join(', ') }}, and {{ dataset['columns'][-1] }}{% endif %}
{% if dataset['stats_table'] %}* Approximate statistics based on the first {{ [dataset['size'], 1000] | min }} samples:
{{ dataset['stats_table'] }}{% endif %}{% if dataset['examples_table'] %}* Samples:
{{ dataset['examples_table'] }}{% endif %}* Loss: {% if dataset[""loss""][""fullname""].startswith(""sentence_transformers."") %}[<code>{{ dataset[""loss""][""fullname""].split(""."")[-1] }}</code>](https://sbert.net/docs/package_reference/sentence_transformer/losses.html#{{ dataset[""loss""][""fullname""].split(""."")[-1].lower() }}){% else %}<code>{{ dataset[""loss""][""fullname""] }}</code>{% endif %}{% if ""config_code"" in dataset[""loss""] %} with these parameters:
{{ dataset[""loss""][""config_code""] }}{% endif %}
{% if dataset_list | length > 3 %}</details>
{% endif %}{% endfor %}{% endif %}{% endfor -%}",reference
"1. **Route** via the H2O classifier to select a subset of paragraphs.
     2. **Retrieve** top‑k relevant passages with FAISS.
     3. **Rescore** with a Bayesian posterior to boost high-confidence results.
     4. **Fallback**: if the posterior confidence is below threshold, generate an answer using a compact Flan‑T5 model.",reference
"# Or call the REST API
curl -X POST https://<your-space>.hf.space/query \
     -d '{""question"":""What is DeepSeek?""}'",reference
"<!-- If there is a paper or blog post introducing the model, the APA and Bibtex information for that should go in this section. -->",reference
"# Or call the REST API
curl -X POST https://<your-space>.hf.space/query \
     -d '{""question"":""What is DeepSeek?""}'",reference
### Downstream Use [optional],other
Update this directory using maint_tools/vendor_array_api_compat.sh,reference
"### Metrics
{% for metrics in eval_metrics %}
#### {{ metrics.description }}
{% if metrics.dataset_name %}
* Dataset{% if metrics.dataset_name is not string and metrics.dataset_name | length > 1 %}s{% endif %}: {% if metrics.dataset_name is string -%}
        `{{ metrics.dataset_name }}`
    {%- else -%}
        {%- for name in metrics.dataset_name -%}
            `{{ name }}`
            {%- if not loop.last -%}
                {%- if loop.index == metrics.dataset_name | length - 1 %} and {% else -%}, {% endif -%}
            {%- endif -%}
        {%- endfor -%}
    {%- endif -%}
{%- endif %}
* Evaluated with {% if metrics.class_name.startswith(""sentence_transformers."") %}[<code>{{ metrics.class_name.split(""."")[-1] }}</code>](https://sbert.net/docs/package_reference/sentence_transformer/evaluation.html#sentence_transformers.evaluation.{{ metrics.class_name.split(""."")[-1] }}){% else %}<code>{{ metrics.class_name }}</code>{% endif %}{% if metrics.config_code %} with these parameters:
{{ metrics.config_code }}{% endif %}",reference
First install the Sentence Transformers library:,installation
First install the Sentence Transformers library:,installation
"```bash
pip install -U sentence-transformers
```",installation
"### Installation
```bash
git clone https://github.com/RohitRajpoot/reposage.git
cd reposage
pip install -r requirements.txt",installation
"<!-- This section describes the source data (e.g. news text and headlines, social media posts, translated sentences, ...). -->",reference
### Source Data,other
## ⚙️ Installation,installation
"<!-- If there is a paper or blog post introducing the model, the APA and Bibtex information for that should go in this section. -->",reference
Update this directory using maint_tools/vendor_array_api_extra.sh,reference
### Model Sources,other
First install the Sentence Transformers library:,installation
<details><summary>Click to expand</summary>,usage
First install the Sentence Transformers library:,installation
<details><summary>Click to see the direct usage in Transformers</summary>,usage
Update this directory using maint_tools/vendor_array_api_compat.sh,reference
#### Summary,other
"{% for name, value in non_default_hyperparameters.items() %}- `{{ name }}`: {{ value }}
{% endfor %}{%- endif %}
#### All Hyperparameters
<details><summary>Click to expand</summary>",usage
"### Metrics
{% for metrics in eval_metrics %}
#### {{ metrics.description }}
{% if metrics.dataset_name %}
* Dataset{% if metrics.dataset_name is not string and metrics.dataset_name | length > 1 %}s{% endif %}: {% if metrics.dataset_name is string -%}
        `{{ metrics.dataset_name }}`
    {%- else -%}
        {%- for name in metrics.dataset_name -%}
            `{{ name }}`
            {%- if not loop.last -%}
                {%- if loop.index == metrics.dataset_name | length - 1 %} and {% else -%}, {% endif -%}
            {%- endif -%}
        {%- endfor -%}
    {%- endif -%}
{%- endif %}
* Evaluated with {% if metrics.class_name.startswith(""sentence_transformers."") %}[<code>{{ metrics.class_name.split(""."")[-1] }}</code>](https://sbert.net/docs/package_reference/cross_encoder/evaluation.html#sentence_transformers.cross_encoder.evaluation.{{ metrics.class_name.split(""."")[-1] }}){% else %}<code>{{ metrics.class_name }}</code>{% endif %}{% if metrics.config_code %} with these parameters:
{{ metrics.config_code }}{% endif %}",reference
"# Slide 3: Team Roles
- PM, Dev, AI Lead, DevOps",other
"# 2) Install dependencies
pip install --upgrade pip setuptools wheel
pip install -r requirements.txt",installation
"# Model Card for {{ model_id | default(""Model ID"", true) }}",other
"- **Hardware Type:** {{ hardware_type | default(""[More Information Needed]"", true)}}
- **Hours used:** {{ hours_used | default(""[More Information Needed]"", true)}}
- **Cloud Provider:** {{ cloud_provider | default(""[More Information Needed]"", true)}}
- **Compute Region:** {{ cloud_region | default(""[More Information Needed]"", true)}}
- **Carbon Emitted:** {{ co2_emitted | default(""[More Information Needed]"", true)}}",other
Update this directory using maint_tools/vendor_array_api_compat.sh,reference
"## Training Details
{% for dataset_type, dataset_list in [(""training"", train_datasets), (""evaluation"", eval_datasets)] %}{% if dataset_list %}
### {{ dataset_type.title() }} Dataset{{""s"" if dataset_list | length > 1 else """"}}
{% for dataset in dataset_list %}{% if dataset_list | length > 3 %}<details><summary>{{ dataset['name'] or 'Unnamed Dataset' }}</summary>
{% endif %}
#### {{ dataset['name'] or 'Unnamed Dataset' }}
{% if dataset['name'] %}
* Dataset: {% if 'id' in dataset %}[{{ dataset['name'] }}](https://huggingface.co/datasets/{{ dataset['id'] }}){% else %}{{ dataset['name'] }}{% endif %}
{%- if 'revision' in dataset and 'id' in dataset %} at [{{ dataset['revision'][:7] }}](https://huggingface.co/datasets/{{ dataset['id'] }}/tree/{{ dataset['revision'] }}){% endif %}{% endif %}
{% if dataset['size'] %}* Size: {{ ""{:,}"".format(dataset['size']) }} {{ dataset_type }} samples
{% endif %}* Columns: {% if dataset['columns'] | length == 1 %}{{ dataset['columns'][0] }}{% elif dataset['columns'] | length == 2 %}{{ dataset['columns'][0] }} and {{ dataset['columns'][1] }}{% else %}{{ dataset['columns'][:-1] | join(', ') }}, and {{ dataset['columns'][-1] }}{% endif %}
{% if dataset['stats_table'] %}* Approximate statistics based on the first {{ [dataset['size'], 1000] | min }} samples:
{{ dataset['stats_table'] }}{% endif %}{% if dataset['examples_table'] %}* Samples:
{{ dataset['examples_table'] }}{% endif %}* Loss: {% if dataset[""loss""][""fullname""].startswith(""sentence_transformers."") %}[<code>{{ dataset[""loss""][""fullname""].split(""."")[-1] }}</code>](https://sbert.net/docs/package_reference/cross_encoder/losses.html#{{ dataset[""loss""][""fullname""].split(""."")[-1].lower() }}){% else %}<code>{{ dataset[""loss""][""fullname""] }}</code>{% endif %}{% if ""config_code"" in dataset[""loss""] %} with these parameters:
{{ dataset[""loss""][""config_code""] }}{% endif %}
{% if dataset_list | length > 3 %}</details>
{% endif %}{% endfor %}{% endif %}{% endfor -%}",reference
"# 2) Install dependencies
pip install --upgrade pip setuptools wheel
pip install -r requirements.txt",installation
"# Or call the REST API
curl -X POST https://<your-space>.hf.space/query \
     -d '{""question"":""What is DeepSeek?""}'",reference
"### Installation
```bash
git clone https://github.com/RohitRajpoot/reposage.git
cd reposage
pip install -r requirements.txt",installation
"```bash
pip install -U sentence-transformers
```",installation
"{%- if eval_lines %}
### Training Logs
{% if hide_eval_lines %}<details><summary>Click to expand</summary>",usage
"## Features
- **DeepSeek Index** of lecture slides, code, and readings  
- **Bayesian Posterior Scoring** for precise answer selection  
- **Transformer Fallback** via HF text2text when confidence is low  
- **CLI & REST API** for local and web deployment  
- **CI/CD** via GitHub Actions → Hugging Face Spaces",usage
"**What it is:**  
- A command-line & web demo (via Streamlit) that shows:
  1. **Embedding Q&A**: nearest‐neighbor lookup in a trained token embedding (`assist/chat.py`)  
  2. **Bayesian Q&A**: frequency‐based “co-occurrence” embedding lookup (`assist/bayes_chat.py`)  
  3. **Transformer Demo**: single‐block transformer next‐token prediction (`assist/transformer_demo.py`)  
  4. **DeepSeek-R1**: calls to a 1.3B-parameter model for generative Q&A (wrapped to skip gracefully in Colab)",other
"{%- if eval_lines %}
### Training Logs
{% if hide_eval_lines %}<details><summary>Click to expand</summary>",usage
"# 3) Smoke-test CLI
python -m assist.main chat ""hello world""",usage
## ⚙️ Installation,installation
"{{ citation_apa | default(""[More Information Needed]"", true)}}",other
"### Installation
```bash
git clone https://github.com/RohitRajpoot/reposage.git
cd reposage
pip install -r requirements.txt",installation
"```bash
pip install -U sentence-transformers
```",installation
"## Features
- **DeepSeek Index** of lecture slides, code, and readings  
- **Bayesian Posterior Scoring** for precise answer selection  
- **Transformer Fallback** via HF text2text when confidence is low  
- **CLI & REST API** for local and web deployment  
- **CI/CD** via GitHub Actions → Hugging Face Spaces",usage
"# Query
reposage query ""What is DeepSeek?"" --threshold 0.3",usage
<details><summary>Click to see the direct usage in Transformers</summary>,usage
"## Training Details
{% for dataset_type, dataset_list in [(""training"", train_datasets), (""evaluation"", eval_datasets)] %}{% if dataset_list %}
### {{ dataset_type.title() }} Dataset{{""s"" if dataset_list | length > 1 else """"}}
{% for dataset in dataset_list %}{% if dataset_list | length > 3 %}<details><summary>{{ dataset['name'] or 'Unnamed Dataset' }}</summary>
{% endif %}
#### {{ dataset['name'] or 'Unnamed Dataset' }}
{% if dataset['name'] %}
* Dataset: {% if 'id' in dataset %}[{{ dataset['name'] }}](https://huggingface.co/datasets/{{ dataset['id'] }}){% else %}{{ dataset['name'] }}{% endif %}
{%- if 'revision' in dataset and 'id' in dataset %} at [{{ dataset['revision'][:7] }}](https://huggingface.co/datasets/{{ dataset['id'] }}/tree/{{ dataset['revision'] }}){% endif %}{% endif %}
{% if dataset['size'] %}* Size: {{ ""{:,}"".format(dataset['size']) }} {{ dataset_type }} samples
{% endif %}* Columns: {% if dataset['columns'] | length == 1 %}{{ dataset['columns'][0] }}{% elif dataset['columns'] | length == 2 %}{{ dataset['columns'][0] }} and {{ dataset['columns'][1] }}{% else %}{{ dataset['columns'][:-1] | join(', ') }}, and {{ dataset['columns'][-1] }}{% endif %}
{% if dataset['stats_table'] %}* Approximate statistics based on the first {{ [dataset['size'], 1000] | min }} samples:
{{ dataset['stats_table'] }}{% endif %}{% if dataset['examples_table'] %}* Samples:
{{ dataset['examples_table'] }}{% endif %}* Loss: {% if dataset[""loss""][""fullname""].startswith(""sentence_transformers."") %}[<code>{{ dataset[""loss""][""fullname""].split(""."")[-1] }}</code>](https://sbert.net/docs/package_reference/sentence_transformer/losses.html#{{ dataset[""loss""][""fullname""].split(""."")[-1].lower() }}){% else %}<code>{{ dataset[""loss""][""fullname""] }}</code>{% endif %}{% if ""config_code"" in dataset[""loss""] %} with these parameters:
{{ dataset[""loss""][""config_code""] }}{% endif %}
{% if dataset_list | length > 3 %}</details>
{% endif %}{% endfor %}{% endif %}{% endfor -%}",reference
"{{ dataset_summary | default("""", true) }}",other
"<!-- If there is a paper or blog post introducing the dataset, the APA and Bibtex information for that should go in this section. -->",reference
First install the Sentence Transformers library:,installation
"# Build the index
reposage index",usage
"- **Documentation:** [Sentence Transformers Documentation](https://sbert.net)
- **Repository:** [Sentence Transformers on GitHub](https://github.com/UKPLab/sentence-transformers)
- **Hugging Face:** [Sentence Transformers on Hugging Face](https://huggingface.co/models?library=sentence-transformers)",other
"{{ model_description | default("""", true) }}",other
<details><summary>Click to expand</summary>,usage
<details><summary>Click to expand</summary>,usage
"{% for name, value in non_default_hyperparameters.items() %}- `{{ name }}`: {{ value }}
{% endfor %}{%- endif %}
#### All Hyperparameters
<details><summary>Click to expand</summary>",usage
### Model Sources [optional],other
<details><summary>Click to see the direct usage in Transformers</summary>,usage
"{{ citation_bibtex | default(""[More Information Needed]"", true)}}",other
"### Usage
# Build the index
reposage index",usage
"{{ metrics.table }}
{%- endfor %}{% endif %}
<!--
## Bias, Risks and Limitations",other
"| Team Member | Role(s)                 | Responsibilities                             |
|-------------|-------------------------|----------------------------------------------|
| Rohit       | CEO / Product Lead,     | • Define product vision & roadmap<br>• Manage scope & stakeholder communication<br>• Write technical documentation |
| Kishan      | CTO / DevOps Lead,      | • Develop DeepSeek index & CLI scaffold<br>• Set up CI/CD pipelines<br>• Manage Hugging Face Spaces deployments   |
| Maitri      | Data Scientist / AI Lead| • Implement Bayesian Q&A layer<br>• Design & tune priors<br>• Oversee transformer fallback prompts          |",usage
"# Query
reposage query ""What is DeepSeek?"" --threshold 0.3",usage
"<!-- State whether the dataset contains data that might be considered personal, sensitive, or private (e.g., data that reveals addresses, uniquely identifiable names or aliases, racial or ethnic origins, sexual orientations, religious beliefs, political opinions, financial or health data, etc.). If efforts were made to anonymize the data, describe the anonymization process. -->",other
<details><summary>Click to expand</summary>,usage
"# Or call the REST API
curl -X POST https://<your-space>.hf.space/query \
     -d '{""question"":""What is DeepSeek?""}'",reference
Update this directory using maint_tools/vendor_array_api_compat.sh,reference
## 🤝 Contributing,other
<details><summary>Click to see the direct usage in Transformers</summary>,usage
"# 3) Smoke-test CLI
python -m assist.main chat ""hello world""",usage
"## Features
- **DeepSeek Index** of lecture slides, code, and readings  
- **Bayesian Posterior Scoring** for precise answer selection  
- **Transformer Fallback** via HF text2text when confidence is low  
- **CLI & REST API** for local and web deployment  
- **CI/CD** via GitHub Actions → Hugging Face Spaces",usage
Update this directory using maint_tools/vendor_array_api_compat.sh,reference
"## Features
- **DeepSeek Index** of lecture slides, code, and readings  
- **Bayesian Posterior Scoring** for precise answer selection  
- **Transformer Fallback** via HF text2text when confidence is low  
- **CLI & REST API** for local and web deployment  
- **CI/CD** via GitHub Actions → Hugging Face Spaces",usage
"# Slide 2: Project Goals
- Traceable, transparent AI engineering
- Real-world deployment to Hugging Face Spaces
- Continuous integration & sprint metrics",other
"# Query
reposage query ""What is DeepSeek?"" --threshold 0.3",usage
"### Testing Data, Factors & Metrics",other
"{{ model_summary | default("""", true) }}",other
"{% for name, value in non_default_hyperparameters.items() %}- `{{ name }}`: {{ value }}
{% endfor %}{%- endif %}
#### All Hyperparameters
<details><summary>Click to expand</summary>",usage
"### Installation
```bash
git clone https://github.com/RohitRajpoot/reposage.git
cd reposage
pip install -r requirements.txt",installation
"<!-- If there is a paper or blog post introducing the model, the APA and Bibtex information for that should go in this section. -->",reference
First install the Sentence Transformers library:,installation
"{%- if eval_lines %}
### Training Logs
{% if hide_eval_lines %}<details><summary>Click to expand</summary>",usage
"```bash
pip install -U sentence-transformers
```",installation
"### Usage
# Build the index
reposage index",usage
"{% for name, value in non_default_hyperparameters.items() %}- `{{ name }}`: {{ value }}
{% endfor %}{%- endif %}
#### All Hyperparameters
<details><summary>Click to expand</summary>",usage
"{{ get_started_code | default(""[More Information Needed]"", true)}}",other
"{%- if eval_lines %}
### Training Logs
{% if hide_eval_lines %}<details><summary>Click to expand</summary>",usage
"### Model Description
- **Model Type:** Sentence Transformer
{% if base_model -%}
    {%- if base_model_revision -%}
    - **Base model:** [{{ base_model }}](https://huggingface.co/{{ base_model }}) <!-- at revision {{ base_model_revision }} -->
    {%- else -%}
    - **Base model:** [{{ base_model }}](https://huggingface.co/{{ base_model }})
    {%- endif -%}
{%- else -%}
    <!-- - **Base model:** [Unknown](https://huggingface.co/unknown) -->
{%- endif %}
- **Maximum Sequence Length:** {{ model_max_length }} tokens
- **Output Dimensionality:** {{ output_dimensionality }} dimensions
- **Similarity Function:** {{ similarity_fn_name }}
{% if train_datasets | selectattr(""name"") | list -%}
    - **Training Dataset{{""s"" if train_datasets | selectattr(""name"") | list | length > 1 else """"}}:**
    {%- for dataset in (train_datasets | selectattr(""name"")) %}
        {%- if dataset.id %}
    - [{{ dataset.name if dataset.name else dataset.id }}](https://huggingface.co/datasets/{{ dataset.id }})
        {%- else %}
    - {{ dataset.name }}
        {%- endif %}
    {%- endfor %}
{%- else -%}
    <!-- - **Training Dataset:** Unknown -->
{%- endif %}
{% if language -%}
    - **Language{{""s"" if language is not string and language | length > 1 else """"}}:**
    {%- if language is string %} {{ language }}
    {%- else %} {% for lang in language -%}
            {{ lang }}{{ "", "" if not loop.last else """" }}
        {%- endfor %}
    {%- endif %}
{%- else -%}
    <!-- - **Language:** Unknown -->
{%- endif %}
{% if license -%}
    - **License:** {{ license }}
{%- else -%}
    <!-- - **License:** Unknown -->
{%- endif %}",other
### Direct Use,other
"```bash
pip install -U sentence-transformers
```",installation
"1. **Route** via the H2O classifier to select a subset of paragraphs.
     2. **Retrieve** top‑k relevant passages with FAISS.
     3. **Rescore** with a Bayesian posterior to boost high-confidence results.
     4. **Fallback**: if the posterior confidence is below threshold, generate an answer using a compact Flan‑T5 model.",reference
"# 3) Smoke-test CLI
python -m assist.main chat ""hello world""",usage
"{{ annotation_process_section | default(""[More Information Needed]"", true)}}",other
"<!-- If there is a paper or blog post introducing the model, the APA and Bibtex information for that should go in this section. -->",reference
"```bash
pip install -U sentence-transformers
```",installation
"{% for name, value in non_default_hyperparameters.items() %}- `{{ name }}`: {{ value }}
{% endfor %}{%- endif %}
#### All Hyperparameters
<details><summary>Click to expand</summary>",usage
"1. Label paragraphs in `data/classification.csv`.
2. Train or fine-tune the H2O classifier (`scripts/train_classifier.py`).
3. Integrate the routing model into your Streamlit UI so that every query is first classified, then answered from the right subset.",other
"## Approach
1. **System prompt**:  
   > You are RepoSage, an AI tutor. Use the indexed materials to answer student questions.",other
## ⚙️ Installation,installation
"In `app.py`, configure paths to:",reference
"Unless required by applicable law or agreed to in writing,
  software distributed under the License is distributed on an
  ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
  KIND, either express or implied.  See the License for the
  specific language governing permissions and limitations
  under the License.
-->",other
<details><summary>Click to see the direct usage in Transformers</summary>,usage
"# Or call the REST API
curl -X POST https://<your-space>.hf.space/query \
     -d '{""question"":""What is DeepSeek?""}'",reference
"# 2) Install dependencies
pip install --upgrade pip setuptools wheel
pip install -r requirements.txt",installation
Update this directory using maint_tools/vendor_array_api_compat.sh,reference
"| Team Member | Role(s)                 | Responsibilities                             |
|-------------|-------------------------|----------------------------------------------|
| Rohit       | CEO / Product Lead,     | • Define product vision & roadmap<br>• Manage scope & stakeholder communication<br>• Write technical documentation |
| Kishan      | CTO / DevOps Lead,      | • Develop DeepSeek index & CLI scaffold<br>• Set up CI/CD pipelines<br>• Manage Hugging Face Spaces deployments   |
| Maitri      | Data Scientist / AI Lead| • Implement Bayesian Q&A layer<br>• Design & tune priors<br>• Oversee transformer fallback prompts          |",usage
<!-- Provide the basic links for the model. -->,other
"### Usage
# Build the index
reposage index",usage
"### Metrics
{% for metrics in eval_metrics %}
#### {{ metrics.description }}
{% if metrics.dataset_name %}
* Dataset{% if metrics.dataset_name is not string and metrics.dataset_name | length > 1 %}s{% endif %}: {% if metrics.dataset_name is string -%}
        `{{ metrics.dataset_name }}`
    {%- else -%}
        {%- for name in metrics.dataset_name -%}
            `{{ name }}`
            {%- if not loop.last -%}
                {%- if loop.index == metrics.dataset_name | length - 1 %} and {% else -%}, {% endif -%}
            {%- endif -%}
        {%- endfor -%}
    {%- endif -%}
{%- endif %}
* Evaluated with {% if metrics.class_name.startswith(""sentence_transformers."") %}[<code>{{ metrics.class_name.split(""."")[-1] }}</code>](https://sbert.net/docs/package_reference/cross_encoder/evaluation.html#sentence_transformers.cross_encoder.evaluation.{{ metrics.class_name.split(""."")[-1] }}){% else %}<code>{{ metrics.class_name }}</code>{% endif %}{% if metrics.config_code %} with these parameters:
{{ metrics.config_code }}{% endif %}",reference
"* After labeling paragraphs (e.g. `installation`, `usage`, `configuration`, `api_reference`, `troubleshooting`), train an H2O AutoML model to predict the category of any incoming question.
   * This routing step ensures each query is answered by the most relevant section of your docs.",installation
"1. Fork and create a feature branch.
2. Add or update scripts/tests as needed.
3. Ensure style checks (`flake8`, `black`) pass.
4. Submit a pull request detailing your enhancements.",reference
## Model Details,other
"# Slide 3: Bayesian Q&A Layer
- Cosine-likelihood softmax → posterior",reference
"```bash
pip install -U sentence-transformers
```",installation
Update this directory using maint_tools/vendor_array_api_compat.sh,reference
"{% for name, value in non_default_hyperparameters.items() %}- `{{ name }}`: {{ value }}
{% endfor %}{%- endif %}
#### All Hyperparameters
<details><summary>Click to expand</summary>",usage
"```bash
pip install -U sentence-transformers
```",installation
"```bash
pip install -U sentence-transformers
```",installation
"# Slide 1: Architecture Overview
- User → RepoSage CLI → index/query modules",usage
"### Metrics
{% for metrics in eval_metrics %}
#### {{ metrics.description }}
{% if metrics.dataset_name %}
* Dataset{% if metrics.dataset_name is not string and metrics.dataset_name | length > 1 %}s{% endif %}: {% if metrics.dataset_name is string -%}
        `{{ metrics.dataset_name }}`
    {%- else -%}
        {%- for name in metrics.dataset_name -%}
            `{{ name }}`
            {%- if not loop.last -%}
                {%- if loop.index == metrics.dataset_name | length - 1 %} and {% else -%}, {% endif -%}
            {%- endif -%}
        {%- endfor -%}
    {%- endif -%}
{%- endif %}
* Evaluated with {% if metrics.class_name.startswith(""sentence_transformers."") %}[<code>{{ metrics.class_name.split(""."")[-1] }}</code>](https://sbert.net/docs/package_reference/cross_encoder/evaluation.html#sentence_transformers.cross_encoder.evaluation.{{ metrics.class_name.split(""."")[-1] }}){% else %}<code>{{ metrics.class_name }}</code>{% endif %}{% if metrics.config_code %} with these parameters:
{{ metrics.config_code }}{% endif %}",reference
<details><summary>Click to see the direct usage in Transformers</summary>,usage
## Evaluation,other
#### Hardware,other
"```bash
pip install -U sentence-transformers
```",installation
## ⚙️ Installation,installation
"{% for name, value in non_default_hyperparameters.items() %}- `{{ name }}`: {{ value }}
{% endfor %}{%- endif %}
#### All Hyperparameters
<details><summary>Click to expand</summary>",usage
"# Query
reposage query ""What is DeepSeek?"" --threshold 0.3",usage
"<!-- This section describes the source data (e.g. news text and headlines, social media posts, translated sentences, ...). -->",reference
"## Training Details
{% for dataset_type, dataset_list in [(""training"", train_datasets), (""evaluation"", eval_datasets)] %}{% if dataset_list %}
### {{ dataset_type.title() }} Dataset{{""s"" if dataset_list | length > 1 else """"}}
{% for dataset in dataset_list %}{% if dataset_list | length > 3 %}<details><summary>{{ dataset['name'] or 'Unnamed Dataset' }}</summary>
{% endif %}
#### {{ dataset['name'] or 'Unnamed Dataset' }}
{% if dataset['name'] %}
* Dataset: {% if 'id' in dataset %}[{{ dataset['name'] }}](https://huggingface.co/datasets/{{ dataset['id'] }}){% else %}{{ dataset['name'] }}{% endif %}
{%- if 'revision' in dataset and 'id' in dataset %} at [{{ dataset['revision'][:7] }}](https://huggingface.co/datasets/{{ dataset['id'] }}/tree/{{ dataset['revision'] }}){% endif %}{% endif %}
{% if dataset['size'] %}* Size: {{ ""{:,}"".format(dataset['size']) }} {{ dataset_type }} samples
{% endif %}* Columns: {% if dataset['columns'] | length == 1 %}{{ dataset['columns'][0] }}{% elif dataset['columns'] | length == 2 %}{{ dataset['columns'][0] }} and {{ dataset['columns'][1] }}{% else %}{{ dataset['columns'][:-1] | join(', ') }}, and {{ dataset['columns'][-1] }}{% endif %}
{% if dataset['stats_table'] %}* Approximate statistics based on the first {{ [dataset['size'], 1000] | min }} samples:
{{ dataset['stats_table'] }}{% endif %}{% if dataset['examples_table'] %}* Samples:
{{ dataset['examples_table'] }}{% endif %}* Loss: {% if dataset[""loss""][""fullname""].startswith(""sentence_transformers."") %}[<code>{{ dataset[""loss""][""fullname""].split(""."")[-1] }}</code>](https://sbert.net/docs/package_reference/cross_encoder/losses.html#{{ dataset[""loss""][""fullname""].split(""."")[-1].lower() }}){% else %}<code>{{ dataset[""loss""][""fullname""] }}</code>{% endif %}{% if ""config_code"" in dataset[""loss""] %} with these parameters:
{{ dataset[""loss""][""config_code""] }}{% endif %}
{% if dataset_list | length > 3 %}</details>
{% endif %}{% endfor %}{% endif %}{% endfor -%}",reference
"{% for name, value in non_default_hyperparameters.items() %}- `{{ name }}`: {{ value }}
{% endfor %}{%- endif %}
#### All Hyperparameters
<details><summary>Click to expand</summary>",usage
"## Training Details
{% for dataset_type, dataset_list in [(""training"", train_datasets), (""evaluation"", eval_datasets)] %}{% if dataset_list %}
### {{ dataset_type.title() }} Dataset{{""s"" if dataset_list | length > 1 else """"}}
{% for dataset in dataset_list %}{% if dataset_list | length > 3 %}<details><summary>{{ dataset['name'] or 'Unnamed Dataset' }}</summary>
{% endif %}
#### {{ dataset['name'] or 'Unnamed Dataset' }}
{% if dataset['name'] %}
* Dataset: {% if 'id' in dataset %}[{{ dataset['name'] }}](https://huggingface.co/datasets/{{ dataset['id'] }}){% else %}{{ dataset['name'] }}{% endif %}
{%- if 'revision' in dataset and 'id' in dataset %} at [{{ dataset['revision'][:7] }}](https://huggingface.co/datasets/{{ dataset['id'] }}/tree/{{ dataset['revision'] }}){% endif %}{% endif %}
{% if dataset['size'] %}* Size: {{ ""{:,}"".format(dataset['size']) }} {{ dataset_type }} samples
{% endif %}* Columns: {% if dataset['columns'] | length == 1 %}{{ dataset['columns'][0] }}{% elif dataset['columns'] | length == 2 %}{{ dataset['columns'][0] }} and {{ dataset['columns'][1] }}{% else %}{{ dataset['columns'][:-1] | join(', ') }}, and {{ dataset['columns'][-1] }}{% endif %}
{% if dataset['stats_table'] %}* Approximate statistics based on the first {{ [dataset['size'], 1000] | min }} samples:
{{ dataset['stats_table'] }}{% endif %}{% if dataset['examples_table'] %}* Samples:
{{ dataset['examples_table'] }}{% endif %}* Loss: {% if dataset[""loss""][""fullname""].startswith(""sentence_transformers."") %}[<code>{{ dataset[""loss""][""fullname""].split(""."")[-1] }}</code>](https://sbert.net/docs/package_reference/sentence_transformer/losses.html#{{ dataset[""loss""][""fullname""].split(""."")[-1].lower() }}){% else %}<code>{{ dataset[""loss""][""fullname""] }}</code>{% endif %}{% if ""config_code"" in dataset[""loss""] %} with these parameters:
{{ dataset[""loss""][""config_code""] }}{% endif %}
{% if dataset_list | length > 3 %}</details>
{% endif %}{% endfor %}{% endif %}{% endfor -%}",reference
"1. **Route** via the H2O classifier to select a subset of paragraphs.
     2. **Retrieve** top‑k relevant passages with FAISS.
     3. **Rescore** with a Bayesian posterior to boost high-confidence results.
     4. **Fallback**: if the posterior confidence is below threshold, generate an answer using a compact Flan‑T5 model.",reference
"{{ more_information | default(""[More Information Needed]"", true)}}",other
# Role Breakdown,other
"* After labeling paragraphs (e.g. `installation`, `usage`, `configuration`, `api_reference`, `troubleshooting`), train an H2O AutoML model to predict the category of any incoming question.
   * This routing step ensures each query is answered by the most relevant section of your docs.",installation
"* After labeling paragraphs (e.g. `installation`, `usage`, `configuration`, `api_reference`, `troubleshooting`), train an H2O AutoML model to predict the category of any incoming question.
   * This routing step ensures each query is answered by the most relevant section of your docs.",installation
"<!-- This section describes the source data (e.g. news text and headlines, social media posts, translated sentences, ...). -->",reference
Run locally:,other
"# Or rank different texts based on similarity to a single text
ranks = model.rank(
    {{ ""%r"" | format(predict_example[0][0] if predict_example else ""How many calories in an egg"") }},
    [
{%- for pair in (predict_example or [[""How many calories in an egg"", ""There are on average between 55 and 80 calories in an egg depending on its size.""], [""How many calories in an egg"", ""Egg whites are very low in calories, have no fat, no cholesterol, and are loaded with protein.""], [""How many calories in an egg"", ""Most of the calories in an egg come from the yellow yolk in the center.""]]) %}
        {{ ""%r"" | format(pair[1]) }},
{%- endfor %}
    ]
)
# [{'corpus_id': ..., 'score': ...}, {'corpus_id': ..., 'score': ...}, ...]{% endif %}
```",other
<details><summary>Click to expand</summary>,usage
"* After labeling paragraphs (e.g. `installation`, `usage`, `configuration`, `api_reference`, `troubleshooting`), train an H2O AutoML model to predict the category of any incoming question.
   * This routing step ensures each query is answered by the most relevant section of your docs.",installation
<details><summary>Click to expand</summary>,usage
"### Installation
```bash
git clone https://github.com/RohitRajpoot/reposage.git
cd reposage
pip install -r requirements.txt",installation
"# 2) Install dependencies
pip install --upgrade pip setuptools wheel
pip install -r requirements.txt",installation
"{{ speeds_sizes_times | default(""[More Information Needed]"", true)}}",other
"<!-- If there is a paper or blog post introducing the dataset, the APA and Bibtex information for that should go in this section. -->",reference
"# Or call the REST API
curl -X POST https://<your-space>.hf.space/query \
     -d '{""question"":""What is DeepSeek?""}'",reference
"```bash
pip install -U sentence-transformers
```",installation
"# 3) Smoke-test CLI
python -m assist.main chat ""hello world""",usage
"1. **Route** via the H2O classifier to select a subset of paragraphs.
     2. **Retrieve** top‑k relevant passages with FAISS.
     3. **Rescore** with a Bayesian posterior to boost high-confidence results.
     4. **Fallback**: if the posterior confidence is below threshold, generate an answer using a compact Flan‑T5 model.",reference
"### Installation
```bash
git clone https://github.com/RohitRajpoot/reposage.git
cd reposage
pip install -r requirements.txt",installation
"## Training Details
{% for dataset_type, dataset_list in [(""training"", train_datasets), (""evaluation"", eval_datasets)] %}{% if dataset_list %}
### {{ dataset_type.title() }} Dataset{{""s"" if dataset_list | length > 1 else """"}}
{% for dataset in dataset_list %}{% if dataset_list | length > 3 %}<details><summary>{{ dataset['name'] or 'Unnamed Dataset' }}</summary>
{% endif %}
#### {{ dataset['name'] or 'Unnamed Dataset' }}
{% if dataset['name'] %}
* Dataset: {% if 'id' in dataset %}[{{ dataset['name'] }}](https://huggingface.co/datasets/{{ dataset['id'] }}){% else %}{{ dataset['name'] }}{% endif %}
{%- if 'revision' in dataset and 'id' in dataset %} at [{{ dataset['revision'][:7] }}](https://huggingface.co/datasets/{{ dataset['id'] }}/tree/{{ dataset['revision'] }}){% endif %}{% endif %}
{% if dataset['size'] %}* Size: {{ ""{:,}"".format(dataset['size']) }} {{ dataset_type }} samples
{% endif %}* Columns: {% if dataset['columns'] | length == 1 %}{{ dataset['columns'][0] }}{% elif dataset['columns'] | length == 2 %}{{ dataset['columns'][0] }} and {{ dataset['columns'][1] }}{% else %}{{ dataset['columns'][:-1] | join(', ') }}, and {{ dataset['columns'][-1] }}{% endif %}
{% if dataset['stats_table'] %}* Approximate statistics based on the first {{ [dataset['size'], 1000] | min }} samples:
{{ dataset['stats_table'] }}{% endif %}{% if dataset['examples_table'] %}* Samples:
{{ dataset['examples_table'] }}{% endif %}* Loss: {% if dataset[""loss""][""fullname""].startswith(""sentence_transformers."") %}[<code>{{ dataset[""loss""][""fullname""].split(""."")[-1] }}</code>](https://sbert.net/docs/package_reference/sentence_transformer/losses.html#{{ dataset[""loss""][""fullname""].split(""."")[-1].lower() }}){% else %}<code>{{ dataset[""loss""][""fullname""] }}</code>{% endif %}{% if ""config_code"" in dataset[""loss""] %} with these parameters:
{{ dataset[""loss""][""config_code""] }}{% endif %}
{% if dataset_list | length > 3 %}</details>
{% endif %}{% endfor %}{% endif %}{% endfor -%}",reference
## ⚙️ Installation,installation
<details><summary>Click to expand</summary>,usage
#### Who are the source data producers?,other
"| Team Member | Role(s)                 | Responsibilities                             |
|-------------|-------------------------|----------------------------------------------|
| Rohit       | CEO / Product Lead,     | • Define product vision & roadmap<br>• Manage scope & stakeholder communication<br>• Write technical documentation |
| Kishan      | CTO / DevOps Lead,      | • Develop DeepSeek index & CLI scaffold<br>• Set up CI/CD pipelines<br>• Manage Hugging Face Spaces deployments   |
| Maitri      | Data Scientist / AI Lead| • Implement Bayesian Q&A layer<br>• Design & tune priors<br>• Oversee transformer fallback prompts          |",usage
"*Lists the people who create the model card, providing recognition and accountability for the detailed work that goes into its construction.*
-->",other
"```bash
pip install -U sentence-transformers
```",installation
"*What are the known or foreseeable issues stemming from this model? You could also flag here known failure cases or weaknesses of the model.*
-->",other
"* After labeling paragraphs (e.g. `installation`, `usage`, `configuration`, `api_reference`, `troubleshooting`), train an H2O AutoML model to predict the category of any incoming question.
   * This routing step ensures each query is answered by the most relevant section of your docs.",installation
"The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.",other
First install the Sentence Transformers library:,installation
"### Metrics
{% for metrics in eval_metrics %}
#### {{ metrics.description }}
{% if metrics.dataset_name %}
* Dataset{% if metrics.dataset_name is not string and metrics.dataset_name | length > 1 %}s{% endif %}: {% if metrics.dataset_name is string -%}
        `{{ metrics.dataset_name }}`
    {%- else -%}
        {%- for name in metrics.dataset_name -%}
            `{{ name }}`
            {%- if not loop.last -%}
                {%- if loop.index == metrics.dataset_name | length - 1 %} and {% else -%}, {% endif -%}
            {%- endif -%}
        {%- endfor -%}
    {%- endif -%}
{%- endif %}
* Evaluated with {% if metrics.class_name.startswith(""sentence_transformers."") %}[<code>{{ metrics.class_name.split(""."")[-1] }}</code>](https://sbert.net/docs/package_reference/sentence_transformer/evaluation.html#sentence_transformers.evaluation.{{ metrics.class_name.split(""."")[-1] }}){% else %}<code>{{ metrics.class_name }}</code>{% endif %}{% if metrics.config_code %} with these parameters:
{{ metrics.config_code }}{% endif %}",reference
"### Metrics
{% for metrics in eval_metrics %}
#### {{ metrics.description }}
{% if metrics.dataset_name %}
* Dataset{% if metrics.dataset_name is not string and metrics.dataset_name | length > 1 %}s{% endif %}: {% if metrics.dataset_name is string -%}
        `{{ metrics.dataset_name }}`
    {%- else -%}
        {%- for name in metrics.dataset_name -%}
            `{{ name }}`
            {%- if not loop.last -%}
                {%- if loop.index == metrics.dataset_name | length - 1 %} and {% else -%}, {% endif -%}
            {%- endif -%}
        {%- endfor -%}
    {%- endif -%}
{%- endif %}
* Evaluated with {% if metrics.class_name.startswith(""sentence_transformers."") %}[<code>{{ metrics.class_name.split(""."")[-1] }}</code>](https://sbert.net/docs/package_reference/sentence_transformer/evaluation.html#sentence_transformers.evaluation.{{ metrics.class_name.split(""."")[-1] }}){% else %}<code>{{ metrics.class_name }}</code>{% endif %}{% if metrics.config_code %} with these parameters:
{{ metrics.config_code }}{% endif %}",reference
You can finetune this model on your own dataset.,other
"* After labeling paragraphs (e.g. `installation`, `usage`, `configuration`, `api_reference`, `troubleshooting`), train an H2O AutoML model to predict the category of any incoming question.
   * This routing step ensures each query is answered by the most relevant section of your docs.",installation
"# 2) Install dependencies
pip install --upgrade pip setuptools wheel
pip install -r requirements.txt",installation
"1. **Route** via the H2O classifier to select a subset of paragraphs.
     2. **Retrieve** top‑k relevant passages with FAISS.
     3. **Rescore** with a Bayesian posterior to boost high-confidence results.
     4. **Fallback**: if the posterior confidence is below threshold, generate an answer using a compact Flan‑T5 model.",reference
"### Installation
```bash
git clone https://github.com/RohitRajpoot/reposage.git
cd reposage
pip install -r requirements.txt",installation
"1. Fork and create a feature branch.
2. Add or update scripts/tests as needed.
3. Ensure style checks (`flake8`, `black`) pass.
4. Submit a pull request detailing your enhancements.",reference
"<!-- This section describes the source data (e.g. news text and headlines, social media posts, translated sentences, ...). -->",reference
First install the Sentence Transformers library:,installation
"# 3) Smoke-test CLI
python -m assist.main chat ""hello world""",usage
<!-- Provide the basic links for the dataset. -->,other
"THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS"" AND
ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED
WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE
DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR
ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES
(INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;
LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON
ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
(INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.",other
"{% for name, value in non_default_hyperparameters.items() %}- `{{ name }}`: {{ value }}
{% endfor %}{%- endif %}
#### All Hyperparameters
<details><summary>Click to expand</summary>",usage
## Contributors,other
Update this directory using maint_tools/vendor_array_api_compat.sh,reference
First install the Sentence Transformers library:,installation
<details><summary>Click to see the direct usage in Transformers</summary>,usage
"# Slide 3: Bayesian Q&A Layer
- Cosine-likelihood softmax → posterior",reference
"<!-- This section describes the source data (e.g. news text and headlines, social media posts, translated sentences, ...). -->",reference
"```bash
pip install -U sentence-transformers
```",installation
"1. Fork and create a feature branch.
2. Add or update scripts/tests as needed.
3. Ensure style checks (`flake8`, `black`) pass.
4. Submit a pull request detailing your enhancements.",reference
First install the Sentence Transformers library:,installation
"## Training Details
{% for dataset_type, dataset_list in [(""training"", train_datasets), (""evaluation"", eval_datasets)] %}{% if dataset_list %}
### {{ dataset_type.title() }} Dataset{{""s"" if dataset_list | length > 1 else """"}}
{% for dataset in dataset_list %}{% if dataset_list | length > 3 %}<details><summary>{{ dataset['name'] or 'Unnamed Dataset' }}</summary>
{% endif %}
#### {{ dataset['name'] or 'Unnamed Dataset' }}
{% if dataset['name'] %}
* Dataset: {% if 'id' in dataset %}[{{ dataset['name'] }}](https://huggingface.co/datasets/{{ dataset['id'] }}){% else %}{{ dataset['name'] }}{% endif %}
{%- if 'revision' in dataset and 'id' in dataset %} at [{{ dataset['revision'][:7] }}](https://huggingface.co/datasets/{{ dataset['id'] }}/tree/{{ dataset['revision'] }}){% endif %}{% endif %}
{% if dataset['size'] %}* Size: {{ ""{:,}"".format(dataset['size']) }} {{ dataset_type }} samples
{% endif %}* Columns: {% if dataset['columns'] | length == 1 %}{{ dataset['columns'][0] }}{% elif dataset['columns'] | length == 2 %}{{ dataset['columns'][0] }} and {{ dataset['columns'][1] }}{% else %}{{ dataset['columns'][:-1] | join(', ') }}, and {{ dataset['columns'][-1] }}{% endif %}
{% if dataset['stats_table'] %}* Approximate statistics based on the first {{ [dataset['size'], 1000] | min }} samples:
{{ dataset['stats_table'] }}{% endif %}{% if dataset['examples_table'] %}* Samples:
{{ dataset['examples_table'] }}{% endif %}* Loss: {% if dataset[""loss""][""fullname""].startswith(""sentence_transformers."") %}[<code>{{ dataset[""loss""][""fullname""].split(""."")[-1] }}</code>](https://sbert.net/docs/package_reference/cross_encoder/losses.html#{{ dataset[""loss""][""fullname""].split(""."")[-1].lower() }}){% else %}<code>{{ dataset[""loss""][""fullname""] }}</code>{% endif %}{% if ""config_code"" in dataset[""loss""] %} with these parameters:
{{ dataset[""loss""][""config_code""] }}{% endif %}
{% if dataset_list | length > 3 %}</details>
{% endif %}{% endfor %}{% endif %}{% endfor -%}",reference
## ⚙️ Installation,installation
"### Installation
```bash
git clone https://github.com/RohitRajpoot/reposage.git
cd reposage
pip install -r requirements.txt",installation
"# Slide 3: Bayesian Q&A Layer
- Cosine-likelihood softmax → posterior",reference
"## Features
- **DeepSeek Index** of lecture slides, code, and readings  
- **Bayesian Posterior Scoring** for precise answer selection  
- **Transformer Fallback** via HF text2text when confidence is low  
- **CLI & REST API** for local and web deployment  
- **CI/CD** via GitHub Actions → Hugging Face Spaces",usage
"### Metrics
{% for metrics in eval_metrics %}
#### {{ metrics.description }}
{% if metrics.dataset_name %}
* Dataset{% if metrics.dataset_name is not string and metrics.dataset_name | length > 1 %}s{% endif %}: {% if metrics.dataset_name is string -%}
        `{{ metrics.dataset_name }}`
    {%- else -%}
        {%- for name in metrics.dataset_name -%}
            `{{ name }}`
            {%- if not loop.last -%}
                {%- if loop.index == metrics.dataset_name | length - 1 %} and {% else -%}, {% endif -%}
            {%- endif -%}
        {%- endfor -%}
    {%- endif -%}
{%- endif %}
* Evaluated with {% if metrics.class_name.startswith(""sentence_transformers."") %}[<code>{{ metrics.class_name.split(""."")[-1] }}</code>](https://sbert.net/docs/package_reference/sentence_transformer/evaluation.html#sentence_transformers.evaluation.{{ metrics.class_name.split(""."")[-1] }}){% else %}<code>{{ metrics.class_name }}</code>{% endif %}{% if metrics.config_code %} with these parameters:
{{ metrics.config_code }}{% endif %}",reference
<details><summary>Click to expand</summary>,usage
## How to Get Started with the Model,other
<details><summary>Click to see the direct usage in Transformers</summary>,usage
"### Installation
```bash
git clone https://github.com/RohitRajpoot/reposage.git
cd reposage
pip install -r requirements.txt",installation
"# 2) Install dependencies
pip install --upgrade pip setuptools wheel
pip install -r requirements.txt",installation
"## Features
- **DeepSeek Index** of lecture slides, code, and readings  
- **Bayesian Posterior Scoring** for precise answer selection  
- **Transformer Fallback** via HF text2text when confidence is low  
- **CLI & REST API** for local and web deployment  
- **CI/CD** via GitHub Actions → Hugging Face Spaces",usage
"1. Fork and create a feature branch.
2. Add or update scripts/tests as needed.
3. Ensure style checks (`flake8`, `black`) pass.
4. Submit a pull request detailing your enhancements.",reference
<details><summary>Click to expand</summary>,usage
"# 3) Smoke-test CLI
python -m assist.main chat ""hello world""",usage
"* After labeling paragraphs (e.g. `installation`, `usage`, `configuration`, `api_reference`, `troubleshooting`), train an H2O AutoML model to predict the category of any incoming question.
   * This routing step ensures each query is answered by the most relevant section of your docs.",installation
<!-- This section is meant to convey both technical and sociotechnical limitations. -->,other
"<!-- If there is a paper or blog post introducing the model, the APA and Bibtex information for that should go in this section. -->",reference
"# 2) Install dependencies
pip install --upgrade pip setuptools wheel
pip install -r requirements.txt",installation
Update this directory using maint_tools/vendor_array_api_compat.sh,reference
"- **Training regime:** {{ training_regime | default(""[More Information Needed]"", true)}} <!--fp32, fp16 mixed precision, bf16 mixed precision, bf16 non-mixed precision, fp16 non-mixed precision, fp8 mixed precision -->",other
"{% endif -%}
{{ eval_lines }}{% if explain_bold_in_eval %}
* The bold row denotes the saved checkpoint.{% endif %}
{%- if hide_eval_lines %}
</details>{% endif %}
{% endif %}",other
"```bash
pip install -U sentence-transformers
```",installation
"{{ training_data | default(""[More Information Needed]"", true)}}",other
"```bash
pip install -U sentence-transformers
```",installation
## ⚙️ Installation,installation
"```bash
pip install -U sentence-transformers
```",installation
"## Features
- **DeepSeek Index** of lecture slides, code, and readings  
- **Bayesian Posterior Scoring** for precise answer selection  
- **Transformer Fallback** via HF text2text when confidence is low  
- **CLI & REST API** for local and web deployment  
- **CI/CD** via GitHub Actions → Hugging Face Spaces",usage
"# Build the index
reposage index",usage
## Glossary [optional],other
"<!--
### Out-of-Scope Use",other
## ⚙️ Installation,installation
"# Query
reposage query ""What is DeepSeek?"" --threshold 0.3",usage
### Dataset Sources [optional],other
"In `app.py`, configure paths to:",reference
"### Metrics
{% for metrics in eval_metrics %}
#### {{ metrics.description }}
{% if metrics.dataset_name %}
* Dataset{% if metrics.dataset_name is not string and metrics.dataset_name | length > 1 %}s{% endif %}: {% if metrics.dataset_name is string -%}
        `{{ metrics.dataset_name }}`
    {%- else -%}
        {%- for name in metrics.dataset_name -%}
            `{{ name }}`
            {%- if not loop.last -%}
                {%- if loop.index == metrics.dataset_name | length - 1 %} and {% else -%}, {% endif -%}
            {%- endif -%}
        {%- endfor -%}
    {%- endif -%}
{%- endif %}
* Evaluated with {% if metrics.class_name.startswith(""sentence_transformers."") %}[<code>{{ metrics.class_name.split(""."")[-1] }}</code>](https://sbert.net/docs/package_reference/sentence_transformer/evaluation.html#sentence_transformers.evaluation.{{ metrics.class_name.split(""."")[-1] }}){% else %}<code>{{ metrics.class_name }}</code>{% endif %}{% if metrics.config_code %} with these parameters:
{{ metrics.config_code }}{% endif %}",reference
"# 2) Install dependencies
pip install --upgrade pip setuptools wheel
pip install -r requirements.txt",installation
"<!-- If there is a paper or blog post introducing the model, the APA and Bibtex information for that should go in this section. -->",reference
"1. Fork and create a feature branch.
2. Add or update scripts/tests as needed.
3. Ensure style checks (`flake8`, `black`) pass.
4. Submit a pull request detailing your enhancements.",reference
Update this directory using maint_tools/vendor_array_api_compat.sh,reference
"{{ software | default(""[More Information Needed]"", true)}}",other
"# 2) Install dependencies
pip install --upgrade pip setuptools wheel
pip install -r requirements.txt",installation
---,other
"## Bias, Risks, and Limitations",other
"## Training Details
{% for dataset_type, dataset_list in [(""training"", train_datasets), (""evaluation"", eval_datasets)] %}{% if dataset_list %}
### {{ dataset_type.title() }} Dataset{{""s"" if dataset_list | length > 1 else """"}}
{% for dataset in dataset_list %}{% if dataset_list | length > 3 %}<details><summary>{{ dataset['name'] or 'Unnamed Dataset' }}</summary>
{% endif %}
#### {{ dataset['name'] or 'Unnamed Dataset' }}
{% if dataset['name'] %}
* Dataset: {% if 'id' in dataset %}[{{ dataset['name'] }}](https://huggingface.co/datasets/{{ dataset['id'] }}){% else %}{{ dataset['name'] }}{% endif %}
{%- if 'revision' in dataset and 'id' in dataset %} at [{{ dataset['revision'][:7] }}](https://huggingface.co/datasets/{{ dataset['id'] }}/tree/{{ dataset['revision'] }}){% endif %}{% endif %}
{% if dataset['size'] %}* Size: {{ ""{:,}"".format(dataset['size']) }} {{ dataset_type }} samples
{% endif %}* Columns: {% if dataset['columns'] | length == 1 %}{{ dataset['columns'][0] }}{% elif dataset['columns'] | length == 2 %}{{ dataset['columns'][0] }} and {{ dataset['columns'][1] }}{% else %}{{ dataset['columns'][:-1] | join(', ') }}, and {{ dataset['columns'][-1] }}{% endif %}
{% if dataset['stats_table'] %}* Approximate statistics based on the first {{ [dataset['size'], 1000] | min }} samples:
{{ dataset['stats_table'] }}{% endif %}{% if dataset['examples_table'] %}* Samples:
{{ dataset['examples_table'] }}{% endif %}* Loss: {% if dataset[""loss""][""fullname""].startswith(""sentence_transformers."") %}[<code>{{ dataset[""loss""][""fullname""].split(""."")[-1] }}</code>](https://sbert.net/docs/package_reference/cross_encoder/losses.html#{{ dataset[""loss""][""fullname""].split(""."")[-1].lower() }}){% else %}<code>{{ dataset[""loss""][""fullname""] }}</code>{% endif %}{% if ""config_code"" in dataset[""loss""] %} with these parameters:
{{ dataset[""loss""][""config_code""] }}{% endif %}
{% if dataset_list | length > 3 %}</details>
{% endif %}{% endfor %}{% endif %}{% endfor -%}",reference
"# Query
reposage query ""What is DeepSeek?"" --threshold 0.3",usage
"### Metrics
{% for metrics in eval_metrics %}
#### {{ metrics.description }}
{% if metrics.dataset_name %}
* Dataset{% if metrics.dataset_name is not string and metrics.dataset_name | length > 1 %}s{% endif %}: {% if metrics.dataset_name is string -%}
        `{{ metrics.dataset_name }}`
    {%- else -%}
        {%- for name in metrics.dataset_name -%}
            `{{ name }}`
            {%- if not loop.last -%}
                {%- if loop.index == metrics.dataset_name | length - 1 %} and {% else -%}, {% endif -%}
            {%- endif -%}
        {%- endfor -%}
    {%- endif -%}
{%- endif %}
* Evaluated with {% if metrics.class_name.startswith(""sentence_transformers."") %}[<code>{{ metrics.class_name.split(""."")[-1] }}</code>](https://sbert.net/docs/package_reference/cross_encoder/evaluation.html#sentence_transformers.cross_encoder.evaluation.{{ metrics.class_name.split(""."")[-1] }}){% else %}<code>{{ metrics.class_name }}</code>{% endif %}{% if metrics.config_code %} with these parameters:
{{ metrics.config_code }}{% endif %}",reference
"## Training Details
{% for dataset_type, dataset_list in [(""training"", train_datasets), (""evaluation"", eval_datasets)] %}{% if dataset_list %}
### {{ dataset_type.title() }} Dataset{{""s"" if dataset_list | length > 1 else """"}}
{% for dataset in dataset_list %}{% if dataset_list | length > 3 %}<details><summary>{{ dataset['name'] or 'Unnamed Dataset' }}</summary>
{% endif %}
#### {{ dataset['name'] or 'Unnamed Dataset' }}
{% if dataset['name'] %}
* Dataset: {% if 'id' in dataset %}[{{ dataset['name'] }}](https://huggingface.co/datasets/{{ dataset['id'] }}){% else %}{{ dataset['name'] }}{% endif %}
{%- if 'revision' in dataset and 'id' in dataset %} at [{{ dataset['revision'][:7] }}](https://huggingface.co/datasets/{{ dataset['id'] }}/tree/{{ dataset['revision'] }}){% endif %}{% endif %}
{% if dataset['size'] %}* Size: {{ ""{:,}"".format(dataset['size']) }} {{ dataset_type }} samples
{% endif %}* Columns: {% if dataset['columns'] | length == 1 %}{{ dataset['columns'][0] }}{% elif dataset['columns'] | length == 2 %}{{ dataset['columns'][0] }} and {{ dataset['columns'][1] }}{% else %}{{ dataset['columns'][:-1] | join(', ') }}, and {{ dataset['columns'][-1] }}{% endif %}
{% if dataset['stats_table'] %}* Approximate statistics based on the first {{ [dataset['size'], 1000] | min }} samples:
{{ dataset['stats_table'] }}{% endif %}{% if dataset['examples_table'] %}* Samples:
{{ dataset['examples_table'] }}{% endif %}* Loss: {% if dataset[""loss""][""fullname""].startswith(""sentence_transformers."") %}[<code>{{ dataset[""loss""][""fullname""].split(""."")[-1] }}</code>](https://sbert.net/docs/package_reference/cross_encoder/losses.html#{{ dataset[""loss""][""fullname""].split(""."")[-1].lower() }}){% else %}<code>{{ dataset[""loss""][""fullname""] }}</code>{% endif %}{% if ""config_code"" in dataset[""loss""] %} with these parameters:
{{ dataset[""loss""][""config_code""] }}{% endif %}
{% if dataset_list | length > 3 %}</details>
{% endif %}{% endfor %}{% endif %}{% endfor -%}",reference
"<!-- If there is a paper or blog post introducing the dataset, the APA and Bibtex information for that should go in this section. -->",reference
## Dataset Creation,other
"# 2) Install dependencies
pip install --upgrade pip setuptools wheel
pip install -r requirements.txt",installation
"| Team Member | Role(s)                 | Responsibilities                             |
|-------------|-------------------------|----------------------------------------------|
| Rohit       | CEO / Product Lead,     | • Define product vision & roadmap<br>• Manage scope & stakeholder communication<br>• Write technical documentation |
| Kishan      | CTO / DevOps Lead,      | • Develop DeepSeek index & CLI scaffold<br>• Set up CI/CD pipelines<br>• Manage Hugging Face Spaces deployments   |
| Maitri      | Data Scientist / AI Lead| • Implement Bayesian Q&A layer<br>• Design & tune priors<br>• Oversee transformer fallback prompts          |",usage
"{%- if eval_lines %}
### Training Logs
{% if hide_eval_lines %}<details><summary>Click to expand</summary>",usage
"# Query
reposage query ""What is DeepSeek?"" --threshold 0.3",usage
First install the Sentence Transformers library:,installation
"# 1) Create & activate venv
python3 -m venv .venv
source .venv/bin/activate",other
"Have you found the toolkit helpful?  Please support NLTK development by donating
to the project via PayPal, using the link on the NLTK homepage.",other
Update this directory using maint_tools/vendor_array_api_extra.sh,reference
First install the Sentence Transformers library:,installation
Update this directory using maint_tools/vendor_array_api_compat.sh,reference
"1. **Route** via the H2O classifier to select a subset of paragraphs.
     2. **Retrieve** top‑k relevant passages with FAISS.
     3. **Rescore** with a Bayesian posterior to boost high-confidence results.
     4. **Fallback**: if the posterior confidence is below threshold, generate an answer using a compact Flan‑T5 model.",reference
"<!-- If there is a paper or blog post introducing the model, the APA and Bibtex information for that should go in this section. -->",reference
"### Installation
```bash
git clone https://github.com/RohitRajpoot/reposage.git
cd reposage
pip install -r requirements.txt",installation
"1. Fork and create a feature branch.
2. Add or update scripts/tests as needed.
3. Ensure style checks (`flake8`, `black`) pass.
4. Submit a pull request detailing your enhancements.",reference
### Direct Use,other
"**This software is dual-licensed under the The University of Illinois/NCSA
Open Source License (NCSA) and The 3-Clause BSD License**",other
"### Installation
```bash
git clone https://github.com/RohitRajpoot/reposage.git
cd reposage
pip install -r requirements.txt",installation
"```bash
pip install -U sentence-transformers
```",installation
<details><summary>Click to see the direct usage in Transformers</summary>,usage
"<!-- If there is a paper or blog post introducing the model, the APA and Bibtex information for that should go in this section. -->",reference
## ⚙️ Installation,installation
## ⚙️ Installation,installation
"## Features
- **DeepSeek Index** of lecture slides, code, and readings  
- **Bayesian Posterior Scoring** for precise answer selection  
- **Transformer Fallback** via HF text2text when confidence is low  
- **CLI & REST API** for local and web deployment  
- **CI/CD** via GitHub Actions → Hugging Face Spaces",usage
"```bash
pip install -U sentence-transformers
```",installation
"# Build the index
reposage index",usage
First install the Sentence Transformers library:,installation
"# Slide 1: Architecture Overview
- User → RepoSage CLI → index/query modules",usage
"<!-- Address questions around how the model is intended to be used, including the foreseeable users of the model and those affected by the model. -->",other
"| Team Member | Role(s)                 | Responsibilities                             |
|-------------|-------------------------|----------------------------------------------|
| Rohit       | CEO / Product Lead,     | • Define product vision & roadmap<br>• Manage scope & stakeholder communication<br>• Write technical documentation |
| Kishan      | CTO / DevOps Lead,      | • Develop DeepSeek index & CLI scaffold<br>• Set up CI/CD pipelines<br>• Manage Hugging Face Spaces deployments   |
| Maitri      | Data Scientist / AI Lead| • Implement Bayesian Q&A layer<br>• Design & tune priors<br>• Oversee transformer fallback prompts          |",usage
"# 2) Install dependencies
pip install --upgrade pip setuptools wheel
pip install -r requirements.txt",installation
"### Usage
# Build the index
reposage index",usage
"## Training Details
{% for dataset_type, dataset_list in [(""training"", train_datasets), (""evaluation"", eval_datasets)] %}{% if dataset_list %}
### {{ dataset_type.title() }} Dataset{{""s"" if dataset_list | length > 1 else """"}}
{% for dataset in dataset_list %}{% if dataset_list | length > 3 %}<details><summary>{{ dataset['name'] or 'Unnamed Dataset' }}</summary>
{% endif %}
#### {{ dataset['name'] or 'Unnamed Dataset' }}
{% if dataset['name'] %}
* Dataset: {% if 'id' in dataset %}[{{ dataset['name'] }}](https://huggingface.co/datasets/{{ dataset['id'] }}){% else %}{{ dataset['name'] }}{% endif %}
{%- if 'revision' in dataset and 'id' in dataset %} at [{{ dataset['revision'][:7] }}](https://huggingface.co/datasets/{{ dataset['id'] }}/tree/{{ dataset['revision'] }}){% endif %}{% endif %}
{% if dataset['size'] %}* Size: {{ ""{:,}"".format(dataset['size']) }} {{ dataset_type }} samples
{% endif %}* Columns: {% if dataset['columns'] | length == 1 %}{{ dataset['columns'][0] }}{% elif dataset['columns'] | length == 2 %}{{ dataset['columns'][0] }} and {{ dataset['columns'][1] }}{% else %}{{ dataset['columns'][:-1] | join(', ') }}, and {{ dataset['columns'][-1] }}{% endif %}
{% if dataset['stats_table'] %}* Approximate statistics based on the first {{ [dataset['size'], 1000] | min }} samples:
{{ dataset['stats_table'] }}{% endif %}{% if dataset['examples_table'] %}* Samples:
{{ dataset['examples_table'] }}{% endif %}* Loss: {% if dataset[""loss""][""fullname""].startswith(""sentence_transformers."") %}[<code>{{ dataset[""loss""][""fullname""].split(""."")[-1] }}</code>](https://sbert.net/docs/package_reference/cross_encoder/losses.html#{{ dataset[""loss""][""fullname""].split(""."")[-1].lower() }}){% else %}<code>{{ dataset[""loss""][""fullname""] }}</code>{% endif %}{% if ""config_code"" in dataset[""loss""] %} with these parameters:
{{ dataset[""loss""][""config_code""] }}{% endif %}
{% if dataset_list | length > 3 %}</details>
{% endif %}{% endfor %}{% endif %}{% endfor -%}",reference
First install the Sentence Transformers library:,installation
"* After labeling paragraphs (e.g. `installation`, `usage`, `configuration`, `api_reference`, `troubleshooting`), train an H2O AutoML model to predict the category of any incoming question.
   * This routing step ensures each query is answered by the most relevant section of your docs.",installation
"{%- if eval_lines %}
### Training Logs
{% if hide_eval_lines %}<details><summary>Click to expand</summary>",usage
"| Team Member | Role(s)                 | Responsibilities                             |
|-------------|-------------------------|----------------------------------------------|
| Rohit       | CEO / Product Lead,     | • Define product vision & roadmap<br>• Manage scope & stakeholder communication<br>• Write technical documentation |
| Kishan      | CTO / DevOps Lead,      | • Develop DeepSeek index & CLI scaffold<br>• Set up CI/CD pipelines<br>• Manage Hugging Face Spaces deployments   |
| Maitri      | Data Scientist / AI Lead| • Implement Bayesian Q&A layer<br>• Design & tune priors<br>• Oversee transformer fallback prompts          |",usage
Update this directory using maint_tools/vendor_array_api_compat.sh,reference
"```markdown
# Prompt Design Strategy",other
"* After labeling paragraphs (e.g. `installation`, `usage`, `configuration`, `api_reference`, `troubleshooting`), train an H2O AutoML model to predict the category of any incoming question.
   * This routing step ensures each query is answered by the most relevant section of your docs.",installation
"# Slide 3: Bayesian Q&A Layer
- Cosine-likelihood softmax → posterior",reference
## Glossary [optional],other
First install the Sentence Transformers library:,installation
"</details>
-->",other
<details><summary>Click to expand</summary>,usage
Update this directory using maint_tools/vendor_array_api_extra.sh,reference
Update this directory using maint_tools/vendor_array_api_compat.sh,reference
"### Installation
```bash
git clone https://github.com/RohitRajpoot/reposage.git
cd reposage
pip install -r requirements.txt",installation
"| Team Member | Role(s)                 | Responsibilities                             |
|-------------|-------------------------|----------------------------------------------|
| Rohit       | CEO / Product Lead,     | • Define product vision & roadmap<br>• Manage scope & stakeholder communication<br>• Write technical documentation |
| Kishan      | CTO / DevOps Lead,      | • Develop DeepSeek index & CLI scaffold<br>• Set up CI/CD pipelines<br>• Manage Hugging Face Spaces deployments   |
| Maitri      | Data Scientist / AI Lead| • Implement Bayesian Q&A layer<br>• Design & tune priors<br>• Oversee transformer fallback prompts          |",usage
Carbon emissions can be estimated using the [Machine Learning Impact calculator](https://mlco2.github.io/impact#compute) presented in [Lacoste et al. (2019)](https://arxiv.org/abs/1910.09700).,other
<details><summary>Click to see the direct usage in Transformers</summary>,usage
"1. Fork and create a feature branch.
2. Add or update scripts/tests as needed.
3. Ensure style checks (`flake8`, `black`) pass.
4. Submit a pull request detailing your enhancements.",reference
"### Usage
# Build the index
reposage index",usage
"{{ testing_metrics | default(""[More Information Needed]"", true)}}",other
"# 2) Install dependencies
pip install --upgrade pip setuptools wheel
pip install -r requirements.txt",installation
"# 3) Smoke-test CLI
python -m assist.main chat ""hello world""",usage
"*Clearly define terms in order to be accessible across audiences.*
-->",other
"1. **Route** via the H2O classifier to select a subset of paragraphs.
     2. **Retrieve** top‑k relevant passages with FAISS.
     3. **Rescore** with a Bayesian posterior to boost high-confidence results.
     4. **Fallback**: if the posterior confidence is below threshold, generate an answer using a compact Flan‑T5 model.",reference
<!-- Relevant interpretability work for the model goes here -->,other
"### Usage
# Build the index
reposage index",usage
First install the Sentence Transformers library:,installation
"# Slide 2: DeepSeek Module
- Embedding engine: all-MiniLM-L6-v2
- FAISS vector store",other
### Dataset Description,other
"### Metrics
{% for metrics in eval_metrics %}
#### {{ metrics.description }}
{% if metrics.dataset_name %}
* Dataset{% if metrics.dataset_name is not string and metrics.dataset_name | length > 1 %}s{% endif %}: {% if metrics.dataset_name is string -%}
        `{{ metrics.dataset_name }}`
    {%- else -%}
        {%- for name in metrics.dataset_name -%}
            `{{ name }}`
            {%- if not loop.last -%}
                {%- if loop.index == metrics.dataset_name | length - 1 %} and {% else -%}, {% endif -%}
            {%- endif -%}
        {%- endfor -%}
    {%- endif -%}
{%- endif %}
* Evaluated with {% if metrics.class_name.startswith(""sentence_transformers."") %}[<code>{{ metrics.class_name.split(""."")[-1] }}</code>](https://sbert.net/docs/package_reference/sentence_transformer/evaluation.html#sentence_transformers.evaluation.{{ metrics.class_name.split(""."")[-1] }}){% else %}<code>{{ metrics.class_name }}</code>{% endif %}{% if metrics.config_code %} with these parameters:
{{ metrics.config_code }}{% endif %}",reference
<details><summary>Click to see the direct usage in Transformers</summary>,usage
"### Metrics
{% for metrics in eval_metrics %}
#### {{ metrics.description }}
{% if metrics.dataset_name %}
* Dataset{% if metrics.dataset_name is not string and metrics.dataset_name | length > 1 %}s{% endif %}: {% if metrics.dataset_name is string -%}
        `{{ metrics.dataset_name }}`
    {%- else -%}
        {%- for name in metrics.dataset_name -%}
            `{{ name }}`
            {%- if not loop.last -%}
                {%- if loop.index == metrics.dataset_name | length - 1 %} and {% else -%}, {% endif -%}
            {%- endif -%}
        {%- endfor -%}
    {%- endif -%}
{%- endif %}
* Evaluated with {% if metrics.class_name.startswith(""sentence_transformers."") %}[<code>{{ metrics.class_name.split(""."")[-1] }}</code>](https://sbert.net/docs/package_reference/cross_encoder/evaluation.html#sentence_transformers.cross_encoder.evaluation.{{ metrics.class_name.split(""."")[-1] }}){% else %}<code>{{ metrics.class_name }}</code>{% endif %}{% if metrics.config_code %} with these parameters:
{{ metrics.config_code }}{% endif %}",reference
"{{ personal_and_sensitive_information | default(""[More Information Needed]"", true)}}",other
<details><summary>Click to expand</summary>,usage
"- Steven Bird <stevenbird1@gmail.com>
- Edward Loper <edloper@gmail.com>
- Ewan Klein <ewan@inf.ed.ac.uk>",other
"# Or call the REST API
curl -X POST https://<your-space>.hf.space/query \
     -d '{""question"":""What is DeepSeek?""}'",reference
First install the Sentence Transformers library:,installation
"1. **Route** via the H2O classifier to select a subset of paragraphs.
     2. **Retrieve** top‑k relevant passages with FAISS.
     3. **Rescore** with a Bayesian posterior to boost high-confidence results.
     4. **Fallback**: if the posterior confidence is below threshold, generate an answer using a compact Flan‑T5 model.",reference
"## Features
- **DeepSeek Index** of lecture slides, code, and readings  
- **Bayesian Posterior Scoring** for precise answer selection  
- **Transformer Fallback** via HF text2text when confidence is low  
- **CLI & REST API** for local and web deployment  
- **CI/CD** via GitHub Actions → Hugging Face Spaces",usage
"# Slide 3: Bayesian Q&A Layer
- Cosine-likelihood softmax → posterior",reference
Update this directory using maint_tools/vendor_array_api_extra.sh,reference
### Annotations [optional],other
"# Slide 1: Architecture Overview
- User → RepoSage CLI → index/query modules",usage
Update this directory using maint_tools/vendor_array_api_compat.sh,reference
"{%- if eval_lines %}
### Training Logs
{% if hide_eval_lines %}<details><summary>Click to expand</summary>",usage
"### Installation
```bash
git clone https://github.com/RohitRajpoot/reposage.git
cd reposage
pip install -r requirements.txt",installation
"{%- if eval_lines %}
### Training Logs
{% if hide_eval_lines %}<details><summary>Click to expand</summary>",usage
4. **Streamlit UI / Hugging Face Space**,other
#NAME?,other
"# Slide 1: Architecture Overview
- User → RepoSage CLI → index/query modules",usage
"<!-- These are the things the evaluation is disaggregating by, e.g., subpopulations or domains. -->",other
## Technical Specifications [optional],other
"{% for name, value in non_default_hyperparameters.items() %}- `{{ name }}`: {{ value }}
{% endfor %}{%- endif %}
#### All Hyperparameters
<details><summary>Click to expand</summary>",usage
## Model Examination [optional],other
"* After labeling paragraphs (e.g. `installation`, `usage`, `configuration`, `api_reference`, `troubleshooting`), train an H2O AutoML model to predict the category of any incoming question.
   * This routing step ensures each query is answered by the most relevant section of your docs.",installation
First install the Sentence Transformers library:,installation
"<!-- If there is a paper or blog post introducing the model, the APA and Bibtex information for that should go in this section. -->",reference
"| Team Member | Role(s)                 | Responsibilities                             |
|-------------|-------------------------|----------------------------------------------|
| Rohit       | CEO / Product Lead,     | • Define product vision & roadmap<br>• Manage scope & stakeholder communication<br>• Write technical documentation |
| Kishan      | CTO / DevOps Lead,      | • Develop DeepSeek index & CLI scaffold<br>• Set up CI/CD pipelines<br>• Manage Hugging Face Spaces deployments   |
| Maitri      | Data Scientist / AI Lead| • Implement Bayesian Q&A layer<br>• Design & tune priors<br>• Oversee transformer fallback prompts          |",usage
"{% for name, value in non_default_hyperparameters.items() %}- `{{ name }}`: {{ value }}
{% endfor %}{%- endif %}
#### All Hyperparameters
<details><summary>Click to expand</summary>",usage
"Redistribution and use in source and binary forms, with or without
modification, are permitted provided that the following conditions are met:",other
"# NCSA Open Source License
**Copyright (c) 2019 Kevin Sheppard. All rights reserved.**",other
"# 3) Smoke-test CLI
python -m assist.main chat ""hello world""",usage
"# Query
reposage query ""What is DeepSeek?"" --threshold 0.3",usage
<details><summary>Click to see the direct usage in Transformers</summary>,usage
"2. Redistributions in binary form must reproduce the above copyright
   notice, this list of conditions and the following disclaimer in the
   documentation and/or other materials provided with the distribution.",other
"1. Fork and create a feature branch.
2. Add or update scripts/tests as needed.
3. Ensure style checks (`flake8`, `black`) pass.
4. Submit a pull request detailing your enhancements.",reference
"{% for name, value in non_default_hyperparameters.items() %}- `{{ name }}`: {{ value }}
{% endfor %}{%- endif %}
#### All Hyperparameters
<details><summary>Click to expand</summary>",usage
"In `app.py`, configure paths to:",reference
"* After labeling paragraphs (e.g. `installation`, `usage`, `configuration`, `api_reference`, `troubleshooting`), train an H2O AutoML model to predict the category of any incoming question.
   * This routing step ensures each query is answered by the most relevant section of your docs.",installation
"# Build the index
reposage index",usage
"# Slide 3: Bayesian Q&A Layer
- Cosine-likelihood softmax → posterior",reference
First install the Sentence Transformers library:,installation
"Bird, Steven, Edward Loper and Ewan Klein (2009).
    Natural Language Processing with Python.  O'Reilly Media Inc.",other
"## Training Details
{% for dataset_type, dataset_list in [(""training"", train_datasets), (""evaluation"", eval_datasets)] %}{% if dataset_list %}
### {{ dataset_type.title() }} Dataset{{""s"" if dataset_list | length > 1 else """"}}
{% for dataset in dataset_list %}{% if dataset_list | length > 3 %}<details><summary>{{ dataset['name'] or 'Unnamed Dataset' }}</summary>
{% endif %}
#### {{ dataset['name'] or 'Unnamed Dataset' }}
{% if dataset['name'] %}
* Dataset: {% if 'id' in dataset %}[{{ dataset['name'] }}](https://huggingface.co/datasets/{{ dataset['id'] }}){% else %}{{ dataset['name'] }}{% endif %}
{%- if 'revision' in dataset and 'id' in dataset %} at [{{ dataset['revision'][:7] }}](https://huggingface.co/datasets/{{ dataset['id'] }}/tree/{{ dataset['revision'] }}){% endif %}{% endif %}
{% if dataset['size'] %}* Size: {{ ""{:,}"".format(dataset['size']) }} {{ dataset_type }} samples
{% endif %}* Columns: {% if dataset['columns'] | length == 1 %}{{ dataset['columns'][0] }}{% elif dataset['columns'] | length == 2 %}{{ dataset['columns'][0] }} and {{ dataset['columns'][1] }}{% else %}{{ dataset['columns'][:-1] | join(', ') }}, and {{ dataset['columns'][-1] }}{% endif %}
{% if dataset['stats_table'] %}* Approximate statistics based on the first {{ [dataset['size'], 1000] | min }} samples:
{{ dataset['stats_table'] }}{% endif %}{% if dataset['examples_table'] %}* Samples:
{{ dataset['examples_table'] }}{% endif %}* Loss: {% if dataset[""loss""][""fullname""].startswith(""sentence_transformers."") %}[<code>{{ dataset[""loss""][""fullname""].split(""."")[-1] }}</code>](https://sbert.net/docs/package_reference/sentence_transformer/losses.html#{{ dataset[""loss""][""fullname""].split(""."")[-1].lower() }}){% else %}<code>{{ dataset[""loss""][""fullname""] }}</code>{% endif %}{% if ""config_code"" in dataset[""loss""] %} with these parameters:
{{ dataset[""loss""][""config_code""] }}{% endif %}
{% if dataset_list | length > 3 %}</details>
{% endif %}{% endfor %}{% endif %}{% endfor -%}",reference
"# 2) Install dependencies
pip install --upgrade pip setuptools wheel
pip install -r requirements.txt",installation
<details><summary>Click to see the direct usage in Transformers</summary>,usage
"# 3) Smoke-test CLI
python -m assist.main chat ""hello world""",usage
"```bash
pip install -U sentence-transformers
```",installation
"### Installation
```bash
git clone https://github.com/RohitRajpoot/reposage.git
cd reposage
pip install -r requirements.txt",installation
First install the Sentence Transformers library:,installation
"# Query
reposage query ""What is DeepSeek?"" --threshold 0.3",usage
"* After labeling paragraphs (e.g. `installation`, `usage`, `configuration`, `api_reference`, `troubleshooting`), train an H2O AutoML model to predict the category of any incoming question.
   * This routing step ensures each query is answered by the most relevant section of your docs.",installation
"{% for name, value in non_default_hyperparameters.items() %}- `{{ name }}`: {{ value }}
{% endfor %}{%- endif %}
#### All Hyperparameters
<details><summary>Click to expand</summary>",usage
"## Training Details
{% for dataset_type, dataset_list in [(""training"", train_datasets), (""evaluation"", eval_datasets)] %}{% if dataset_list %}
### {{ dataset_type.title() }} Dataset{{""s"" if dataset_list | length > 1 else """"}}
{% for dataset in dataset_list %}{% if dataset_list | length > 3 %}<details><summary>{{ dataset['name'] or 'Unnamed Dataset' }}</summary>
{% endif %}
#### {{ dataset['name'] or 'Unnamed Dataset' }}
{% if dataset['name'] %}
* Dataset: {% if 'id' in dataset %}[{{ dataset['name'] }}](https://huggingface.co/datasets/{{ dataset['id'] }}){% else %}{{ dataset['name'] }}{% endif %}
{%- if 'revision' in dataset and 'id' in dataset %} at [{{ dataset['revision'][:7] }}](https://huggingface.co/datasets/{{ dataset['id'] }}/tree/{{ dataset['revision'] }}){% endif %}{% endif %}
{% if dataset['size'] %}* Size: {{ ""{:,}"".format(dataset['size']) }} {{ dataset_type }} samples
{% endif %}* Columns: {% if dataset['columns'] | length == 1 %}{{ dataset['columns'][0] }}{% elif dataset['columns'] | length == 2 %}{{ dataset['columns'][0] }} and {{ dataset['columns'][1] }}{% else %}{{ dataset['columns'][:-1] | join(', ') }}, and {{ dataset['columns'][-1] }}{% endif %}
{% if dataset['stats_table'] %}* Approximate statistics based on the first {{ [dataset['size'], 1000] | min }} samples:
{{ dataset['stats_table'] }}{% endif %}{% if dataset['examples_table'] %}* Samples:
{{ dataset['examples_table'] }}{% endif %}* Loss: {% if dataset[""loss""][""fullname""].startswith(""sentence_transformers."") %}[<code>{{ dataset[""loss""][""fullname""].split(""."")[-1] }}</code>](https://sbert.net/docs/package_reference/cross_encoder/losses.html#{{ dataset[""loss""][""fullname""].split(""."")[-1].lower() }}){% else %}<code>{{ dataset[""loss""][""fullname""] }}</code>{% endif %}{% if ""config_code"" in dataset[""loss""] %} with these parameters:
{{ dataset[""loss""][""config_code""] }}{% endif %}
{% if dataset_list | length > 3 %}</details>
{% endif %}{% endfor %}{% endif %}{% endfor -%}",reference
"### Installation
```bash
git clone https://github.com/RohitRajpoot/reposage.git
cd reposage
pip install -r requirements.txt",installation
"{% for name, value in non_default_hyperparameters.items() %}- `{{ name }}`: {{ value }}
{% endfor %}{%- endif %}
#### All Hyperparameters
<details><summary>Click to expand</summary>",usage
"# Slide 1: Architecture Overview
- User → RepoSage CLI → index/query modules",usage
"---
# For reference on model card metadata, see the spec: https://github.com/huggingface/hub-docs/blob/main/modelcard.md?plain=1
# Doc / guide: https://huggingface.co/docs/hub/model-cards
{{ card_data }}
---",other
"### Metrics
{% for metrics in eval_metrics %}
#### {{ metrics.description }}
{% if metrics.dataset_name %}
* Dataset{% if metrics.dataset_name is not string and metrics.dataset_name | length > 1 %}s{% endif %}: {% if metrics.dataset_name is string -%}
        `{{ metrics.dataset_name }}`
    {%- else -%}
        {%- for name in metrics.dataset_name -%}
            `{{ name }}`
            {%- if not loop.last -%}
                {%- if loop.index == metrics.dataset_name | length - 1 %} and {% else -%}, {% endif -%}
            {%- endif -%}
        {%- endfor -%}
    {%- endif -%}
{%- endif %}
* Evaluated with {% if metrics.class_name.startswith(""sentence_transformers."") %}[<code>{{ metrics.class_name.split(""."")[-1] }}</code>](https://sbert.net/docs/package_reference/cross_encoder/evaluation.html#sentence_transformers.cross_encoder.evaluation.{{ metrics.class_name.split(""."")[-1] }}){% else %}<code>{{ metrics.class_name }}</code>{% endif %}{% if metrics.config_code %} with these parameters:
{{ metrics.config_code }}{% endif %}",reference
"{% for name, value in non_default_hyperparameters.items() %}- `{{ name }}`: {{ value }}
{% endfor %}{%- endif %}
#### All Hyperparameters
<details><summary>Click to expand</summary>",usage
"```bash
pip install -U sentence-transformers
```",installation
<details><summary>Click to see the direct usage in Transformers</summary>,usage
"### Usage
# Build the index
reposage index",usage
"1. **Route** via the H2O classifier to select a subset of paragraphs.
     2. **Retrieve** top‑k relevant passages with FAISS.
     3. **Rescore** with a Bayesian posterior to boost high-confidence results.
     4. **Fallback**: if the posterior confidence is below threshold, generate an answer using a compact Flan‑T5 model.",reference
## Citing,other
"{%- if eval_lines %}
### Training Logs
{% if hide_eval_lines %}<details><summary>Click to expand</summary>",usage
<details><summary>Click to expand</summary>,usage
## ⚙️ Installation,installation
"# Query
reposage query ""What is DeepSeek?"" --threshold 0.3",usage
"## Features
- **DeepSeek Index** of lecture slides, code, and readings  
- **Bayesian Posterior Scoring** for precise answer selection  
- **Transformer Fallback** via HF text2text when confidence is low  
- **CLI & REST API** for local and web deployment  
- **CI/CD** via GitHub Actions → Hugging Face Spaces",usage
"# Query
reposage query ""What is DeepSeek?"" --threshold 0.3",usage
"In `app.py`, configure paths to:",reference
"# 2) Install dependencies
pip install --upgrade pip setuptools wheel
pip install -r requirements.txt",installation
"| Team Member | Role(s)                 | Responsibilities                             |
|-------------|-------------------------|----------------------------------------------|
| Rohit       | CEO / Product Lead,     | • Define product vision & roadmap<br>• Manage scope & stakeholder communication<br>• Write technical documentation |
| Kishan      | CTO / DevOps Lead,      | • Develop DeepSeek index & CLI scaffold<br>• Set up CI/CD pipelines<br>• Manage Hugging Face Spaces deployments   |
| Maitri      | Data Scientist / AI Lead| • Implement Bayesian Q&A layer<br>• Design & tune priors<br>• Oversee transformer fallback prompts          |",usage
First install the Sentence Transformers library:,installation
"### Installation
```bash
git clone https://github.com/RohitRajpoot/reposage.git
cd reposage
pip install -r requirements.txt",installation
**BibTeX:**,other
<details><summary>Click to see the direct usage in Transformers</summary>,usage
"## Features
- **DeepSeek Index** of lecture slides, code, and readings  
- **Bayesian Posterior Scoring** for precise answer selection  
- **Transformer Fallback** via HF text2text when confidence is low  
- **CLI & REST API** for local and web deployment  
- **CI/CD** via GitHub Actions → Hugging Face Spaces",usage
"### Installation
```bash
git clone https://github.com/RohitRajpoot/reposage.git
cd reposage
pip install -r requirements.txt",installation
"In `app.py`, configure paths to:",reference
"### Installation
```bash
git clone https://github.com/RohitRajpoot/reposage.git
cd reposage
pip install -r requirements.txt",installation
"1. **Route** via the H2O classifier to select a subset of paragraphs.
     2. **Retrieve** top‑k relevant passages with FAISS.
     3. **Rescore** with a Bayesian posterior to boost high-confidence results.
     4. **Fallback**: if the posterior confidence is below threshold, generate an answer using a compact Flan‑T5 model.",reference
"# Slide 1: RepoSage for AML3304
- Simulates AI-centric product pipeline
- DeepSeek + Bayesian Q&A + Transformer fallback",other
"{%- if eval_lines %}
### Training Logs
{% if hide_eval_lines %}<details><summary>Click to expand</summary>",usage
## Citation,other
<!-- Address questions around how the dataset is intended to be used. -->,other
### Results,other
First install the Sentence Transformers library:,installation
"# 2) Install dependencies
pip install --upgrade pip setuptools wheel
pip install -r requirements.txt",installation
## Environmental Impact,other
"## Features
- **DeepSeek Index** of lecture slides, code, and readings  
- **Bayesian Posterior Scoring** for precise answer selection  
- **Transformer Fallback** via HF text2text when confidence is low  
- **CLI & REST API** for local and web deployment  
- **CI/CD** via GitHub Actions → Hugging Face Spaces",usage
"{% for name, value in non_default_hyperparameters.items() %}- `{{ name }}`: {{ value }}
{% endfor %}{%- endif %}
#### All Hyperparameters
<details><summary>Click to expand</summary>",usage
"In `app.py`, configure paths to:",reference
"{%- if eval_lines %}
### Training Logs
{% if hide_eval_lines %}<details><summary>Click to expand</summary>",usage
"1. Fork and create a feature branch.
2. Add or update scripts/tests as needed.
3. Ensure style checks (`flake8`, `black`) pass.
4. Submit a pull request detailing your enhancements.",reference
<!-- Provide a longer summary of what this model is. -->,other
"### Training Hardware
- **On Cloud**: {{ ""Yes"" if co2_eq_emissions[""on_cloud""] else ""No"" }}
- **GPU Model**: {{ co2_eq_emissions[""hardware_used""] or ""No GPU used"" }}
- **CPU Model**: {{ co2_eq_emissions[""cpu_model""] }}
- **RAM Size**: {{ ""%.2f""|format(co2_eq_emissions[""ram_total_size""]) }} GB
{% endif %}
### Framework Versions
- Python: {{ version[""python""] }}
- Sentence Transformers: {{ version[""sentence_transformers""] }}
- Transformers: {{ version[""transformers""] }}
- PyTorch: {{ version[""torch""] }}
- Accelerate: {{ version[""accelerate""] }}
- Datasets: {{ version[""datasets""] }}
- Tokenizers: {{ version[""tokenizers""] }}",other
Update this directory using maint_tools/vendor_array_api_compat.sh,reference
"```bash
pip install -U sentence-transformers
```",installation
"# 3) Smoke-test CLI
python -m assist.main chat ""hello world""",usage
"# Or call the REST API
curl -X POST https://<your-space>.hf.space/query \
     -d '{""question"":""What is DeepSeek?""}'",reference
First install the Sentence Transformers library:,installation
"```bash
pip install -U sentence-transformers
```",installation
<details><summary>Click to expand</summary>,usage
"<!-- If there is a paper or blog post introducing the model, the APA and Bibtex information for that should go in this section. -->",reference
## ⚙️ Installation,installation
"{{ results_summary | default("""", true) }}",other
"<!-- These are the evaluation metrics being used, ideally with a description of why. -->",other
"<!-- This section describes the source data (e.g. news text and headlines, social media posts, translated sentences, ...). -->",reference
"* After labeling paragraphs (e.g. `installation`, `usage`, `configuration`, `api_reference`, `troubleshooting`), train an H2O AutoML model to predict the category of any incoming question.
   * This routing step ensures each query is answered by the most relevant section of your docs.",installation
"{% for name, value in non_default_hyperparameters.items() %}- `{{ name }}`: {{ value }}
{% endfor %}{%- endif %}
#### All Hyperparameters
<details><summary>Click to expand</summary>",usage
"In `app.py`, configure paths to:",reference
### Training Procedure,other
"Redistributions in binary form must reproduce the above copyright notice, this
list of conditions and the following disclaimers in the documentation and/or
other materials provided with the distribution.",other
"<!---
  Licensed to the Apache Software Foundation (ASF) under one
  or more contributor license agreements.  See the NOTICE file
  distributed with this work for additional information
  regarding copyright ownership.  The ASF licenses this file
  to you under the Apache License, Version 2.0 (the
  ""License""); you may not use this file except in compliance
  with the License.  You may obtain a copy of the License at",other
"# Build the index
reposage index",usage
"---
# For reference on model card metadata, see the spec: https://github.com/huggingface/hub-docs/blob/main/modelcard.md?plain=1
# Doc / guide: https://huggingface.co/docs/hub/model-cards
{{ card_data }}
---",other
"# 3) Smoke-test CLI
python -m assist.main chat ""hello world""",usage
"1. Fork and create a feature branch.
2. Add or update scripts/tests as needed.
3. Ensure style checks (`flake8`, `black`) pass.
4. Submit a pull request detailing your enhancements.",reference
First install the Sentence Transformers library:,installation
Update this directory using maint_tools/vendor_array_api_compat.sh,reference
"```bash
pip install -U sentence-transformers
```",installation
"{% for name, value in non_default_hyperparameters.items() %}- `{{ name }}`: {{ value }}
{% endfor %}{%- endif %}
#### All Hyperparameters
<details><summary>Click to expand</summary>",usage
"### Metrics
{% for metrics in eval_metrics %}
#### {{ metrics.description }}
{% if metrics.dataset_name %}
* Dataset{% if metrics.dataset_name is not string and metrics.dataset_name | length > 1 %}s{% endif %}: {% if metrics.dataset_name is string -%}
        `{{ metrics.dataset_name }}`
    {%- else -%}
        {%- for name in metrics.dataset_name -%}
            `{{ name }}`
            {%- if not loop.last -%}
                {%- if loop.index == metrics.dataset_name | length - 1 %} and {% else -%}, {% endif -%}
            {%- endif -%}
        {%- endfor -%}
    {%- endif -%}
{%- endif %}
* Evaluated with {% if metrics.class_name.startswith(""sentence_transformers."") %}[<code>{{ metrics.class_name.split(""."")[-1] }}</code>](https://sbert.net/docs/package_reference/cross_encoder/evaluation.html#sentence_transformers.cross_encoder.evaluation.{{ metrics.class_name.split(""."")[-1] }}){% else %}<code>{{ metrics.class_name }}</code>{% endif %}{% if metrics.config_code %} with these parameters:
{{ metrics.config_code }}{% endif %}",reference
"```bash
pip install -U sentence-transformers
```",installation
"1. **Route** via the H2O classifier to select a subset of paragraphs.
     2. **Retrieve** top‑k relevant passages with FAISS.
     3. **Rescore** with a Bayesian posterior to boost high-confidence results.
     4. **Fallback**: if the posterior confidence is below threshold, generate an answer using a compact Flan‑T5 model.",reference
"# 3) Smoke-test CLI
python -m assist.main chat ""hello world""",usage
First install the Sentence Transformers library:,installation
## ⚙️ Installation,installation
"2. Redistributions in binary form must reproduce the above copyright notice,
   this list of conditions and the following disclaimer in the documentation
   and/or other materials provided with the distribution.",other
"### Usage
# Build the index
reposage index",usage
"```bash
pip install -U sentence-transformers
```",installation
First install the Sentence Transformers library:,installation
First install the Sentence Transformers library:,installation
"### Metrics
{% for metrics in eval_metrics %}
#### {{ metrics.description }}
{% if metrics.dataset_name %}
* Dataset{% if metrics.dataset_name is not string and metrics.dataset_name | length > 1 %}s{% endif %}: {% if metrics.dataset_name is string -%}
        `{{ metrics.dataset_name }}`
    {%- else -%}
        {%- for name in metrics.dataset_name -%}
            `{{ name }}`
            {%- if not loop.last -%}
                {%- if loop.index == metrics.dataset_name | length - 1 %} and {% else -%}, {% endif -%}
            {%- endif -%}
        {%- endfor -%}
    {%- endif -%}
{%- endif %}
* Evaluated with {% if metrics.class_name.startswith(""sentence_transformers."") %}[<code>{{ metrics.class_name.split(""."")[-1] }}</code>](https://sbert.net/docs/package_reference/sentence_transformer/evaluation.html#sentence_transformers.evaluation.{{ metrics.class_name.split(""."")[-1] }}){% else %}<code>{{ metrics.class_name }}</code>{% endif %}{% if metrics.config_code %} with these parameters:
{{ metrics.config_code }}{% endif %}",reference
"<!-- This section describes the source data (e.g. news text and headlines, social media posts, translated sentences, ...). -->",reference
"### Installation
```bash
git clone https://github.com/RohitRajpoot/reposage.git
cd reposage
pip install -r requirements.txt",installation
"## Training Details
{% for dataset_type, dataset_list in [(""training"", train_datasets), (""evaluation"", eval_datasets)] %}{% if dataset_list %}
### {{ dataset_type.title() }} Dataset{{""s"" if dataset_list | length > 1 else """"}}
{% for dataset in dataset_list %}{% if dataset_list | length > 3 %}<details><summary>{{ dataset['name'] or 'Unnamed Dataset' }}</summary>
{% endif %}
#### {{ dataset['name'] or 'Unnamed Dataset' }}
{% if dataset['name'] %}
* Dataset: {% if 'id' in dataset %}[{{ dataset['name'] }}](https://huggingface.co/datasets/{{ dataset['id'] }}){% else %}{{ dataset['name'] }}{% endif %}
{%- if 'revision' in dataset and 'id' in dataset %} at [{{ dataset['revision'][:7] }}](https://huggingface.co/datasets/{{ dataset['id'] }}/tree/{{ dataset['revision'] }}){% endif %}{% endif %}
{% if dataset['size'] %}* Size: {{ ""{:,}"".format(dataset['size']) }} {{ dataset_type }} samples
{% endif %}* Columns: {% if dataset['columns'] | length == 1 %}{{ dataset['columns'][0] }}{% elif dataset['columns'] | length == 2 %}{{ dataset['columns'][0] }} and {{ dataset['columns'][1] }}{% else %}{{ dataset['columns'][:-1] | join(', ') }}, and {{ dataset['columns'][-1] }}{% endif %}
{% if dataset['stats_table'] %}* Approximate statistics based on the first {{ [dataset['size'], 1000] | min }} samples:
{{ dataset['stats_table'] }}{% endif %}{% if dataset['examples_table'] %}* Samples:
{{ dataset['examples_table'] }}{% endif %}* Loss: {% if dataset[""loss""][""fullname""].startswith(""sentence_transformers."") %}[<code>{{ dataset[""loss""][""fullname""].split(""."")[-1] }}</code>](https://sbert.net/docs/package_reference/cross_encoder/losses.html#{{ dataset[""loss""][""fullname""].split(""."")[-1].lower() }}){% else %}<code>{{ dataset[""loss""][""fullname""] }}</code>{% endif %}{% if ""config_code"" in dataset[""loss""] %} with these parameters:
{{ dataset[""loss""][""config_code""] }}{% endif %}
{% if dataset_list | length > 3 %}</details>
{% endif %}{% endfor %}{% endif %}{% endfor -%}",reference
"* After labeling paragraphs (e.g. `installation`, `usage`, `configuration`, `api_reference`, `troubleshooting`), train an H2O AutoML model to predict the category of any incoming question.
   * This routing step ensures each query is answered by the most relevant section of your docs.",installation
"### Installation
```bash
git clone https://github.com/RohitRajpoot/reposage.git
cd reposage
pip install -r requirements.txt",installation
"<!-- If there is a paper or blog post introducing the dataset, the APA and Bibtex information for that should go in this section. -->",reference
<details><summary>Click to see the direct usage in Transformers</summary>,usage
"{%- if eval_lines %}
### Training Logs
{% if hide_eval_lines %}<details><summary>Click to expand</summary>",usage
Update this directory using maint_tools/vendor_array_api_extra.sh,reference
"| Team Member | Role(s)                 | Responsibilities                             |
|-------------|-------------------------|----------------------------------------------|
| Rohit       | CEO / Product Lead,     | • Define product vision & roadmap<br>• Manage scope & stakeholder communication<br>• Write technical documentation |
| Kishan      | CTO / DevOps Lead,      | • Develop DeepSeek index & CLI scaffold<br>• Set up CI/CD pipelines<br>• Manage Hugging Face Spaces deployments   |
| Maitri      | Data Scientist / AI Lead| • Implement Bayesian Q&A layer<br>• Design & tune priors<br>• Oversee transformer fallback prompts          |",usage
"Neither the names of Kevin Sheppard, nor the names of any contributors may be
used to endorse or promote products derived from this Software without specific
prior written permission.",other
"### Model Description
- **Model Type:** Cross Encoder
{% if base_model -%}
    {%- if base_model_revision -%}
    - **Base model:** [{{ base_model }}](https://huggingface.co/{{ base_model }}) <!-- at revision {{ base_model_revision }} -->
    {%- else -%}
    - **Base model:** [{{ base_model }}](https://huggingface.co/{{ base_model }})
    {%- endif -%}
{%- else -%}
    <!-- - **Base model:** [Unknown](https://huggingface.co/unknown) -->
{%- endif %}
- **Maximum Sequence Length:** {{ model_max_length }} tokens
- **Number of Output Labels:** {{ model_num_labels }} label{{ ""s"" if model_num_labels > 1 else """" }}
{% if train_datasets | selectattr(""name"") | list -%}
    - **Training Dataset{{""s"" if train_datasets | selectattr(""name"") | list | length > 1 else """"}}:**
    {%- for dataset in (train_datasets | selectattr(""name"")) %}
        {%- if dataset.id %}
    - [{{ dataset.name if dataset.name else dataset.id }}](https://huggingface.co/datasets/{{ dataset.id }})
        {%- else %}
    - {{ dataset.name }}
        {%- endif %}
    {%- endfor %}
{%- else -%}
    <!-- - **Training Dataset:** Unknown -->
{%- endif %}
{% if language -%}
    - **Language{{""s"" if language is not string and language | length > 1 else """"}}:**
    {%- if language is string %} {{ language }}
    {%- else %} {% for lang in language -%}
            {{ lang }}{{ "", "" if not loop.last else """" }}
        {%- endfor %}
    {%- endif %}
{%- else -%}
    <!-- - **Language:** Unknown -->
{%- endif %}
{% if license -%}
    - **License:** {{ license }}
{%- else -%}
    <!-- - **License:** Unknown -->
{%- endif %}",other
"{{ model_card_authors | default(""[More Information Needed]"", true)}}",other
"# 2) Install dependencies
pip install --upgrade pip setuptools wheel
pip install -r requirements.txt",installation
"*What are recommendations with respect to the foreseeable issues? For example, filtering explicit content.*
-->",other
"<!--
## Model Card Contact",other
Update this directory using maint_tools/vendor_array_api_compat.sh,reference
---,other
<!-- Provide a longer summary of what this dataset is. -->,other
Use the code below to get started with the model.,other
"## Training Details
{% for dataset_type, dataset_list in [(""training"", train_datasets), (""evaluation"", eval_datasets)] %}{% if dataset_list %}
### {{ dataset_type.title() }} Dataset{{""s"" if dataset_list | length > 1 else """"}}
{% for dataset in dataset_list %}{% if dataset_list | length > 3 %}<details><summary>{{ dataset['name'] or 'Unnamed Dataset' }}</summary>
{% endif %}
#### {{ dataset['name'] or 'Unnamed Dataset' }}
{% if dataset['name'] %}
* Dataset: {% if 'id' in dataset %}[{{ dataset['name'] }}](https://huggingface.co/datasets/{{ dataset['id'] }}){% else %}{{ dataset['name'] }}{% endif %}
{%- if 'revision' in dataset and 'id' in dataset %} at [{{ dataset['revision'][:7] }}](https://huggingface.co/datasets/{{ dataset['id'] }}/tree/{{ dataset['revision'] }}){% endif %}{% endif %}
{% if dataset['size'] %}* Size: {{ ""{:,}"".format(dataset['size']) }} {{ dataset_type }} samples
{% endif %}* Columns: {% if dataset['columns'] | length == 1 %}{{ dataset['columns'][0] }}{% elif dataset['columns'] | length == 2 %}{{ dataset['columns'][0] }} and {{ dataset['columns'][1] }}{% else %}{{ dataset['columns'][:-1] | join(', ') }}, and {{ dataset['columns'][-1] }}{% endif %}
{% if dataset['stats_table'] %}* Approximate statistics based on the first {{ [dataset['size'], 1000] | min }} samples:
{{ dataset['stats_table'] }}{% endif %}{% if dataset['examples_table'] %}* Samples:
{{ dataset['examples_table'] }}{% endif %}* Loss: {% if dataset[""loss""][""fullname""].startswith(""sentence_transformers."") %}[<code>{{ dataset[""loss""][""fullname""].split(""."")[-1] }}</code>](https://sbert.net/docs/package_reference/cross_encoder/losses.html#{{ dataset[""loss""][""fullname""].split(""."")[-1].lower() }}){% else %}<code>{{ dataset[""loss""][""fullname""] }}</code>{% endif %}{% if ""config_code"" in dataset[""loss""] %} with these parameters:
{{ dataset[""loss""][""config_code""] }}{% endif %}
{% if dataset_list | length > 3 %}</details>
{% endif %}{% endfor %}{% endif %}{% endfor -%}",reference
"1. Fork and create a feature branch.
2. Add or update scripts/tests as needed.
3. Ensure style checks (`flake8`, `black`) pass.
4. Submit a pull request detailing your enhancements.",reference
"<!-- This section describes the source data (e.g. news text and headlines, social media posts, translated sentences, ...). -->",reference
# Natural Language Toolkit (NLTK) Authors,other
"<!-- If there is a paper or blog post introducing the dataset, the APA and Bibtex information for that should go in this section. -->",reference
## ⚙️ Installation,installation
<details><summary>Click to see the direct usage in Transformers</summary>,usage
## Original Authors,other
"```bash
pip install -U sentence-transformers
```",installation
## More Information [optional],other
<details><summary>Click to expand</summary>,usage
<details><summary>Click to expand</summary>,usage
"Permission is hereby granted, free of charge, to any person obtaining a copy of
this software and associated documentation files (the ""Software""), to deal with
the Software without restriction, including without limitation the rights to
use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies
of the Software, and to permit persons to whom the Software is furnished to do
so, subject to the following conditions:",other
### Model Architecture and Objective,other
"### Installation
```bash
git clone https://github.com/RohitRajpoot/reposage.git
cd reposage
pip install -r requirements.txt",installation
"### Usage
# Build the index
reposage index",usage
"{% for name, value in non_default_hyperparameters.items() %}- `{{ name }}`: {{ value }}
{% endfor %}{%- endif %}
#### All Hyperparameters
<details><summary>Click to expand</summary>",usage
"* After labeling paragraphs (e.g. `installation`, `usage`, `configuration`, `api_reference`, `troubleshooting`), train an H2O AutoML model to predict the category of any incoming question.
   * This routing step ensures each query is answered by the most relevant section of your docs.",installation
"## Training Details
{% for dataset_type, dataset_list in [(""training"", train_datasets), (""evaluation"", eval_datasets)] %}{% if dataset_list %}
### {{ dataset_type.title() }} Dataset{{""s"" if dataset_list | length > 1 else """"}}
{% for dataset in dataset_list %}{% if dataset_list | length > 3 %}<details><summary>{{ dataset['name'] or 'Unnamed Dataset' }}</summary>
{% endif %}
#### {{ dataset['name'] or 'Unnamed Dataset' }}
{% if dataset['name'] %}
* Dataset: {% if 'id' in dataset %}[{{ dataset['name'] }}](https://huggingface.co/datasets/{{ dataset['id'] }}){% else %}{{ dataset['name'] }}{% endif %}
{%- if 'revision' in dataset and 'id' in dataset %} at [{{ dataset['revision'][:7] }}](https://huggingface.co/datasets/{{ dataset['id'] }}/tree/{{ dataset['revision'] }}){% endif %}{% endif %}
{% if dataset['size'] %}* Size: {{ ""{:,}"".format(dataset['size']) }} {{ dataset_type }} samples
{% endif %}* Columns: {% if dataset['columns'] | length == 1 %}{{ dataset['columns'][0] }}{% elif dataset['columns'] | length == 2 %}{{ dataset['columns'][0] }} and {{ dataset['columns'][1] }}{% else %}{{ dataset['columns'][:-1] | join(', ') }}, and {{ dataset['columns'][-1] }}{% endif %}
{% if dataset['stats_table'] %}* Approximate statistics based on the first {{ [dataset['size'], 1000] | min }} samples:
{{ dataset['stats_table'] }}{% endif %}{% if dataset['examples_table'] %}* Samples:
{{ dataset['examples_table'] }}{% endif %}* Loss: {% if dataset[""loss""][""fullname""].startswith(""sentence_transformers."") %}[<code>{{ dataset[""loss""][""fullname""].split(""."")[-1] }}</code>](https://sbert.net/docs/package_reference/cross_encoder/losses.html#{{ dataset[""loss""][""fullname""].split(""."")[-1].lower() }}){% else %}<code>{{ dataset[""loss""][""fullname""] }}</code>{% endif %}{% if ""config_code"" in dataset[""loss""] %} with these parameters:
{{ dataset[""loss""][""config_code""] }}{% endif %}
{% if dataset_list | length > 3 %}</details>
{% endif %}{% endfor %}{% endif %}{% endfor -%}",reference
"<!-- If there is a paper or blog post introducing the dataset, the APA and Bibtex information for that should go in this section. -->",reference
<details><summary>Click to see the direct usage in Transformers</summary>,usage
"# 3) Smoke-test CLI
python -m assist.main chat ""hello world""",usage
---,other
"### Usage
# Build the index
reposage index",usage
"{% for name, value in non_default_hyperparameters.items() %}- `{{ name }}`: {{ value }}
{% endfor %}{%- endif %}
#### All Hyperparameters
<details><summary>Click to expand</summary>",usage
"# JupyterChart
This directory contains the JavaScript portion of the Altair `JupyterChart`. The `JupyterChart` is based on the [AnyWidget](https://anywidget.dev/) project.",other
1. **Corpus Preparation**,other
"## Training Details
{% for dataset_type, dataset_list in [(""training"", train_datasets), (""evaluation"", eval_datasets)] %}{% if dataset_list %}
### {{ dataset_type.title() }} Dataset{{""s"" if dataset_list | length > 1 else """"}}
{% for dataset in dataset_list %}{% if dataset_list | length > 3 %}<details><summary>{{ dataset['name'] or 'Unnamed Dataset' }}</summary>
{% endif %}
#### {{ dataset['name'] or 'Unnamed Dataset' }}
{% if dataset['name'] %}
* Dataset: {% if 'id' in dataset %}[{{ dataset['name'] }}](https://huggingface.co/datasets/{{ dataset['id'] }}){% else %}{{ dataset['name'] }}{% endif %}
{%- if 'revision' in dataset and 'id' in dataset %} at [{{ dataset['revision'][:7] }}](https://huggingface.co/datasets/{{ dataset['id'] }}/tree/{{ dataset['revision'] }}){% endif %}{% endif %}
{% if dataset['size'] %}* Size: {{ ""{:,}"".format(dataset['size']) }} {{ dataset_type }} samples
{% endif %}* Columns: {% if dataset['columns'] | length == 1 %}{{ dataset['columns'][0] }}{% elif dataset['columns'] | length == 2 %}{{ dataset['columns'][0] }} and {{ dataset['columns'][1] }}{% else %}{{ dataset['columns'][:-1] | join(', ') }}, and {{ dataset['columns'][-1] }}{% endif %}
{% if dataset['stats_table'] %}* Approximate statistics based on the first {{ [dataset['size'], 1000] | min }} samples:
{{ dataset['stats_table'] }}{% endif %}{% if dataset['examples_table'] %}* Samples:
{{ dataset['examples_table'] }}{% endif %}* Loss: {% if dataset[""loss""][""fullname""].startswith(""sentence_transformers."") %}[<code>{{ dataset[""loss""][""fullname""].split(""."")[-1] }}</code>](https://sbert.net/docs/package_reference/cross_encoder/losses.html#{{ dataset[""loss""][""fullname""].split(""."")[-1].lower() }}){% else %}<code>{{ dataset[""loss""][""fullname""] }}</code>{% endif %}{% if ""config_code"" in dataset[""loss""] %} with these parameters:
{{ dataset[""loss""][""config_code""] }}{% endif %}
{% if dataset_list | length > 3 %}</details>
{% endif %}{% endfor %}{% endif %}{% endfor -%}",reference
"```bash
pip install -U sentence-transformers
```",installation
## 🚀 Live Demo,other
"| Team Member | Role(s)                 | Responsibilities                             |
|-------------|-------------------------|----------------------------------------------|
| Rohit       | CEO / Product Lead,     | • Define product vision & roadmap<br>• Manage scope & stakeholder communication<br>• Write technical documentation |
| Kishan      | CTO / DevOps Lead,      | • Develop DeepSeek index & CLI scaffold<br>• Set up CI/CD pipelines<br>• Manage Hugging Face Spaces deployments   |
| Maitri      | Data Scientist / AI Lead| • Implement Bayesian Q&A layer<br>• Design & tune priors<br>• Oversee transformer fallback prompts          |",usage
## Uses,other
"3. Neither the name of the copyright holder nor the names of its
   contributors may be used to endorse or promote products derived from
   this software without specific prior written permission.",other
"In `app.py`, configure paths to:",reference
"1. **Route** via the H2O classifier to select a subset of paragraphs.
     2. **Retrieve** top‑k relevant passages with FAISS.
     3. **Rescore** with a Bayesian posterior to boost high-confidence results.
     4. **Fallback**: if the posterior confidence is below threshold, generate an answer using a compact Flan‑T5 model.",reference
"| Team Member | Role(s)                 | Responsibilities                             |
|-------------|-------------------------|----------------------------------------------|
| Rohit       | CEO / Product Lead,     | • Define product vision & roadmap<br>• Manage scope & stakeholder communication<br>• Write technical documentation |
| Kishan      | CTO / DevOps Lead,      | • Develop DeepSeek index & CLI scaffold<br>• Set up CI/CD pipelines<br>• Manage Hugging Face Spaces deployments   |
| Maitri      | Data Scientist / AI Lead| • Implement Bayesian Q&A layer<br>• Design & tune priors<br>• Oversee transformer fallback prompts          |",usage
First install the Sentence Transformers library:,installation
"```bash
pip install -U sentence-transformers
```",installation
"## Training Details
{% for dataset_type, dataset_list in [(""training"", train_datasets), (""evaluation"", eval_datasets)] %}{% if dataset_list %}
### {{ dataset_type.title() }} Dataset{{""s"" if dataset_list | length > 1 else """"}}
{% for dataset in dataset_list %}{% if dataset_list | length > 3 %}<details><summary>{{ dataset['name'] or 'Unnamed Dataset' }}</summary>
{% endif %}
#### {{ dataset['name'] or 'Unnamed Dataset' }}
{% if dataset['name'] %}
* Dataset: {% if 'id' in dataset %}[{{ dataset['name'] }}](https://huggingface.co/datasets/{{ dataset['id'] }}){% else %}{{ dataset['name'] }}{% endif %}
{%- if 'revision' in dataset and 'id' in dataset %} at [{{ dataset['revision'][:7] }}](https://huggingface.co/datasets/{{ dataset['id'] }}/tree/{{ dataset['revision'] }}){% endif %}{% endif %}
{% if dataset['size'] %}* Size: {{ ""{:,}"".format(dataset['size']) }} {{ dataset_type }} samples
{% endif %}* Columns: {% if dataset['columns'] | length == 1 %}{{ dataset['columns'][0] }}{% elif dataset['columns'] | length == 2 %}{{ dataset['columns'][0] }} and {{ dataset['columns'][1] }}{% else %}{{ dataset['columns'][:-1] | join(', ') }}, and {{ dataset['columns'][-1] }}{% endif %}
{% if dataset['stats_table'] %}* Approximate statistics based on the first {{ [dataset['size'], 1000] | min }} samples:
{{ dataset['stats_table'] }}{% endif %}{% if dataset['examples_table'] %}* Samples:
{{ dataset['examples_table'] }}{% endif %}* Loss: {% if dataset[""loss""][""fullname""].startswith(""sentence_transformers."") %}[<code>{{ dataset[""loss""][""fullname""].split(""."")[-1] }}</code>](https://sbert.net/docs/package_reference/sentence_transformer/losses.html#{{ dataset[""loss""][""fullname""].split(""."")[-1].lower() }}){% else %}<code>{{ dataset[""loss""][""fullname""] }}</code>{% endif %}{% if ""config_code"" in dataset[""loss""] %} with these parameters:
{{ dataset[""loss""][""config_code""] }}{% endif %}
{% if dataset_list | length > 3 %}</details>
{% endif %}{% endfor %}{% endif %}{% endfor -%}",reference
"### Metrics
{% for metrics in eval_metrics %}
#### {{ metrics.description }}
{% if metrics.dataset_name %}
* Dataset{% if metrics.dataset_name is not string and metrics.dataset_name | length > 1 %}s{% endif %}: {% if metrics.dataset_name is string -%}
        `{{ metrics.dataset_name }}`
    {%- else -%}
        {%- for name in metrics.dataset_name -%}
            `{{ name }}`
            {%- if not loop.last -%}
                {%- if loop.index == metrics.dataset_name | length - 1 %} and {% else -%}, {% endif -%}
            {%- endif -%}
        {%- endfor -%}
    {%- endif -%}
{%- endif %}
* Evaluated with {% if metrics.class_name.startswith(""sentence_transformers."") %}[<code>{{ metrics.class_name.split(""."")[-1] }}</code>](https://sbert.net/docs/package_reference/cross_encoder/evaluation.html#sentence_transformers.cross_encoder.evaluation.{{ metrics.class_name.split(""."")[-1] }}){% else %}<code>{{ metrics.class_name }}</code>{% endif %}{% if metrics.config_code %} with these parameters:
{{ metrics.config_code }}{% endif %}",reference
"```bash
pip install -U sentence-transformers
```",installation
<details><summary>Click to expand</summary>,usage
"{{ glossary | default(""[More Information Needed]"", true)}}",other
<details><summary>Click to see the direct usage in Transformers</summary>,usage
## Model Card Contact,other
"<!-- This section describes the source data (e.g. news text and headlines, social media posts, translated sentences, ...). -->",reference
"## Training Details
{% for dataset_type, dataset_list in [(""training"", train_datasets), (""evaluation"", eval_datasets)] %}{% if dataset_list %}
### {{ dataset_type.title() }} Dataset{{""s"" if dataset_list | length > 1 else """"}}
{% for dataset in dataset_list %}{% if dataset_list | length > 3 %}<details><summary>{{ dataset['name'] or 'Unnamed Dataset' }}</summary>
{% endif %}
#### {{ dataset['name'] or 'Unnamed Dataset' }}
{% if dataset['name'] %}
* Dataset: {% if 'id' in dataset %}[{{ dataset['name'] }}](https://huggingface.co/datasets/{{ dataset['id'] }}){% else %}{{ dataset['name'] }}{% endif %}
{%- if 'revision' in dataset and 'id' in dataset %} at [{{ dataset['revision'][:7] }}](https://huggingface.co/datasets/{{ dataset['id'] }}/tree/{{ dataset['revision'] }}){% endif %}{% endif %}
{% if dataset['size'] %}* Size: {{ ""{:,}"".format(dataset['size']) }} {{ dataset_type }} samples
{% endif %}* Columns: {% if dataset['columns'] | length == 1 %}{{ dataset['columns'][0] }}{% elif dataset['columns'] | length == 2 %}{{ dataset['columns'][0] }} and {{ dataset['columns'][1] }}{% else %}{{ dataset['columns'][:-1] | join(', ') }}, and {{ dataset['columns'][-1] }}{% endif %}
{% if dataset['stats_table'] %}* Approximate statistics based on the first {{ [dataset['size'], 1000] | min }} samples:
{{ dataset['stats_table'] }}{% endif %}{% if dataset['examples_table'] %}* Samples:
{{ dataset['examples_table'] }}{% endif %}* Loss: {% if dataset[""loss""][""fullname""].startswith(""sentence_transformers."") %}[<code>{{ dataset[""loss""][""fullname""].split(""."")[-1] }}</code>](https://sbert.net/docs/package_reference/sentence_transformer/losses.html#{{ dataset[""loss""][""fullname""].split(""."")[-1].lower() }}){% else %}<code>{{ dataset[""loss""][""fullname""] }}</code>{% endif %}{% if ""config_code"" in dataset[""loss""] %} with these parameters:
{{ dataset[""loss""][""config_code""] }}{% endif %}
{% if dataset_list | length > 3 %}</details>
{% endif %}{% endfor %}{% endif %}{% endfor -%}",reference
"{%- if eval_lines %}
### Training Logs
{% if hide_eval_lines %}<details><summary>Click to expand</summary>",usage
"# Query
reposage query ""What is DeepSeek?"" --threshold 0.3",usage
"In `app.py`, configure paths to:",reference
"{%- if eval_lines %}
### Training Logs
{% if hide_eval_lines %}<details><summary>Click to expand</summary>",usage
"# 2) Install dependencies
pip install --upgrade pip setuptools wheel
pip install -r requirements.txt",installation
First install the Sentence Transformers library:,installation
"{{ dataset_card_authors | default(""[More Information Needed]"", true)}}",other
"# 2) Install dependencies
pip install --upgrade pip setuptools wheel
pip install -r requirements.txt",installation
"### Metrics
{% for metrics in eval_metrics %}
#### {{ metrics.description }}
{% if metrics.dataset_name %}
* Dataset{% if metrics.dataset_name is not string and metrics.dataset_name | length > 1 %}s{% endif %}: {% if metrics.dataset_name is string -%}
        `{{ metrics.dataset_name }}`
    {%- else -%}
        {%- for name in metrics.dataset_name -%}
            `{{ name }}`
            {%- if not loop.last -%}
                {%- if loop.index == metrics.dataset_name | length - 1 %} and {% else -%}, {% endif -%}
            {%- endif -%}
        {%- endfor -%}
    {%- endif -%}
{%- endif %}
* Evaluated with {% if metrics.class_name.startswith(""sentence_transformers."") %}[<code>{{ metrics.class_name.split(""."")[-1] }}</code>](https://sbert.net/docs/package_reference/sentence_transformer/evaluation.html#sentence_transformers.evaluation.{{ metrics.class_name.split(""."")[-1] }}){% else %}<code>{{ metrics.class_name }}</code>{% endif %}{% if metrics.config_code %} with these parameters:
{{ metrics.config_code }}{% endif %}",reference
"# Slide 3: Bayesian Q&A Layer
- Cosine-likelihood softmax → posterior",reference
<details><summary>Click to expand</summary>,usage
### Full Model Architecture,other
"* After labeling paragraphs (e.g. `installation`, `usage`, `configuration`, `api_reference`, `troubleshooting`), train an H2O AutoML model to predict the category of any incoming question.
   * This routing step ensures each query is answered by the most relevant section of your docs.",installation
"### Metrics
{% for metrics in eval_metrics %}
#### {{ metrics.description }}
{% if metrics.dataset_name %}
* Dataset{% if metrics.dataset_name is not string and metrics.dataset_name | length > 1 %}s{% endif %}: {% if metrics.dataset_name is string -%}
        `{{ metrics.dataset_name }}`
    {%- else -%}
        {%- for name in metrics.dataset_name -%}
            `{{ name }}`
            {%- if not loop.last -%}
                {%- if loop.index == metrics.dataset_name | length - 1 %} and {% else -%}, {% endif -%}
            {%- endif -%}
        {%- endfor -%}
    {%- endif -%}
{%- endif %}
* Evaluated with {% if metrics.class_name.startswith(""sentence_transformers."") %}[<code>{{ metrics.class_name.split(""."")[-1] }}</code>](https://sbert.net/docs/package_reference/cross_encoder/evaluation.html#sentence_transformers.cross_encoder.evaluation.{{ metrics.class_name.split(""."")[-1] }}){% else %}<code>{{ metrics.class_name }}</code>{% endif %}{% if metrics.config_code %} with these parameters:
{{ metrics.config_code }}{% endif %}",reference
<!-- This section is meant to convey both technical and sociotechnical limitations. -->,other
"# Or call the REST API
curl -X POST https://<your-space>.hf.space/query \
     -d '{""question"":""What is DeepSeek?""}'",reference
<details><summary>Click to expand</summary>,usage
<details><summary>Click to see the direct usage in Transformers</summary>,usage
"# 3) Smoke-test CLI
python -m assist.main chat ""hello world""",usage
First install the Sentence Transformers library:,installation
"{% for name, value in all_hyperparameters.items() %}- `{{ name }}`: {{ value }}
{% endfor %}
</details>
{% endif %}",other
"```bash
pip install -U sentence-transformers
```",installation
"### Metrics
{% for metrics in eval_metrics %}
#### {{ metrics.description }}
{% if metrics.dataset_name %}
* Dataset{% if metrics.dataset_name is not string and metrics.dataset_name | length > 1 %}s{% endif %}: {% if metrics.dataset_name is string -%}
        `{{ metrics.dataset_name }}`
    {%- else -%}
        {%- for name in metrics.dataset_name -%}
            `{{ name }}`
            {%- if not loop.last -%}
                {%- if loop.index == metrics.dataset_name | length - 1 %} and {% else -%}, {% endif -%}
            {%- endif -%}
        {%- endfor -%}
    {%- endif -%}
{%- endif %}
* Evaluated with {% if metrics.class_name.startswith(""sentence_transformers."") %}[<code>{{ metrics.class_name.split(""."")[-1] }}</code>](https://sbert.net/docs/package_reference/sentence_transformer/evaluation.html#sentence_transformers.evaluation.{{ metrics.class_name.split(""."")[-1] }}){% else %}<code>{{ metrics.class_name }}</code>{% endif %}{% if metrics.config_code %} with these parameters:
{{ metrics.config_code }}{% endif %}",reference
"<!-- This section describes the source data (e.g. news text and headlines, social media posts, translated sentences, ...). -->",reference
<details><summary>Click to expand</summary>,usage
"### Installation
```bash
git clone https://github.com/RohitRajpoot/reposage.git
cd reposage
pip install -r requirements.txt",installation
First install the Sentence Transformers library:,installation
"```bash
pip install -U sentence-transformers
```",installation
"1. Fork and create a feature branch.
2. Add or update scripts/tests as needed.
3. Ensure style checks (`flake8`, `black`) pass.
4. Submit a pull request detailing your enhancements.",reference
"<!-- This section provides information about throughput, start/end time, checkpoint size if relevant, etc. -->",other
"1. **Route** via the H2O classifier to select a subset of paragraphs.
     2. **Retrieve** top‑k relevant passages with FAISS.
     3. **Rescore** with a Bayesian posterior to boost high-confidence results.
     4. **Fallback**: if the posterior confidence is below threshold, generate an answer using a compact Flan‑T5 model.",reference
"### Usage
# Build the index
reposage index",usage
"This repository contains the core components of **RepoSage**, a hybrid AI-powered study assistant that routes student queries to the right documentation sections and delivers precise answers using a blend of classification, retrieval, Bayesian scoring, and generative fallback.",other
"* Recursively scan **all** project markdowns (`reposage/README.md`, other `*.md`).
   * Split each file into paragraphs and export to `data/classification.csv` with an empty `category` column for manual labeling.",other
"{{ citation_apa | default(""[More Information Needed]"", true)}}",other
"1. **Route** via the H2O classifier to select a subset of paragraphs.
     2. **Retrieve** top‑k relevant passages with FAISS.
     3. **Rescore** with a Bayesian posterior to boost high-confidence results.
     4. **Fallback**: if the posterior confidence is below threshold, generate an answer using a compact Flan‑T5 model.",reference
First install the Sentence Transformers library:,installation
"* After labeling paragraphs (e.g. `installation`, `usage`, `configuration`, `api_reference`, `troubleshooting`), train an H2O AutoML model to predict the category of any incoming question.
   * This routing step ensures each query is answered by the most relevant section of your docs.",installation
"{{ bias_recommendations | default(""Users should be made aware of the risks, biases and limitations of the dataset. More information needed for further recommendations."", true)}}",other
"# 3) Smoke-test CLI
python -m assist.main chat ""hello world""",usage
"```
├─ data/
│  └─ classification.csv       # Paragraphs + category column
├─ models/
│  ├─ routing_model.zip       # Trained H2O classifier
│  └─ faiss_index.bin         # FAISS index of embeddings
├─ scripts/
│  ├─ prepare_corpus.py       # Splits & exports paragraphs
│  ├─ train_classifier.py     # H2O AutoML training
│  └─ build_index.py          # Embedding + FAISS index builder
├─ app.py                     # Streamlit (or Gradio) Q&A app
├─ requirements.txt           # Python dependencies
└─ README.md                  # (This file)
```",other
"1. Fork and create a feature branch.
2. Add or update scripts/tests as needed.
3. Ensure style checks (`flake8`, `black`) pass.
4. Submit a pull request detailing your enhancements.",reference
"# Or call the REST API
curl -X POST https://<your-space>.hf.space/query \
     -d '{""question"":""What is DeepSeek?""}'",reference
"## Training Details
{% for dataset_type, dataset_list in [(""training"", train_datasets), (""evaluation"", eval_datasets)] %}{% if dataset_list %}
### {{ dataset_type.title() }} Dataset{{""s"" if dataset_list | length > 1 else """"}}
{% for dataset in dataset_list %}{% if dataset_list | length > 3 %}<details><summary>{{ dataset['name'] or 'Unnamed Dataset' }}</summary>
{% endif %}
#### {{ dataset['name'] or 'Unnamed Dataset' }}
{% if dataset['name'] %}
* Dataset: {% if 'id' in dataset %}[{{ dataset['name'] }}](https://huggingface.co/datasets/{{ dataset['id'] }}){% else %}{{ dataset['name'] }}{% endif %}
{%- if 'revision' in dataset and 'id' in dataset %} at [{{ dataset['revision'][:7] }}](https://huggingface.co/datasets/{{ dataset['id'] }}/tree/{{ dataset['revision'] }}){% endif %}{% endif %}
{% if dataset['size'] %}* Size: {{ ""{:,}"".format(dataset['size']) }} {{ dataset_type }} samples
{% endif %}* Columns: {% if dataset['columns'] | length == 1 %}{{ dataset['columns'][0] }}{% elif dataset['columns'] | length == 2 %}{{ dataset['columns'][0] }} and {{ dataset['columns'][1] }}{% else %}{{ dataset['columns'][:-1] | join(', ') }}, and {{ dataset['columns'][-1] }}{% endif %}
{% if dataset['stats_table'] %}* Approximate statistics based on the first {{ [dataset['size'], 1000] | min }} samples:
{{ dataset['stats_table'] }}{% endif %}{% if dataset['examples_table'] %}* Samples:
{{ dataset['examples_table'] }}{% endif %}* Loss: {% if dataset[""loss""][""fullname""].startswith(""sentence_transformers."") %}[<code>{{ dataset[""loss""][""fullname""].split(""."")[-1] }}</code>](https://sbert.net/docs/package_reference/sentence_transformer/losses.html#{{ dataset[""loss""][""fullname""].split(""."")[-1].lower() }}){% else %}<code>{{ dataset[""loss""][""fullname""] }}</code>{% endif %}{% if ""config_code"" in dataset[""loss""] %} with these parameters:
{{ dataset[""loss""][""config_code""] }}{% endif %}
{% if dataset_list | length > 3 %}</details>
{% endif %}{% endfor %}{% endif %}{% endfor -%}",reference
"# 2) Install dependencies
pip install --upgrade pip setuptools wheel
pip install -r requirements.txt",installation
First install the Sentence Transformers library:,installation
"# Slide 3: Bayesian Q&A Layer
- Cosine-likelihood softmax → posterior",reference
### Recommendations,other
---,other
# Components,other
<details><summary>Click to expand</summary>,usage
"```bash
git clone https://github.com/rohitrajpoot/reposage.git
cd reposage",other
"# Download from the {{ hf_emoji }} Hub
model = CrossEncoder(""{{ model_id | default('cross_encoder_model_id', true) }}"")
# Get scores for pairs of texts
pairs = [
{%- for text in (predict_example or [[""How many calories in an egg"", ""There are on average between 55 and 80 calories in an egg depending on its size.""], [""How many calories in an egg"", ""Egg whites are very low in calories, have no fat, no cholesterol, and are loaded with protein.""], [""How many calories in an egg"", ""Most of the calories in an egg come from the yellow yolk in the center.""]]) %}
    {{ ""%r"" | format(text) }},
{%- endfor %}
]
scores = model.predict(pairs)
print(scores.shape)
# ({{ (predict_example or [""dummy"", ""dummy"", ""dummy""]) | length }},{{ ("" %d"" | format(model_num_labels)) if model_num_labels > 1 else """" }}){% if model_num_labels == 1 %}",other
http://www.apache.org/licenses/LICENSE-2.0,other
"## Goals
- Guide the transformer fallback to produce concise, context-aware answers.
- Maintain consistency with our domain (AML3304 materials).",other
Update this directory using maint_tools/vendor_array_api_extra.sh,reference
### Recommendations,other
"{% for name, value in all_hyperparameters.items() %}- `{{ name }}`: {{ value }}
{% endfor %}
</details>
{% endif %}",other
"### Installation
```bash
git clone https://github.com/RohitRajpoot/reposage.git
cd reposage
pip install -r requirements.txt",installation
<details><summary>Click to see the direct usage in Transformers</summary>,usage
"# Get the similarity scores for the embeddings
similarities = model.similarity(embeddings, embeddings)
print(similarities.shape)
# [{{ (predict_example or [""The weather is lovely today."", ""It's so sunny outside!"", ""He drove to the stadium.""]) | length}}, {{ (predict_example or [""The weather is lovely today."", ""It's so sunny outside!"", ""He drove to the stadium.""]) | length}}]
```",other
"```bash
pip install -U sentence-transformers
```",installation
"## Features
- **DeepSeek Index** of lecture slides, code, and readings  
- **Bayesian Posterior Scoring** for precise answer selection  
- **Transformer Fallback** via HF text2text when confidence is low  
- **CLI & REST API** for local and web deployment  
- **CI/CD** via GitHub Actions → Hugging Face Spaces",usage
---,other
First install the Sentence Transformers library:,installation
<details><summary>Click to see the direct usage in Transformers</summary>,usage
"<!-- This section describes the source data (e.g. news text and headlines, social media posts, translated sentences, ...). -->",reference
"<!-- This section describes the source data (e.g. news text and headlines, social media posts, translated sentences, ...). -->",reference
"### Metrics
{% for metrics in eval_metrics %}
#### {{ metrics.description }}
{% if metrics.dataset_name %}
* Dataset{% if metrics.dataset_name is not string and metrics.dataset_name | length > 1 %}s{% endif %}: {% if metrics.dataset_name is string -%}
        `{{ metrics.dataset_name }}`
    {%- else -%}
        {%- for name in metrics.dataset_name -%}
            `{{ name }}`
            {%- if not loop.last -%}
                {%- if loop.index == metrics.dataset_name | length - 1 %} and {% else -%}, {% endif -%}
            {%- endif -%}
        {%- endfor -%}
    {%- endif -%}
{%- endif %}
* Evaluated with {% if metrics.class_name.startswith(""sentence_transformers."") %}[<code>{{ metrics.class_name.split(""."")[-1] }}</code>](https://sbert.net/docs/package_reference/cross_encoder/evaluation.html#sentence_transformers.cross_encoder.evaluation.{{ metrics.class_name.split(""."")[-1] }}){% else %}<code>{{ metrics.class_name }}</code>{% endif %}{% if metrics.config_code %} with these parameters:
{{ metrics.config_code }}{% endif %}",reference
<details><summary>Click to see the direct usage in Transformers</summary>,usage
"### Usage
# Build the index
reposage index",usage
"## Features
- **DeepSeek Index** of lecture slides, code, and readings  
- **Bayesian Posterior Scoring** for precise answer selection  
- **Transformer Fallback** via HF text2text when confidence is low  
- **CLI & REST API** for local and web deployment  
- **CI/CD** via GitHub Actions → Hugging Face Spaces",usage
"### Installation
```bash
git clone https://github.com/RohitRajpoot/reposage.git
cd reposage
pip install -r requirements.txt",installation
First install the Sentence Transformers library:,installation
<details><summary>Click to expand</summary>,usage
"1. Fork and create a feature branch.
2. Add or update scripts/tests as needed.
3. Ensure style checks (`flake8`, `black`) pass.
4. Submit a pull request detailing your enhancements.",reference
"# {{ model_name if model_name else ""Sentence Transformer model"" }}",other
"<!--
## Model Card Authors",other
First install the Sentence Transformers library:,installation
"<!-- This section describes the data collection and processing process such as data selection criteria, filtering and normalization methods, tools and libraries used, etc. -->",other
"{{ testing_data | default(""[More Information Needed]"", true)}}",other
"# Build the index
reposage index",usage
"{%- if eval_lines %}
### Training Logs
{% if hide_eval_lines %}<details><summary>Click to expand</summary>",usage
"<!-- If there is a paper or blog post introducing the model, the APA and Bibtex information for that should go in this section. -->",reference
"1. Fork and create a feature branch.
2. Add or update scripts/tests as needed.
3. Ensure style checks (`flake8`, `black`) pass.
4. Submit a pull request detailing your enhancements.",reference
### Out-of-Scope Use,other
"<!--
### Downstream Usage (Sentence Transformers)",other
First install the Sentence Transformers library:,installation
"```bash
pip install -U sentence-transformers
```",installation
#### Who are the annotators?,other
"### Metrics
{% for metrics in eval_metrics %}
#### {{ metrics.description }}
{% if metrics.dataset_name %}
* Dataset{% if metrics.dataset_name is not string and metrics.dataset_name | length > 1 %}s{% endif %}: {% if metrics.dataset_name is string -%}
        `{{ metrics.dataset_name }}`
    {%- else -%}
        {%- for name in metrics.dataset_name -%}
            `{{ name }}`
            {%- if not loop.last -%}
                {%- if loop.index == metrics.dataset_name | length - 1 %} and {% else -%}, {% endif -%}
            {%- endif -%}
        {%- endfor -%}
    {%- endif -%}
{%- endif %}
* Evaluated with {% if metrics.class_name.startswith(""sentence_transformers."") %}[<code>{{ metrics.class_name.split(""."")[-1] }}</code>](https://sbert.net/docs/package_reference/sentence_transformer/evaluation.html#sentence_transformers.evaluation.{{ metrics.class_name.split(""."")[-1] }}){% else %}<code>{{ metrics.class_name }}</code>{% endif %}{% if metrics.config_code %} with these parameters:
{{ metrics.config_code }}{% endif %}",reference
## Getting Started,other
"### Usage
# Build the index
reposage index",usage
"1. **Route** via the H2O classifier to select a subset of paragraphs.
     2. **Retrieve** top‑k relevant passages with FAISS.
     3. **Rescore** with a Bayesian posterior to boost high-confidence results.
     4. **Fallback**: if the posterior confidence is below threshold, generate an answer using a compact Flan‑T5 model.",reference
"# Slide 3: Bayesian Q&A Layer
- Cosine-likelihood softmax → posterior",reference
"# Or call the REST API
curl -X POST https://<your-space>.hf.space/query \
     -d '{""question"":""What is DeepSeek?""}'",reference
"* After labeling paragraphs (e.g. `installation`, `usage`, `configuration`, `api_reference`, `troubleshooting`), train an H2O AutoML model to predict the category of any incoming question.
   * This routing step ensures each query is answered by the most relevant section of your docs.",installation
"<!-- This section addresses misuse, malicious use, and uses that the dataset will not work well for. -->",other
"THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS
""AS IS"" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT
LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR
A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT
HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED
TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.",other
First install the Sentence Transformers library:,installation
"{{ metrics.table }}
{%- endfor %}{% endif %}
<!--
## Bias, Risks and Limitations",other
## ⚙️ Installation,installation
"Then you can load this model and run inference.
```python
from sentence_transformers import CrossEncoder",other
"| Team Member | Role(s)                 | Responsibilities                             |
|-------------|-------------------------|----------------------------------------------|
| Rohit       | CEO / Product Lead,     | • Define product vision & roadmap<br>• Manage scope & stakeholder communication<br>• Write technical documentation |
| Kishan      | CTO / DevOps Lead,      | • Develop DeepSeek index & CLI scaffold<br>• Set up CI/CD pipelines<br>• Manage Hugging Face Spaces deployments   |
| Maitri      | Data Scientist / AI Lead| • Implement Bayesian Q&A layer<br>• Design & tune priors<br>• Oversee transformer fallback prompts          |",usage
"# Query
reposage query ""What is DeepSeek?"" --threshold 0.3",usage
"{% for name, value in non_default_hyperparameters.items() %}- `{{ name }}`: {{ value }}
{% endfor %}{%- endif %}
#### All Hyperparameters
<details><summary>Click to expand</summary>",usage
"---
# For reference on dataset card metadata, see the spec: https://github.com/huggingface/hub-docs/blob/main/datasetcard.md?plain=1
# Doc / guide: https://huggingface.co/docs/hub/datasets-cards
{{ card_data }}
---",other
"<!-- This section is for the model use when fine-tuned for a task, or when plugged into a larger ecosystem/app -->",other
## Citation [optional],other
<details><summary>Click to expand</summary>,usage
"# Build the index
reposage index",usage
"3. Neither the name of the copyright holder nor the names of its contributors
   may be used to endorse or promote products derived from this software
   without specific prior written permission.",other
"# {{ model_name if model_name else ""Cross Encoder model"" }}",other
"In `app.py`, configure paths to:",reference
"## Training Details
{% for dataset_type, dataset_list in [(""training"", train_datasets), (""evaluation"", eval_datasets)] %}{% if dataset_list %}
### {{ dataset_type.title() }} Dataset{{""s"" if dataset_list | length > 1 else """"}}
{% for dataset in dataset_list %}{% if dataset_list | length > 3 %}<details><summary>{{ dataset['name'] or 'Unnamed Dataset' }}</summary>
{% endif %}
#### {{ dataset['name'] or 'Unnamed Dataset' }}
{% if dataset['name'] %}
* Dataset: {% if 'id' in dataset %}[{{ dataset['name'] }}](https://huggingface.co/datasets/{{ dataset['id'] }}){% else %}{{ dataset['name'] }}{% endif %}
{%- if 'revision' in dataset and 'id' in dataset %} at [{{ dataset['revision'][:7] }}](https://huggingface.co/datasets/{{ dataset['id'] }}/tree/{{ dataset['revision'] }}){% endif %}{% endif %}
{% if dataset['size'] %}* Size: {{ ""{:,}"".format(dataset['size']) }} {{ dataset_type }} samples
{% endif %}* Columns: {% if dataset['columns'] | length == 1 %}{{ dataset['columns'][0] }}{% elif dataset['columns'] | length == 2 %}{{ dataset['columns'][0] }} and {{ dataset['columns'][1] }}{% else %}{{ dataset['columns'][:-1] | join(', ') }}, and {{ dataset['columns'][-1] }}{% endif %}
{% if dataset['stats_table'] %}* Approximate statistics based on the first {{ [dataset['size'], 1000] | min }} samples:
{{ dataset['stats_table'] }}{% endif %}{% if dataset['examples_table'] %}* Samples:
{{ dataset['examples_table'] }}{% endif %}* Loss: {% if dataset[""loss""][""fullname""].startswith(""sentence_transformers."") %}[<code>{{ dataset[""loss""][""fullname""].split(""."")[-1] }}</code>](https://sbert.net/docs/package_reference/cross_encoder/losses.html#{{ dataset[""loss""][""fullname""].split(""."")[-1].lower() }}){% else %}<code>{{ dataset[""loss""][""fullname""] }}</code>{% endif %}{% if ""config_code"" in dataset[""loss""] %} with these parameters:
{{ dataset[""loss""][""config_code""] }}{% endif %}
{% if dataset_list | length > 3 %}</details>
{% endif %}{% endfor %}{% endif %}{% endfor -%}",reference
"- **Documentation:** [Sentence Transformers Documentation](https://sbert.net)
- **Documentation:** [Cross Encoder Documentation](https://www.sbert.net/docs/cross_encoder/usage/usage.html)
- **Repository:** [Sentence Transformers on GitHub](https://github.com/UKPLab/sentence-transformers)
- **Hugging Face:** [Cross Encoders on Hugging Face](https://huggingface.co/models?library=sentence-transformers&other=cross-encoder)",other
"# Query
reposage query ""What is DeepSeek?"" --threshold 0.3",usage
"### Metrics
{% for metrics in eval_metrics %}
#### {{ metrics.description }}
{% if metrics.dataset_name %}
* Dataset{% if metrics.dataset_name is not string and metrics.dataset_name | length > 1 %}s{% endif %}: {% if metrics.dataset_name is string -%}
        `{{ metrics.dataset_name }}`
    {%- else -%}
        {%- for name in metrics.dataset_name -%}
            `{{ name }}`
            {%- if not loop.last -%}
                {%- if loop.index == metrics.dataset_name | length - 1 %} and {% else -%}, {% endif -%}
            {%- endif -%}
        {%- endfor -%}
    {%- endif -%}
{%- endif %}
* Evaluated with {% if metrics.class_name.startswith(""sentence_transformers."") %}[<code>{{ metrics.class_name.split(""."")[-1] }}</code>](https://sbert.net/docs/package_reference/sentence_transformer/evaluation.html#sentence_transformers.evaluation.{{ metrics.class_name.split(""."")[-1] }}){% else %}<code>{{ metrics.class_name }}</code>{% endif %}{% if metrics.config_code %} with these parameters:
{{ metrics.config_code }}{% endif %}",reference
"### Installation
```bash
git clone https://github.com/RohitRajpoot/reposage.git
cd reposage
pip install -r requirements.txt",installation
"# Or call the REST API
curl -X POST https://<your-space>.hf.space/query \
     -d '{""question"":""What is DeepSeek?""}'",reference
"### Installation
```bash
git clone https://github.com/RohitRajpoot/reposage.git
cd reposage
pip install -r requirements.txt",installation
"{{ compute_infrastructure | default(""[More Information Needed]"", true)}}",other
First install the Sentence Transformers library:,installation
"```bash
pip install -U sentence-transformers
```",installation
<details><summary>Click to see the direct usage in Transformers</summary>,usage
### Model Description,other
"```bash
pip install -U sentence-transformers
```",installation
"1. **Route** via the H2O classifier to select a subset of paragraphs.
     2. **Retrieve** top‑k relevant passages with FAISS.
     3. **Rescore** with a Bayesian posterior to boost high-confidence results.
     4. **Fallback**: if the posterior confidence is below threshold, generate an answer using a compact Flan‑T5 model.",reference
#### Software,other
"<!--
### Recommendations",other
First install the Sentence Transformers library:,installation
"# Slide 1: Architecture Overview
- User → RepoSage CLI → index/query modules",usage
"{{ downstream_use | default(""[More Information Needed]"", true)}}",other
"### Installation
```bash
git clone https://github.com/RohitRajpoot/reposage.git
cd reposage
pip install -r requirements.txt",installation
"1. Fork and create a feature branch.
2. Add or update scripts/tests as needed.
3. Ensure style checks (`flake8`, `black`) pass.
4. Submit a pull request detailing your enhancements.",reference
"# Download from the {{ hf_emoji }} Hub
model = SentenceTransformer(""{{ model_id | default('sentence_transformers_model_id', true) }}"")
# Run inference
sentences = [
{%- for text in (predict_example or [""The weather is lovely today."", ""It's so sunny outside!"", ""He drove to the stadium.""]) %}
    {{ ""%r"" | format(text) }},
{%- endfor %}
]
embeddings = model.encode(sentences)
print(embeddings.shape)
# [{{ (predict_example or [""The weather is lovely today."", ""It's so sunny outside!"", ""He drove to the stadium.""]) | length}}, {{ output_dimensionality | default(1024, true) }}]",other
"<!--
## Model Card Contact",other
Update this directory using maint_tools/vendor_array_api_compat.sh,reference
---,other
"# Slide 3: Bayesian Q&A Layer
- Cosine-likelihood softmax → posterior",reference
"### Usage
# Build the index
reposage index",usage
"{% for name, value in non_default_hyperparameters.items() %}- `{{ name }}`: {{ value }}
{% endfor %}{%- endif %}
#### All Hyperparameters
<details><summary>Click to expand</summary>",usage
"# Slide 3: Bayesian Q&A Layer
- Cosine-likelihood softmax → posterior",reference
"# 2) Install dependencies
pip install --upgrade pip setuptools wheel
pip install -r requirements.txt",installation
Update this directory using maint_tools/vendor_array_api_extra.sh,reference
"# Or call the REST API
curl -X POST https://<your-space>.hf.space/query \
     -d '{""question"":""What is DeepSeek?""}'",reference
"</details>
-->",other
"{% if all_hyperparameters %}
### Training Hyperparameters
{% if non_default_hyperparameters -%}
#### Non-Default Hyperparameters",other
Update this directory using maint_tools/vendor_array_api_compat.sh,reference
"<!--
### Recommendations",other
<details><summary>Click to see the direct usage in Transformers</summary>,usage
"1. **Route** via the H2O classifier to select a subset of paragraphs.
     2. **Retrieve** top‑k relevant passages with FAISS.
     3. **Rescore** with a Bayesian posterior to boost high-confidence results.
     4. **Fallback**: if the posterior confidence is below threshold, generate an answer using a compact Flan‑T5 model.",reference
"# Query
reposage query ""What is DeepSeek?"" --threshold 0.3",usage
"## Training Details
{% for dataset_type, dataset_list in [(""training"", train_datasets), (""evaluation"", eval_datasets)] %}{% if dataset_list %}
### {{ dataset_type.title() }} Dataset{{""s"" if dataset_list | length > 1 else """"}}
{% for dataset in dataset_list %}{% if dataset_list | length > 3 %}<details><summary>{{ dataset['name'] or 'Unnamed Dataset' }}</summary>
{% endif %}
#### {{ dataset['name'] or 'Unnamed Dataset' }}
{% if dataset['name'] %}
* Dataset: {% if 'id' in dataset %}[{{ dataset['name'] }}](https://huggingface.co/datasets/{{ dataset['id'] }}){% else %}{{ dataset['name'] }}{% endif %}
{%- if 'revision' in dataset and 'id' in dataset %} at [{{ dataset['revision'][:7] }}](https://huggingface.co/datasets/{{ dataset['id'] }}/tree/{{ dataset['revision'] }}){% endif %}{% endif %}
{% if dataset['size'] %}* Size: {{ ""{:,}"".format(dataset['size']) }} {{ dataset_type }} samples
{% endif %}* Columns: {% if dataset['columns'] | length == 1 %}{{ dataset['columns'][0] }}{% elif dataset['columns'] | length == 2 %}{{ dataset['columns'][0] }} and {{ dataset['columns'][1] }}{% else %}{{ dataset['columns'][:-1] | join(', ') }}, and {{ dataset['columns'][-1] }}{% endif %}
{% if dataset['stats_table'] %}* Approximate statistics based on the first {{ [dataset['size'], 1000] | min }} samples:
{{ dataset['stats_table'] }}{% endif %}{% if dataset['examples_table'] %}* Samples:
{{ dataset['examples_table'] }}{% endif %}* Loss: {% if dataset[""loss""][""fullname""].startswith(""sentence_transformers."") %}[<code>{{ dataset[""loss""][""fullname""].split(""."")[-1] }}</code>](https://sbert.net/docs/package_reference/sentence_transformer/losses.html#{{ dataset[""loss""][""fullname""].split(""."")[-1].lower() }}){% else %}<code>{{ dataset[""loss""][""fullname""] }}</code>{% endif %}{% if ""config_code"" in dataset[""loss""] %} with these parameters:
{{ dataset[""loss""][""config_code""] }}{% endif %}
{% if dataset_list | length > 3 %}</details>
{% endif %}{% endfor %}{% endif %}{% endfor -%}",reference
"<!-- If there is a paper or blog post introducing the model, the APA and Bibtex information for that should go in this section. -->",reference
"### Usage
# Build the index
reposage index",usage
"# Query
reposage query ""What is DeepSeek?"" --threshold 0.3",usage
"# Or call the REST API
curl -X POST https://<your-space>.hf.space/query \
     -d '{""question"":""What is DeepSeek?""}'",reference
"# 2) Install dependencies
pip install --upgrade pip setuptools wheel
pip install -r requirements.txt",installation
"* After labeling paragraphs (e.g. `installation`, `usage`, `configuration`, `api_reference`, `troubleshooting`), train an H2O AutoML model to predict the category of any incoming question.
   * This routing step ensures each query is answered by the most relevant section of your docs.",installation
"### Usage
# Build the index
reposage index",usage
## ⚙️ Installation,installation
Update this directory using maint_tools/vendor_array_api_compat.sh,reference
"<!-- This section describes the source data (e.g. news text and headlines, social media posts, translated sentences, ...). -->",reference
### Direct Usage (Sentence Transformers),other
"## Features
- **DeepSeek Index** of lecture slides, code, and readings  
- **Bayesian Posterior Scoring** for precise answer selection  
- **Transformer Fallback** via HF text2text when confidence is low  
- **CLI & REST API** for local and web deployment  
- **CI/CD** via GitHub Actions → Hugging Face Spaces",usage
"# Slide 3: Bayesian Q&A Layer
- Cosine-likelihood softmax → posterior",reference
"<!-- This section describes the source data (e.g. news text and headlines, social media posts, translated sentences, ...). -->",reference
"<!-- This section describes the source data (e.g. news text and headlines, social media posts, translated sentences, ...). -->",reference
## Model Details,other
#### Factors,other
"```bash
pip install -U sentence-transformers
```",installation
"- **Developed by:** {{ developers | default(""[More Information Needed]"", true)}}
- **Funded by [optional]:** {{ funded_by | default(""[More Information Needed]"", true)}}
- **Shared by [optional]:** {{ shared_by | default(""[More Information Needed]"", true)}}
- **Model type:** {{ model_type | default(""[More Information Needed]"", true)}}
- **Language(s) (NLP):** {{ language | default(""[More Information Needed]"", true)}}
- **License:** {{ license | default(""[More Information Needed]"", true)}}
- **Finetuned from model [optional]:** {{ base_model | default(""[More Information Needed]"", true)}}",other
"{{ glossary | default(""[More Information Needed]"", true)}}",other
#NAME?,other
"* After labeling paragraphs (e.g. `installation`, `usage`, `configuration`, `api_reference`, `troubleshooting`), train an H2O AutoML model to predict the category of any incoming question.
   * This routing step ensures each query is answered by the most relevant section of your docs.",installation
"# Slide 1: Architecture Overview
- User → RepoSage CLI → index/query modules",usage
"*What are recommendations with respect to the foreseeable issues? For example, filtering explicit content.*
-->",other
First install the Sentence Transformers library:,installation
### Out-of-Scope Use,other
First install the Sentence Transformers library:,installation
"* Build a FAISS index over all paragraphs (using sentence-transformer embeddings).
   * On query:",other
"# Build the index
reposage index",usage
"# Query
reposage query ""What is DeepSeek?"" --threshold 0.3",usage
"{{ bias_risks_limitations | default(""[More Information Needed]"", true)}}",other
"| Team Member | Role(s)                 | Responsibilities                             |
|-------------|-------------------------|----------------------------------------------|
| Rohit       | CEO / Product Lead,     | • Define product vision & roadmap<br>• Manage scope & stakeholder communication<br>• Write technical documentation |
| Kishan      | CTO / DevOps Lead,      | • Develop DeepSeek index & CLI scaffold<br>• Set up CI/CD pipelines<br>• Manage Hugging Face Spaces deployments   |
| Maitri      | Data Scientist / AI Lead| • Implement Bayesian Q&A layer<br>• Design & tune priors<br>• Oversee transformer fallback prompts          |",usage
"### Usage
# Build the index
reposage index",usage
"### Installation
```bash
git clone https://github.com/RohitRajpoot/reposage.git
cd reposage
pip install -r requirements.txt",installation
"* After labeling paragraphs (e.g. `installation`, `usage`, `configuration`, `api_reference`, `troubleshooting`), train an H2O AutoML model to predict the category of any incoming question.
   * This routing step ensures each query is answered by the most relevant section of your docs.",installation
"# Query
reposage query ""What is DeepSeek?"" --threshold 0.3",usage
"* After labeling paragraphs (e.g. `installation`, `usage`, `configuration`, `api_reference`, `troubleshooting`), train an H2O AutoML model to predict the category of any incoming question.
   * This routing step ensures each query is answered by the most relevant section of your docs.",installation
"## Training Details
{% for dataset_type, dataset_list in [(""training"", train_datasets), (""evaluation"", eval_datasets)] %}{% if dataset_list %}
### {{ dataset_type.title() }} Dataset{{""s"" if dataset_list | length > 1 else """"}}
{% for dataset in dataset_list %}{% if dataset_list | length > 3 %}<details><summary>{{ dataset['name'] or 'Unnamed Dataset' }}</summary>
{% endif %}
#### {{ dataset['name'] or 'Unnamed Dataset' }}
{% if dataset['name'] %}
* Dataset: {% if 'id' in dataset %}[{{ dataset['name'] }}](https://huggingface.co/datasets/{{ dataset['id'] }}){% else %}{{ dataset['name'] }}{% endif %}
{%- if 'revision' in dataset and 'id' in dataset %} at [{{ dataset['revision'][:7] }}](https://huggingface.co/datasets/{{ dataset['id'] }}/tree/{{ dataset['revision'] }}){% endif %}{% endif %}
{% if dataset['size'] %}* Size: {{ ""{:,}"".format(dataset['size']) }} {{ dataset_type }} samples
{% endif %}* Columns: {% if dataset['columns'] | length == 1 %}{{ dataset['columns'][0] }}{% elif dataset['columns'] | length == 2 %}{{ dataset['columns'][0] }} and {{ dataset['columns'][1] }}{% else %}{{ dataset['columns'][:-1] | join(', ') }}, and {{ dataset['columns'][-1] }}{% endif %}
{% if dataset['stats_table'] %}* Approximate statistics based on the first {{ [dataset['size'], 1000] | min }} samples:
{{ dataset['stats_table'] }}{% endif %}{% if dataset['examples_table'] %}* Samples:
{{ dataset['examples_table'] }}{% endif %}* Loss: {% if dataset[""loss""][""fullname""].startswith(""sentence_transformers."") %}[<code>{{ dataset[""loss""][""fullname""].split(""."")[-1] }}</code>](https://sbert.net/docs/package_reference/sentence_transformer/losses.html#{{ dataset[""loss""][""fullname""].split(""."")[-1].lower() }}){% else %}<code>{{ dataset[""loss""][""fullname""] }}</code>{% endif %}{% if ""config_code"" in dataset[""loss""] %} with these parameters:
{{ dataset[""loss""][""config_code""] }}{% endif %}
{% if dataset_list | length > 3 %}</details>
{% endif %}{% endfor %}{% endif %}{% endfor -%}",reference
"{% for name, value in non_default_hyperparameters.items() %}- `{{ name }}`: {{ value }}
{% endfor %}{%- endif %}
#### All Hyperparameters
<details><summary>Click to expand</summary>",usage
"## Training Details
{% for dataset_type, dataset_list in [(""training"", train_datasets), (""evaluation"", eval_datasets)] %}{% if dataset_list %}
### {{ dataset_type.title() }} Dataset{{""s"" if dataset_list | length > 1 else """"}}
{% for dataset in dataset_list %}{% if dataset_list | length > 3 %}<details><summary>{{ dataset['name'] or 'Unnamed Dataset' }}</summary>
{% endif %}
#### {{ dataset['name'] or 'Unnamed Dataset' }}
{% if dataset['name'] %}
* Dataset: {% if 'id' in dataset %}[{{ dataset['name'] }}](https://huggingface.co/datasets/{{ dataset['id'] }}){% else %}{{ dataset['name'] }}{% endif %}
{%- if 'revision' in dataset and 'id' in dataset %} at [{{ dataset['revision'][:7] }}](https://huggingface.co/datasets/{{ dataset['id'] }}/tree/{{ dataset['revision'] }}){% endif %}{% endif %}
{% if dataset['size'] %}* Size: {{ ""{:,}"".format(dataset['size']) }} {{ dataset_type }} samples
{% endif %}* Columns: {% if dataset['columns'] | length == 1 %}{{ dataset['columns'][0] }}{% elif dataset['columns'] | length == 2 %}{{ dataset['columns'][0] }} and {{ dataset['columns'][1] }}{% else %}{{ dataset['columns'][:-1] | join(', ') }}, and {{ dataset['columns'][-1] }}{% endif %}
{% if dataset['stats_table'] %}* Approximate statistics based on the first {{ [dataset['size'], 1000] | min }} samples:
{{ dataset['stats_table'] }}{% endif %}{% if dataset['examples_table'] %}* Samples:
{{ dataset['examples_table'] }}{% endif %}* Loss: {% if dataset[""loss""][""fullname""].startswith(""sentence_transformers."") %}[<code>{{ dataset[""loss""][""fullname""].split(""."")[-1] }}</code>](https://sbert.net/docs/package_reference/cross_encoder/losses.html#{{ dataset[""loss""][""fullname""].split(""."")[-1].lower() }}){% else %}<code>{{ dataset[""loss""][""fullname""] }}</code>{% endif %}{% if ""config_code"" in dataset[""loss""] %} with these parameters:
{{ dataset[""loss""][""config_code""] }}{% endif %}
{% if dataset_list | length > 3 %}</details>
{% endif %}{% endfor %}{% endif %}{% endfor -%}",reference
"# 2) Install dependencies
pip install --upgrade pip setuptools wheel
pip install -r requirements.txt",installation
### Contributors to the Porter Stemmer,other
<details><summary>Click to expand</summary>,usage
"In `app.py`, configure paths to:",reference
"1. Fork and create a feature branch.
2. Add or update scripts/tests as needed.
3. Ensure style checks (`flake8`, `black`) pass.
4. Submit a pull request detailing your enhancements.",reference
First install the Sentence Transformers library:,installation
"# Or call the REST API
curl -X POST https://<your-space>.hf.space/query \
     -d '{""question"":""What is DeepSeek?""}'",reference
"# 3) Smoke-test CLI
python -m assist.main chat ""hello world""",usage
## Uses,other
"```bash
pip install -U sentence-transformers
```",installation
"# Build the index
reposage index",usage
Update this directory using maint_tools/vendor_array_api_compat.sh,reference
"# Or call the REST API
curl -X POST https://<your-space>.hf.space/query \
     -d '{""question"":""What is DeepSeek?""}'",reference
"## Training Details
{% for dataset_type, dataset_list in [(""training"", train_datasets), (""evaluation"", eval_datasets)] %}{% if dataset_list %}
### {{ dataset_type.title() }} Dataset{{""s"" if dataset_list | length > 1 else """"}}
{% for dataset in dataset_list %}{% if dataset_list | length > 3 %}<details><summary>{{ dataset['name'] or 'Unnamed Dataset' }}</summary>
{% endif %}
#### {{ dataset['name'] or 'Unnamed Dataset' }}
{% if dataset['name'] %}
* Dataset: {% if 'id' in dataset %}[{{ dataset['name'] }}](https://huggingface.co/datasets/{{ dataset['id'] }}){% else %}{{ dataset['name'] }}{% endif %}
{%- if 'revision' in dataset and 'id' in dataset %} at [{{ dataset['revision'][:7] }}](https://huggingface.co/datasets/{{ dataset['id'] }}/tree/{{ dataset['revision'] }}){% endif %}{% endif %}
{% if dataset['size'] %}* Size: {{ ""{:,}"".format(dataset['size']) }} {{ dataset_type }} samples
{% endif %}* Columns: {% if dataset['columns'] | length == 1 %}{{ dataset['columns'][0] }}{% elif dataset['columns'] | length == 2 %}{{ dataset['columns'][0] }} and {{ dataset['columns'][1] }}{% else %}{{ dataset['columns'][:-1] | join(', ') }}, and {{ dataset['columns'][-1] }}{% endif %}
{% if dataset['stats_table'] %}* Approximate statistics based on the first {{ [dataset['size'], 1000] | min }} samples:
{{ dataset['stats_table'] }}{% endif %}{% if dataset['examples_table'] %}* Samples:
{{ dataset['examples_table'] }}{% endif %}* Loss: {% if dataset[""loss""][""fullname""].startswith(""sentence_transformers."") %}[<code>{{ dataset[""loss""][""fullname""].split(""."")[-1] }}</code>](https://sbert.net/docs/package_reference/sentence_transformer/losses.html#{{ dataset[""loss""][""fullname""].split(""."")[-1].lower() }}){% else %}<code>{{ dataset[""loss""][""fullname""] }}</code>{% endif %}{% if ""config_code"" in dataset[""loss""] %} with these parameters:
{{ dataset[""loss""][""config_code""] }}{% endif %}
{% if dataset_list | length > 3 %}</details>
{% endif %}{% endfor %}{% endif %}{% endfor -%}",reference
"# Or call the REST API
curl -X POST https://<your-space>.hf.space/query \
     -d '{""question"":""What is DeepSeek?""}'",reference
## ⚙️ Installation,installation
2. **H2O Classifier (Routing Layer)**,other
"### Installation
```bash
git clone https://github.com/RohitRajpoot/reposage.git
cd reposage
pip install -r requirements.txt",installation
"# Build the index
reposage index",usage
"```bash
pip install -U sentence-transformers
```",installation
"# Slide 5: Deployment & CI/CD
- GitHub Actions → HF Spaces deploy",other
"### Metrics
{% for metrics in eval_metrics %}
#### {{ metrics.description }}
{% if metrics.dataset_name %}
* Dataset{% if metrics.dataset_name is not string and metrics.dataset_name | length > 1 %}s{% endif %}: {% if metrics.dataset_name is string -%}
        `{{ metrics.dataset_name }}`
    {%- else -%}
        {%- for name in metrics.dataset_name -%}
            `{{ name }}`
            {%- if not loop.last -%}
                {%- if loop.index == metrics.dataset_name | length - 1 %} and {% else -%}, {% endif -%}
            {%- endif -%}
        {%- endfor -%}
    {%- endif -%}
{%- endif %}
* Evaluated with {% if metrics.class_name.startswith(""sentence_transformers."") %}[<code>{{ metrics.class_name.split(""."")[-1] }}</code>](https://sbert.net/docs/package_reference/cross_encoder/evaluation.html#sentence_transformers.cross_encoder.evaluation.{{ metrics.class_name.split(""."")[-1] }}){% else %}<code>{{ metrics.class_name }}</code>{% endif %}{% if metrics.config_code %} with these parameters:
{{ metrics.config_code }}{% endif %}",reference
"**THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
CONTRIBUTORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS WITH
THE SOFTWARE.**",other
"1. **Route** via the H2O classifier to select a subset of paragraphs.
     2. **Retrieve** top‑k relevant passages with FAISS.
     3. **Rescore** with a Bayesian posterior to boost high-confidence results.
     4. **Fallback**: if the posterior confidence is below threshold, generate an answer using a compact Flan‑T5 model.",reference
"```bash
pip install -U sentence-transformers
```",installation
"### Metrics
{% for metrics in eval_metrics %}
#### {{ metrics.description }}
{% if metrics.dataset_name %}
* Dataset{% if metrics.dataset_name is not string and metrics.dataset_name | length > 1 %}s{% endif %}: {% if metrics.dataset_name is string -%}
        `{{ metrics.dataset_name }}`
    {%- else -%}
        {%- for name in metrics.dataset_name -%}
            `{{ name }}`
            {%- if not loop.last -%}
                {%- if loop.index == metrics.dataset_name | length - 1 %} and {% else -%}, {% endif -%}
            {%- endif -%}
        {%- endfor -%}
    {%- endif -%}
{%- endif %}
* Evaluated with {% if metrics.class_name.startswith(""sentence_transformers."") %}[<code>{{ metrics.class_name.split(""."")[-1] }}</code>](https://sbert.net/docs/package_reference/sentence_transformer/evaluation.html#sentence_transformers.evaluation.{{ metrics.class_name.split(""."")[-1] }}){% else %}<code>{{ metrics.class_name }}</code>{% endif %}{% if metrics.config_code %} with these parameters:
{{ metrics.config_code }}{% endif %}",reference
"{{ bias_recommendations | default(""Users (both direct and downstream) should be made aware of the risks, biases and limitations of the model. More information needed for further recommendations."", true)}}",other
"```bash
pip install -U sentence-transformers
```",installation
"{%- if eval_lines %}
### Training Logs
{% if hide_eval_lines %}<details><summary>Click to expand</summary>",usage
"### Metrics
{% for metrics in eval_metrics %}
#### {{ metrics.description }}
{% if metrics.dataset_name %}
* Dataset{% if metrics.dataset_name is not string and metrics.dataset_name | length > 1 %}s{% endif %}: {% if metrics.dataset_name is string -%}
        `{{ metrics.dataset_name }}`
    {%- else -%}
        {%- for name in metrics.dataset_name -%}
            `{{ name }}`
            {%- if not loop.last -%}
                {%- if loop.index == metrics.dataset_name | length - 1 %} and {% else -%}, {% endif -%}
            {%- endif -%}
        {%- endfor -%}
    {%- endif -%}
{%- endif %}
* Evaluated with {% if metrics.class_name.startswith(""sentence_transformers."") %}[<code>{{ metrics.class_name.split(""."")[-1] }}</code>](https://sbert.net/docs/package_reference/cross_encoder/evaluation.html#sentence_transformers.cross_encoder.evaluation.{{ metrics.class_name.split(""."")[-1] }}){% else %}<code>{{ metrics.class_name }}</code>{% endif %}{% if metrics.config_code %} with these parameters:
{{ metrics.config_code }}{% endif %}",reference
## ⚙️ Installation,installation
"- Tom Aarsen
- Rami Al-Rfou'
- Mark Amery
- Greg Aumann
- Ivan Barria
- Ingolf Becker
- Yonatan Becker
- Paul Bedaride
- Steven Bethard
- Robert Berwick
- Dan Blanchard
- Nathan Bodenstab
- Alexander Böhm
- Francis Bond
- Paul Bone
- Jordan Boyd-Graber
- Daniel Blanchard
- Phil Blunsom
- Lars Buitinck
- Cristian Capdevila
- Steve Cassidy
- Chen-Fu Chiang
- Dmitry Chichkov
- Jinyoung Choi
- Andrew Clausen
- Lucas Champollion
- Graham Christensen
- Trevor Cohn
- David Coles
- Tom Conroy <https://github.com/tconroy>
- Claude Coulombe
- Lucas Cooper
- Robin Cooper
- Chris Crowner
- James Curran
- Arthur Darcet
- Dariel Dato-on
- Selina Dennis
- Leon Derczynski
- Alexis Dimitriadis
- Nikhil Dinesh
- Liang Dong
- David Doukhan
- Rebecca Dridan
- Pablo Duboue
- Long Duong
- Christian Federmann
- Campion Fellin
- Michelle Fullwood
- Dan Garrette
- Maciej Gawinecki
- Jean Mark Gawron
- Sumukh Ghodke
- Yoav Goldberg
- Michael Wayne Goodman
- Dougal Graham
- Brent Gray
- Simon Greenhill
- Clark Grubb
- Eduardo Pereira Habkost
- Masato Hagiwara
- Lauri Hallila
- Michael Hansen
- Yurie Hara
- Will Hardy
- Tyler Hartley
- Peter Hawkins
- Saimadhav Heblikar
- Fredrik Hedman
- Helder
- Michael Heilman
- Ofer Helman
- Christopher Hench
- Bruce Hill
- Amy Holland
- Kristy Hollingshead
- Marcus Huderle
- Baden Hughes
- Nancy Ide
- Rebecca Ingram
- Edward Ivanovic
- Thomas Jakobsen
- Nick Johnson
- Eric Kafe
- Piotr Kasprzyk
- Angelos Katharopoulos
- Sudharshan Kaushik
- Chris Koenig
- Mikhail Korobov
- Denis Krusko
- Ilia Kurenkov
- Stefano Lattarini
- Pierre-François Laquerre
- Stefano Lattarini
- Haejoong Lee
- Jackson Lee
- Max Leonov
- Chris Liechti
- Hyuckin David Lim
- Tom Lippincott
- Peter Ljunglöf
- Alex Louden
- David Lukeš
- Joseph Lynch
- Nitin Madnani
- Felipe Madrigal
- Bjørn Mæland
- Dean Malmgren
- Christopher Maloof
- Rob Malouf
- Iker Manterola
- Carl de Marcken
- Mitch Marcus
- Torsten Marek
- Robert Marshall
- Marius Mather
- Duncan McGreggor
- David McClosky
- Xinfan Meng
- Dmitrijs Milajevs
- Matt Miller
- Margaret Mitchell
- Tomonori Nagano
- Jason Narad
- Shari A’aidil Nasruddin
- Lance Nathan
- Morten Neergaard
- David Nemeskey
- Eric Nichols
- Joel Nothman
- Alireza Nourian
- Alexander Oleynikov
- Pierpaolo Pantone
- Ted Pedersen
- Jacob Perkins
- Alberto Planas
- Ondrej Platek
- Alessandro Presta
- Qi Liu
- Martin Thorsen Ranang
- Michael Recachinas
- Brandon Rhodes
- Joshua Ritterman
- Will Roberts
- Stuart Robinson
- Carlos Rodriguez
- Lorenzo Rubio
- Alex Rudnick
- Jussi Salmela
- Geoffrey Sampson
- Kepa Sarasola
- Kevin Scannell
- Nathan Schneider
- Rico Sennrich
- Thomas Skardal
- Eric Smith
- Lynn Soe
- Rob Speer
- Peter Spiller
- Richard Sproat
- Ceri Stagg
- Peter Stahl
- Oliver Steele
- Thomas Stieglmaier
- Jan Strunk
- Liling Tan
- Claire Taylor
- Louis Tiao
- Steven Tomcavage
- Tiago Tresoldi
- Marcus Uneson
- Yu Usami
- Petro Verkhogliad
- Peter Wang
- Zhe Wang
- Charlotte Wilson
- Chuck Wooters
- Steven Xu
- Beracah Yankama
- Lei Ye (叶磊)
- Patrick Ye
- Geraldine Sim Wei Ying
- Jason Yoder
- Thomas Zieglier
- 0ssifrage
- ducki13
- kiwipi
- lade
- isnowfy
- onesandzeros
- pquentin
- wvanlint
- Álvaro Justen <https://github.com/turicas>
- bjut-hz
- Sergio Oller
- Izam Mohammed <https://github.com/izam-mohammed>
- Will Monroe
- Elijah Rippeth
- Emil Manukyan
- Casper Lehmann-Strøm
- Andrew Giel
- Tanin Na Nakorn
- Linghao Zhang
- Colin Carroll
- Heguang Miao
- Hannah Aizenman (story645)
- George Berry
- Adam Nelson
- J Richard Snape
- Alex Constantin <alex@keyworder.ch>
- Tsolak Ghukasyan
- Prasasto Adi
- Safwan Kamarrudin
- Arthur Tilley
- Vilhjalmur Thorsteinsson
- Jaehoon Hwang <https://github.com/jaehoonhwang>
- Chintan Shah <https://github.com/chintanshah24>
- sbagan
- Zicheng Xu
- Albert Au Yeung <https://github.com/albertauyeung>
- Shenjian Zhao
- Deng Wang <https://github.com/lmatt-bit>
- Ali Abdullah
- Stoytcho Stoytchev
- Lakhdar Benzahia
- Kheireddine Abainia <https://github.com/xprogramer>
- Yibin Lin <https://github.com/yibinlin>
- Artiem Krinitsyn
- Björn Mattsson
- Oleg Chislov
- Pavan Gururaj Joshi <https://github.com/PavanGJ>
- Ethan Hill <https://github.com/hill1303>
- Vivek Lakshmanan
- Somnath Rakshit <https://github.com/somnathrakshit>
- Anlan Du
- Pulkit Maloo <https://github.com/pulkitmaloo>
- Brandon M. Burroughs <https://github.com/brandonmburroughs>
- John Stewart <https://github.com/free-variation>
- Iaroslav Tymchenko <https://github.com/myproblemchild>
- Aleš Tamchyna
- Tim Gianitsos <https://github.com/timgianitsos>
- Philippe Partarrieu <https://github.com/ppartarr>
- Andrew Owen Martin
- Adrian Ellis <https://github.com/adrianjellis>
- Nat Quayle Nelson <https://github.com/nqnstudios>
- Yanpeng Zhao <https://github.com/zhaoyanpeng>
- Matan Rak <https://github.com/matanrak>
- Nick Ulle <https://github.com/nick-ulle>
- Uday Krishna <https://github.com/udaykrishna>
- Osman Zubair <https://github.com/okz12>
- Viresh Gupta <https://github.com/virresh>
- Ondřej Cífka <https://github.com/cifkao>
- Iris X. Zhou <https://github.com/irisxzhou>
- Devashish Lal <https://github.com/BLaZeKiLL>
- Gerhard Kremer <https://github.com/GerhardKa>
- Nicolas Darr <https://github.com/ndarr>
- Hervé Nicol <https://github.com/hervenicol>
- Alexandre H. T. Dias <https://github.com/alexandredias3d>
- Daksh Shah <https://github.com/Daksh>
- Jacob Weightman <https://github.com/jacobdweightman>
- Bonifacio de Oliveira <https://github.com/Bonifacio2>
- Armins Bagrats Stepanjans <https://github.com/ab-10>
- Vassilis Palassopoulos <https://github.com/palasso>
- Ram Rachum <https://github.com/cool-RR>
- Or Sharir <https://github.com/orsharir>
- Denali Molitor <https://github.com/dmmolitor>
- Jacob Moorman <https://github.com/jdmoorman>
- Cory Nezin <https://github.com/corynezin>
- Matt Chaput
- Danny Sepler <https://github.com/dannysepler>
- Akshita Bhagia <https://github.com/AkshitaB>
- Pratap Yadav <https://github.com/prtpydv>
- Hiroki Teranishi <https://github.com/chantera>
- Ruben Cartuyvels <https://github.com/rubencart>
- Dalton Pearson <https://github.com/daltonpearson>
- Robby Horvath <https://github.com/robbyhorvath>
- Gavish Poddar <https://github.com/gavishpoddar>
- Saibo Geng <https://github.com/Saibo-creator>
- Ahmet Yildirim <https://github.com/RnDevelover>
- Yuta Nakamura <https://github.com/yutanakamura-tky>
- Adam Hawley <https://github.com/adamjhawley>
- Panagiotis Simakis <https://github.com/sp1thas>
- Richard Wang <https://github.com/richarddwang>
- Alexandre Perez-Lebel <https://github.com/aperezlebel>
- Fernando Carranza <https://github.com/fernandocar86>
- Martin Kondratzky <https://github.com/martinkondra>
- Heungson Lee <https://github.com/heungson>
- M.K. Pawelkiewicz <https://github.com/hamiltonianflow>
- Steven Thomas Smith <https://github.com/essandess>
- Jan Lennartz <https://github.com/Madnex>
- Tim Sockel <https://github.com/TiMauzi>
- Akihiro Yamazaki <https://github.com/zakkie>
- Ron Urbach <https://github.com/sharpblade4>
- Vivek Kalyan <https://github.com/vivekkalyan>
- Tom Strange https://github.com/strangetom",other
"# Query
reposage query ""What is DeepSeek?"" --threshold 0.3",usage
"```bash
pip install -U sentence-transformers
```",installation
"<!-- If there is a paper or blog post introducing the model, the APA and Bibtex information for that should go in this section. -->",reference
"### Installation
```bash
git clone https://github.com/RohitRajpoot/reposage.git
cd reposage
pip install -r requirements.txt",installation
## ⚙️ Installation,installation
"## Features
- **DeepSeek Index** of lecture slides, code, and readings  
- **Bayesian Posterior Scoring** for precise answer selection  
- **Transformer Fallback** via HF text2text when confidence is low  
- **CLI & REST API** for local and web deployment  
- **CI/CD** via GitHub Actions → Hugging Face Spaces",usage
"{% for name, value in non_default_hyperparameters.items() %}- `{{ name }}`: {{ value }}
{% endfor %}{%- endif %}
#### All Hyperparameters
<details><summary>Click to expand</summary>",usage
First install the Sentence Transformers library:,installation
## Dataset Card Authors [optional],other
"### Installation
```bash
git clone https://github.com/RohitRajpoot/reposage.git
cd reposage
pip install -r requirements.txt",installation
## Dataset Card Contact,other
"```bash
pip install -U sentence-transformers
```",installation
"* After labeling paragraphs (e.g. `installation`, `usage`, `configuration`, `api_reference`, `troubleshooting`), train an H2O AutoML model to predict the category of any incoming question.
   * This routing step ensures each query is answered by the most relevant section of your docs.",installation
"{{ bias_risks_limitations | default(""[More Information Needed]"", true)}}",other
"*List how the model may foreseeably be misused and address what users ought not to do with the model.*
-->
{% if eval_metrics %}
## Evaluation",other
"# Natural Language Toolkit (NLTK)
[![PyPI](https://img.shields.io/pypi/v/nltk.svg)](https://pypi.python.org/pypi/nltk)
![CI](https://github.com/nltk/nltk/actions/workflows/ci.yaml/badge.svg?branch=develop)",other
"# Slide 3: Bayesian Q&A Layer
- Cosine-likelihood softmax → posterior",reference
"| Team Member | Role(s)                 | Responsibilities                             |
|-------------|-------------------------|----------------------------------------------|
| Rohit       | CEO / Product Lead,     | • Define product vision & roadmap<br>• Manage scope & stakeholder communication<br>• Write technical documentation |
| Kishan      | CTO / DevOps Lead,      | • Develop DeepSeek index & CLI scaffold<br>• Set up CI/CD pipelines<br>• Manage Hugging Face Spaces deployments   |
| Maitri      | Data Scientist / AI Lead| • Implement Bayesian Q&A layer<br>• Design & tune priors<br>• Oversee transformer fallback prompts          |",usage
First install the Sentence Transformers library:,installation
"1. Fork and create a feature branch.
2. Add or update scripts/tests as needed.
3. Ensure style checks (`flake8`, `black`) pass.
4. Submit a pull request detailing your enhancements.",reference
"<!-- If there is a paper or blog post introducing the dataset, the APA and Bibtex information for that should go in this section. -->",reference
"Redistribution and use in source and binary forms, with or without
modification, are permitted provided that the following conditions are
met:",other
First install the Sentence Transformers library:,installation
"# Query
reposage query ""What is DeepSeek?"" --threshold 0.3",usage
"# Slide 3: Bayesian Q&A Layer
- Cosine-likelihood softmax → posterior",reference
<details><summary>Click to expand</summary>,usage
"```bash
pip install -U sentence-transformers
```",installation
<details><summary>Click to see the direct usage in Transformers</summary>,usage
Update this directory using maint_tools/vendor_array_api_compat.sh,reference
"THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
SOFTWARE.",other
"{%- if eval_lines %}
### Training Logs
{% if hide_eval_lines %}<details><summary>Click to expand</summary>",usage
"## Day 1
- Decision: Workspace ready, boards & sites created.
- Next: Build embedding notebook tomorrow.",other
"# Query
reposage query ""What is DeepSeek?"" --threshold 0.3",usage
"```bash
pip install -U sentence-transformers
```",installation
"# Build the index
reposage index",usage
"* After labeling paragraphs (e.g. `installation`, `usage`, `configuration`, `api_reference`, `troubleshooting`), train an H2O AutoML model to predict the category of any incoming question.
   * This routing step ensures each query is answered by the most relevant section of your docs.",installation
2. **User prompt template**:,other
"# 3) Smoke-test CLI
python -m assist.main chat ""hello world""",usage
First install the Sentence Transformers library:,installation
"In `app.py`, configure paths to:",reference
"<!-- If there is a paper or blog post introducing the dataset, the APA and Bibtex information for that should go in this section. -->",reference
"{{ dataset_description | default("""", true) }}",other
"## Features
- **DeepSeek Index** of lecture slides, code, and readings  
- **Bayesian Posterior Scoring** for precise answer selection  
- **Transformer Fallback** via HF text2text when confidence is low  
- **CLI & REST API** for local and web deployment  
- **CI/CD** via GitHub Actions → Hugging Face Spaces",usage
"### BibTeX
{% for loss_name, citation in citations.items() %}
#### {{ loss_name }}
```bibtex
{{ citation | trim }}
```
{% endfor %}
<!--
## Glossary",other
"# 2) Install dependencies
pip install --upgrade pip setuptools wheel
pip install -r requirements.txt",installation
"| Team Member | Role(s)                 | Responsibilities                             |
|-------------|-------------------------|----------------------------------------------|
| Rohit       | CEO / Product Lead,     | • Define product vision & roadmap<br>• Manage scope & stakeholder communication<br>• Write technical documentation |
| Kishan      | CTO / DevOps Lead,      | • Develop DeepSeek index & CLI scaffold<br>• Set up CI/CD pipelines<br>• Manage Hugging Face Spaces deployments   |
| Maitri      | Data Scientist / AI Lead| • Implement Bayesian Q&A layer<br>• Design & tune priors<br>• Oversee transformer fallback prompts          |",usage
<details><summary>Click to see the direct usage in Transformers</summary>,usage
"### BibTeX
{% for loss_name, citation in citations.items() %}
#### {{ loss_name }}
```bibtex
{{ citation | trim }}
```
{% endfor %}
<!--
## Glossary",other
"```bash
pip install -U sentence-transformers
```",installation
"{{ direct_use | default(""[More Information Needed]"", true)}}",other
Update this directory using maint_tools/vendor_array_api_extra.sh,reference
"### Metrics
{% for metrics in eval_metrics %}
#### {{ metrics.description }}
{% if metrics.dataset_name %}
* Dataset{% if metrics.dataset_name is not string and metrics.dataset_name | length > 1 %}s{% endif %}: {% if metrics.dataset_name is string -%}
        `{{ metrics.dataset_name }}`
    {%- else -%}
        {%- for name in metrics.dataset_name -%}
            `{{ name }}`
            {%- if not loop.last -%}
                {%- if loop.index == metrics.dataset_name | length - 1 %} and {% else -%}, {% endif -%}
            {%- endif -%}
        {%- endfor -%}
    {%- endif -%}
{%- endif %}
* Evaluated with {% if metrics.class_name.startswith(""sentence_transformers."") %}[<code>{{ metrics.class_name.split(""."")[-1] }}</code>](https://sbert.net/docs/package_reference/cross_encoder/evaluation.html#sentence_transformers.cross_encoder.evaluation.{{ metrics.class_name.split(""."")[-1] }}){% else %}<code>{{ metrics.class_name }}</code>{% endif %}{% if metrics.config_code %} with these parameters:
{{ metrics.config_code }}{% endif %}",reference
"{%- if eval_lines %}
### Training Logs
{% if hide_eval_lines %}<details><summary>Click to expand</summary>",usage
"<!-- This section addresses misuse, malicious use, and uses that the model will not work well for. -->",other
### Direct Usage (Sentence Transformers),other
Update this directory using maint_tools/vendor_array_api_extra.sh,reference
BSD 3-Clause License,other
"{{ model_specs | default(""[More Information Needed]"", true)}}",other
"<!-- If there is a paper or blog post introducing the model, the APA and Bibtex information for that should go in this section. -->",reference
"Redistributions of source code must retain the above copyright notice, this
list of conditions and the following disclaimers.",other
"* After labeling paragraphs (e.g. `installation`, `usage`, `configuration`, `api_reference`, `troubleshooting`), train an H2O AutoML model to predict the category of any incoming question.
   * This routing step ensures each query is answered by the most relevant section of your docs.",installation
"1. **Route** via the H2O classifier to select a subset of paragraphs.
     2. **Retrieve** top‑k relevant passages with FAISS.
     3. **Rescore** with a Bayesian posterior to boost high-confidence results.
     4. **Fallback**: if the posterior confidence is below threshold, generate an answer using a compact Flan‑T5 model.",reference
<details><summary>Click to see the direct usage in Transformers</summary>,usage
Update this directory using maint_tools/vendor_array_api_compat.sh,reference
"<!-- This section describes the source data (e.g. news text and headlines, social media posts, translated sentences, ...). -->",reference
"| Team Member | Role(s)                 | Responsibilities                             |
|-------------|-------------------------|----------------------------------------------|
| Rohit       | CEO / Product Lead,     | • Define product vision & roadmap<br>• Manage scope & stakeholder communication<br>• Write technical documentation |
| Kishan      | CTO / DevOps Lead,      | • Develop DeepSeek index & CLI scaffold<br>• Set up CI/CD pipelines<br>• Manage Hugging Face Spaces deployments   |
| Maitri      | Data Scientist / AI Lead| • Implement Bayesian Q&A layer<br>• Design & tune priors<br>• Oversee transformer fallback prompts          |",usage
<details><summary>Click to expand</summary>,usage
"* After labeling paragraphs (e.g. `installation`, `usage`, `configuration`, `api_reference`, `troubleshooting`), train an H2O AutoML model to predict the category of any incoming question.
   * This routing step ensures each query is answered by the most relevant section of your docs.",installation
"If you add a file to this directory, you **MUST** update
`torch/CMakeLists.txt` and add the file as a dependency to
the `add_custom_command` call.",other
"{%- if eval_lines %}
### Training Logs
{% if hide_eval_lines %}<details><summary>Click to expand</summary>",usage
"- NLTK source code is distributed under the Apache 2.0 License.
- NLTK documentation is distributed under the Creative Commons
  Attribution-Noncommercial-No Derivative Works 3.0 United States license.
- NLTK corpora are provided under the terms given in the README file for each
  corpus; all are redistributable and available for non-commercial use.
- NLTK may be freely redistributed, subject to the provisions of these licenses.",other
---,other
<details><summary>Click to see the direct usage in Transformers</summary>,usage
#### Data Collection and Processing,other
"<!-- If there is a paper or blog post introducing the dataset, the APA and Bibtex information for that should go in this section. -->",reference
"```bash
pip install -U sentence-transformers
```",installation
First install the Sentence Transformers library:,installation
## Dataset Details,other
"## Training Details
{% for dataset_type, dataset_list in [(""training"", train_datasets), (""evaluation"", eval_datasets)] %}{% if dataset_list %}
### {{ dataset_type.title() }} Dataset{{""s"" if dataset_list | length > 1 else """"}}
{% for dataset in dataset_list %}{% if dataset_list | length > 3 %}<details><summary>{{ dataset['name'] or 'Unnamed Dataset' }}</summary>
{% endif %}
#### {{ dataset['name'] or 'Unnamed Dataset' }}
{% if dataset['name'] %}
* Dataset: {% if 'id' in dataset %}[{{ dataset['name'] }}](https://huggingface.co/datasets/{{ dataset['id'] }}){% else %}{{ dataset['name'] }}{% endif %}
{%- if 'revision' in dataset and 'id' in dataset %} at [{{ dataset['revision'][:7] }}](https://huggingface.co/datasets/{{ dataset['id'] }}/tree/{{ dataset['revision'] }}){% endif %}{% endif %}
{% if dataset['size'] %}* Size: {{ ""{:,}"".format(dataset['size']) }} {{ dataset_type }} samples
{% endif %}* Columns: {% if dataset['columns'] | length == 1 %}{{ dataset['columns'][0] }}{% elif dataset['columns'] | length == 2 %}{{ dataset['columns'][0] }} and {{ dataset['columns'][1] }}{% else %}{{ dataset['columns'][:-1] | join(', ') }}, and {{ dataset['columns'][-1] }}{% endif %}
{% if dataset['stats_table'] %}* Approximate statistics based on the first {{ [dataset['size'], 1000] | min }} samples:
{{ dataset['stats_table'] }}{% endif %}{% if dataset['examples_table'] %}* Samples:
{{ dataset['examples_table'] }}{% endif %}* Loss: {% if dataset[""loss""][""fullname""].startswith(""sentence_transformers."") %}[<code>{{ dataset[""loss""][""fullname""].split(""."")[-1] }}</code>](https://sbert.net/docs/package_reference/sentence_transformer/losses.html#{{ dataset[""loss""][""fullname""].split(""."")[-1].lower() }}){% else %}<code>{{ dataset[""loss""][""fullname""] }}</code>{% endif %}{% if ""config_code"" in dataset[""loss""] %} with these parameters:
{{ dataset[""loss""][""config_code""] }}{% endif %}
{% if dataset_list | length > 3 %}</details>
{% endif %}{% endfor %}{% endif %}{% endfor -%}",reference
"# 3) Smoke-test CLI
python -m assist.main chat ""hello world""",usage
"- **Curated by:** {{ curators | default(""[More Information Needed]"", true)}}
- **Funded by [optional]:** {{ funded_by | default(""[More Information Needed]"", true)}}
- **Shared by [optional]:** {{ shared_by | default(""[More Information Needed]"", true)}}
- **Language(s) (NLP):** {{ language | default(""[More Information Needed]"", true)}}
- **License:** {{ license | default(""[More Information Needed]"", true)}}",other
<details><summary>Click to see the direct usage in Transformers</summary>,usage
"* After labeling paragraphs (e.g. `installation`, `usage`, `configuration`, `api_reference`, `troubleshooting`), train an H2O AutoML model to predict the category of any incoming question.
   * This routing step ensures each query is answered by the most relevant section of your docs.",installation
"1. **Route** via the H2O classifier to select a subset of paragraphs.
     2. **Retrieve** top‑k relevant passages with FAISS.
     3. **Rescore** with a Bayesian posterior to boost high-confidence results.
     4. **Fallback**: if the posterior confidence is below threshold, generate an answer using a compact Flan‑T5 model.",reference
"{% for name, value in non_default_hyperparameters.items() %}- `{{ name }}`: {{ value }}
{% endfor %}{%- endif %}
#### All Hyperparameters
<details><summary>Click to expand</summary>",usage
"<!--
### Direct Usage (Transformers)",other
"```bash
pip install -U sentence-transformers
```",installation
"{%- if eval_lines %}
### Training Logs
{% if hide_eval_lines %}<details><summary>Click to expand</summary>",usage
"### Metrics
{% for metrics in eval_metrics %}
#### {{ metrics.description }}
{% if metrics.dataset_name %}
* Dataset{% if metrics.dataset_name is not string and metrics.dataset_name | length > 1 %}s{% endif %}: {% if metrics.dataset_name is string -%}
        `{{ metrics.dataset_name }}`
    {%- else -%}
        {%- for name in metrics.dataset_name -%}
            `{{ name }}`
            {%- if not loop.last -%}
                {%- if loop.index == metrics.dataset_name | length - 1 %} and {% else -%}, {% endif -%}
            {%- endif -%}
        {%- endfor -%}
    {%- endif -%}
{%- endif %}
* Evaluated with {% if metrics.class_name.startswith(""sentence_transformers."") %}[<code>{{ metrics.class_name.split(""."")[-1] }}</code>](https://sbert.net/docs/package_reference/sentence_transformer/evaluation.html#sentence_transformers.evaluation.{{ metrics.class_name.split(""."")[-1] }}){% else %}<code>{{ metrics.class_name }}</code>{% endif %}{% if metrics.config_code %} with these parameters:
{{ metrics.config_code }}{% endif %}",reference
"| Team Member | Role(s)                 | Responsibilities                             |
|-------------|-------------------------|----------------------------------------------|
| Rohit       | CEO / Product Lead,     | • Define product vision & roadmap<br>• Manage scope & stakeholder communication<br>• Write technical documentation |
| Kishan      | CTO / DevOps Lead,      | • Develop DeepSeek index & CLI scaffold<br>• Set up CI/CD pipelines<br>• Manage Hugging Face Spaces deployments   |
| Maitri      | Data Scientist / AI Lead| • Implement Bayesian Q&A layer<br>• Design & tune priors<br>• Oversee transformer fallback prompts          |",usage
Update this directory using maint_tools/vendor_array_api_compat.sh,reference
## Model Details,other
Update this directory using maint_tools/vendor_array_api_extra.sh,reference
"<!-- This section is meant to convey recommendations with respect to the bias, risk, and technical limitations. -->",other
[AUTHORS.md](AUTHORS.md) contains a list of everyone who has contributed to NLTK.,other
"Copyright (c) 2013-2024, Kim Davies and contributors.
All rights reserved.",other
First install the Sentence Transformers library:,installation
"<!-- If relevant, include terms and calculations in this section that can help readers understand the dataset or dataset card. -->",other
"* After labeling paragraphs (e.g. `installation`, `usage`, `configuration`, `api_reference`, `troubleshooting`), train an H2O AutoML model to predict the category of any incoming question.
   * This routing step ensures each query is answered by the most relevant section of your docs.",installation
## Dataset Structure,other
---,other
"{%- if eval_lines %}
### Training Logs
{% if hide_eval_lines %}<details><summary>Click to expand</summary>",usage
"<!--
### Direct Usage (Transformers)",other
---,other
"{%- if eval_lines %}
### Training Logs
{% if hide_eval_lines %}<details><summary>Click to expand</summary>",usage
Update this directory using maint_tools/vendor_array_api_compat.sh,reference
"* After labeling paragraphs (e.g. `installation`, `usage`, `configuration`, `api_reference`, `troubleshooting`), train an H2O AutoML model to predict the category of any incoming question.
   * This routing step ensures each query is answered by the most relevant section of your docs.",installation
## ⚙️ Installation,installation
"{%- if eval_lines %}
### Training Logs
{% if hide_eval_lines %}<details><summary>Click to expand</summary>",usage
"# Query
reposage query ""What is DeepSeek?"" --threshold 0.3",usage
"1. Fork and create a feature branch.
2. Add or update scripts/tests as needed.
3. Ensure style checks (`flake8`, `black`) pass.
4. Submit a pull request detailing your enhancements.",reference
"```bash
streamlit run app.py
```",other
"<!-- This section describes the source data (e.g. news text and headlines, social media posts, translated sentences, ...). -->",reference
"1. **Route** via the H2O classifier to select a subset of paragraphs.
     2. **Retrieve** top‑k relevant passages with FAISS.
     3. **Rescore** with a Bayesian posterior to boost high-confidence results.
     4. **Fallback**: if the posterior confidence is below threshold, generate an answer using a compact Flan‑T5 model.",reference
## Usage,other
Update this directory using maint_tools/vendor_array_api_compat.sh,reference
## ⚙️ Installation,installation
<details><summary>Click to expand</summary>,usage
<details><summary>Click to expand</summary>,usage
"<!-- This section describes the source data (e.g. news text and headlines, social media posts, translated sentences, ...). -->",reference
"# 2) Install dependencies
pip install --upgrade pip setuptools wheel
pip install -r requirements.txt",installation
"{% for name, value in non_default_hyperparameters.items() %}- `{{ name }}`: {{ value }}
{% endfor %}{%- endif %}
#### All Hyperparameters
<details><summary>Click to expand</summary>",usage
"## Training Details
{% for dataset_type, dataset_list in [(""training"", train_datasets), (""evaluation"", eval_datasets)] %}{% if dataset_list %}
### {{ dataset_type.title() }} Dataset{{""s"" if dataset_list | length > 1 else """"}}
{% for dataset in dataset_list %}{% if dataset_list | length > 3 %}<details><summary>{{ dataset['name'] or 'Unnamed Dataset' }}</summary>
{% endif %}
#### {{ dataset['name'] or 'Unnamed Dataset' }}
{% if dataset['name'] %}
* Dataset: {% if 'id' in dataset %}[{{ dataset['name'] }}](https://huggingface.co/datasets/{{ dataset['id'] }}){% else %}{{ dataset['name'] }}{% endif %}
{%- if 'revision' in dataset and 'id' in dataset %} at [{{ dataset['revision'][:7] }}](https://huggingface.co/datasets/{{ dataset['id'] }}/tree/{{ dataset['revision'] }}){% endif %}{% endif %}
{% if dataset['size'] %}* Size: {{ ""{:,}"".format(dataset['size']) }} {{ dataset_type }} samples
{% endif %}* Columns: {% if dataset['columns'] | length == 1 %}{{ dataset['columns'][0] }}{% elif dataset['columns'] | length == 2 %}{{ dataset['columns'][0] }} and {{ dataset['columns'][1] }}{% else %}{{ dataset['columns'][:-1] | join(', ') }}, and {{ dataset['columns'][-1] }}{% endif %}
{% if dataset['stats_table'] %}* Approximate statistics based on the first {{ [dataset['size'], 1000] | min }} samples:
{{ dataset['stats_table'] }}{% endif %}{% if dataset['examples_table'] %}* Samples:
{{ dataset['examples_table'] }}{% endif %}* Loss: {% if dataset[""loss""][""fullname""].startswith(""sentence_transformers."") %}[<code>{{ dataset[""loss""][""fullname""].split(""."")[-1] }}</code>](https://sbert.net/docs/package_reference/cross_encoder/losses.html#{{ dataset[""loss""][""fullname""].split(""."")[-1].lower() }}){% else %}<code>{{ dataset[""loss""][""fullname""] }}</code>{% endif %}{% if ""config_code"" in dataset[""loss""] %} with these parameters:
{{ dataset[""loss""][""config_code""] }}{% endif %}
{% if dataset_list | length > 3 %}</details>
{% endif %}{% endfor %}{% endif %}{% endfor -%}",reference
"In `app.py`, configure paths to:",reference
"* After labeling paragraphs (e.g. `installation`, `usage`, `configuration`, `api_reference`, `troubleshooting`), train an H2O AutoML model to predict the category of any incoming question.
   * This routing step ensures each query is answered by the most relevant section of your docs.",installation
"| Team Member | Role(s)                 | Responsibilities                             |
|-------------|-------------------------|----------------------------------------------|
| Rohit       | CEO / Product Lead,     | • Define product vision & roadmap<br>• Manage scope & stakeholder communication<br>• Write technical documentation |
| Kishan      | CTO / DevOps Lead,      | • Develop DeepSeek index & CLI scaffold<br>• Set up CI/CD pipelines<br>• Manage Hugging Face Spaces deployments   |
| Maitri      | Data Scientist / AI Lead| • Implement Bayesian Q&A layer<br>• Design & tune priors<br>• Oversee transformer fallback prompts          |",usage
"<!-- This should link to a Dataset Card, perhaps with a short stub of information on what the training data is all about as well as documentation related to data pre-processing or additional filtering. -->",other
"<!--
### Downstream Usage (Sentence Transformers)",other
"### Metrics
{% for metrics in eval_metrics %}
#### {{ metrics.description }}
{% if metrics.dataset_name %}
* Dataset{% if metrics.dataset_name is not string and metrics.dataset_name | length > 1 %}s{% endif %}: {% if metrics.dataset_name is string -%}
        `{{ metrics.dataset_name }}`
    {%- else -%}
        {%- for name in metrics.dataset_name -%}
            `{{ name }}`
            {%- if not loop.last -%}
                {%- if loop.index == metrics.dataset_name | length - 1 %} and {% else -%}, {% endif -%}
            {%- endif -%}
        {%- endfor -%}
    {%- endif -%}
{%- endif %}
* Evaluated with {% if metrics.class_name.startswith(""sentence_transformers."") %}[<code>{{ metrics.class_name.split(""."")[-1] }}</code>](https://sbert.net/docs/package_reference/cross_encoder/evaluation.html#sentence_transformers.cross_encoder.evaluation.{{ metrics.class_name.split(""."")[-1] }}){% else %}<code>{{ metrics.class_name }}</code>{% endif %}{% if metrics.config_code %} with these parameters:
{{ metrics.config_code }}{% endif %}",reference
<!-- Provide a quick summary of what the model is/does. -->,other
"# Build the index
reposage index",usage
"### Usage
# Build the index
reposage index",usage
"{% for name, value in non_default_hyperparameters.items() %}- `{{ name }}`: {{ value }}
{% endfor %}{%- endif %}
#### All Hyperparameters
<details><summary>Click to expand</summary>",usage
"- **Repository:** {{ repo | default(""[More Information Needed]"", true)}}
- **Paper [optional]:** {{ paper | default(""[More Information Needed]"", true)}}
- **Demo [optional]:** {{ demo | default(""[More Information Needed]"", true)}}",other
"```bash
python scripts/train_classifier.py \
  --data data/classification.csv \
  --model-out models/routing_model.zip
```",other
"```bash
pip install -U sentence-transformers
```",installation
"<!-- If there is a paper or blog post introducing the model, the APA and Bibtex information for that should go in this section. -->",reference
"* Redistributions of source code must retain the above copyright notice, this
  list of conditions and the following disclaimer.
* Redistributions in binary form must reproduce the above copyright notice, this
  list of conditions and the following disclaimer in the documentation and/or
  other materials provided with the distribution.
* Neither the name of the copyright holder nor the names of its contributors may
  be used to endorse or promote products derived from this software without
  specific prior written permission.",other
"### Installation
```bash
git clone https://github.com/RohitRajpoot/reposage.git
cd reposage
pip install -r requirements.txt",installation
"### Metrics
{% for metrics in eval_metrics %}
#### {{ metrics.description }}
{% if metrics.dataset_name %}
* Dataset{% if metrics.dataset_name is not string and metrics.dataset_name | length > 1 %}s{% endif %}: {% if metrics.dataset_name is string -%}
        `{{ metrics.dataset_name }}`
    {%- else -%}
        {%- for name in metrics.dataset_name -%}
            `{{ name }}`
            {%- if not loop.last -%}
                {%- if loop.index == metrics.dataset_name | length - 1 %} and {% else -%}, {% endif -%}
            {%- endif -%}
        {%- endfor -%}
    {%- endif -%}
{%- endif %}
* Evaluated with {% if metrics.class_name.startswith(""sentence_transformers."") %}[<code>{{ metrics.class_name.split(""."")[-1] }}</code>](https://sbert.net/docs/package_reference/cross_encoder/evaluation.html#sentence_transformers.cross_encoder.evaluation.{{ metrics.class_name.split(""."")[-1] }}){% else %}<code>{{ metrics.class_name }}</code>{% endif %}{% if metrics.config_code %} with these parameters:
{{ metrics.config_code }}{% endif %}",reference
"# Query
reposage query ""What is DeepSeek?"" --threshold 0.3",usage
"This is a [sentence-transformers](https://www.SBERT.net) model{% if base_model %} finetuned from [{{ base_model }}](https://huggingface.co/{{ base_model }}){% else %} trained{% endif %}{% if train_datasets | selectattr(""name"") | list %} on {% if train_datasets | selectattr(""name"") | map(attribute=""name"") | join("", "") | length > 200 %}{{ train_datasets | length }}{% else %}the {% for dataset in (train_datasets | selectattr(""name"")) %}{% if dataset.id %}[{{ dataset.name if dataset.name else dataset.id }}](https://huggingface.co/datasets/{{ dataset.id }}){% else %}{{ dataset.name }}{% endif %}{% if not loop.last %}{% if loop.index == (train_datasets | selectattr(""name"") | list | length - 1) %} and {% else %}, {% endif %}{% endif %}{% endfor %}{% endif %} dataset{{""s"" if train_datasets | selectattr(""name"") | list | length > 1 else """"}}{% endif %}. It maps sentences & paragraphs to a {{ output_dimensionality }}-dimensional dense vector space and can be used for {{ task_name }}.",other
First install the Sentence Transformers library:,installation
"#### Speeds, Sizes, Times [optional]",other
"### Installation
```bash
git clone https://github.com/RohitRajpoot/reposage.git
cd reposage
pip install -r requirements.txt",installation
"### Metrics
{% for metrics in eval_metrics %}
#### {{ metrics.description }}
{% if metrics.dataset_name %}
* Dataset{% if metrics.dataset_name is not string and metrics.dataset_name | length > 1 %}s{% endif %}: {% if metrics.dataset_name is string -%}
        `{{ metrics.dataset_name }}`
    {%- else -%}
        {%- for name in metrics.dataset_name -%}
            `{{ name }}`
            {%- if not loop.last -%}
                {%- if loop.index == metrics.dataset_name | length - 1 %} and {% else -%}, {% endif -%}
            {%- endif -%}
        {%- endfor -%}
    {%- endif -%}
{%- endif %}
* Evaluated with {% if metrics.class_name.startswith(""sentence_transformers."") %}[<code>{{ metrics.class_name.split(""."")[-1] }}</code>](https://sbert.net/docs/package_reference/cross_encoder/evaluation.html#sentence_transformers.cross_encoder.evaluation.{{ metrics.class_name.split(""."")[-1] }}){% else %}<code>{{ metrics.class_name }}</code>{% endif %}{% if metrics.config_code %} with these parameters:
{{ metrics.config_code }}{% endif %}",reference
"<!-- This section describes the source data (e.g. news text and headlines, social media posts, translated sentences, ...). -->",reference
First install the Sentence Transformers library:,installation
#### Testing Data,other
"```bash
pip install -U sentence-transformers
```",installation
<details><summary>Click to see the direct usage in Transformers</summary>,usage
"{%- if eval_lines %}
### Training Logs
{% if hide_eval_lines %}<details><summary>Click to expand</summary>",usage
"```bash
pip install -U sentence-transformers
```",installation
"---
title: RepoSage Chatbot
emoji: 🤖
colorFrom: indigo
colorTo: blue
sdk: streamlit
sdk_version: ""1.46.0""
app_file: app.py
pinned: true
---
# RepoSage™ Chatbot",other
Update this directory using maint_tools/vendor_array_api_extra.sh,reference
"```bash
pip install -U sentence-transformers
```",installation
"# 3) Smoke-test CLI
python -m assist.main chat ""hello world""",usage
"<!-- If there is a paper or blog post introducing the model, the APA and Bibtex information for that should go in this section. -->",reference
"```bash
pip install -U sentence-transformers
```",installation
<details><summary>Click to see the direct usage in Transformers</summary>,usage
"```bash
pip install -U sentence-transformers
```",installation
"{%- if eval_lines %}
### Training Logs
{% if hide_eval_lines %}<details><summary>Click to expand</summary>",usage
"<!-- If there is a paper or blog post introducing the dataset, the APA and Bibtex information for that should go in this section. -->",reference
"{{ curation_rationale_section | default(""[More Information Needed]"", true)}}",other
"# Or call the REST API
curl -X POST https://<your-space>.hf.space/query \
     -d '{""question"":""What is DeepSeek?""}'",reference
<details><summary>Click to expand</summary>,usage
"### Metrics
{% for metrics in eval_metrics %}
#### {{ metrics.description }}
{% if metrics.dataset_name %}
* Dataset{% if metrics.dataset_name is not string and metrics.dataset_name | length > 1 %}s{% endif %}: {% if metrics.dataset_name is string -%}
        `{{ metrics.dataset_name }}`
    {%- else -%}
        {%- for name in metrics.dataset_name -%}
            `{{ name }}`
            {%- if not loop.last -%}
                {%- if loop.index == metrics.dataset_name | length - 1 %} and {% else -%}, {% endif -%}
            {%- endif -%}
        {%- endfor -%}
    {%- endif -%}
{%- endif %}
* Evaluated with {% if metrics.class_name.startswith(""sentence_transformers."") %}[<code>{{ metrics.class_name.split(""."")[-1] }}</code>](https://sbert.net/docs/package_reference/cross_encoder/evaluation.html#sentence_transformers.cross_encoder.evaluation.{{ metrics.class_name.split(""."")[-1] }}){% else %}<code>{{ metrics.class_name }}</code>{% endif %}{% if metrics.config_code %} with these parameters:
{{ metrics.config_code }}{% endif %}",reference
"# Query
reposage query ""What is DeepSeek?"" --threshold 0.3",usage
"# 3) Smoke-test CLI
python -m assist.main chat ""hello world""",usage
Update this directory using maint_tools/vendor_array_api_compat.sh,reference
First install the Sentence Transformers library:,installation
Update this directory using maint_tools/vendor_array_api_extra.sh,reference
<details><summary>Click to see the direct usage in Transformers</summary>,usage
"## Features
- **DeepSeek Index** of lecture slides, code, and readings  
- **Bayesian Posterior Scoring** for precise answer selection  
- **Transformer Fallback** via HF text2text when confidence is low  
- **CLI & REST API** for local and web deployment  
- **CI/CD** via GitHub Actions → Hugging Face Spaces",usage
"# 3) Smoke-test CLI
python -m assist.main chat ""hello world""",usage
"</details>
-->",other
"```bash
pip install -U sentence-transformers
```",installation
"* After labeling paragraphs (e.g. `installation`, `usage`, `configuration`, `api_reference`, `troubleshooting`), train an H2O AutoML model to predict the category of any incoming question.
   * This routing step ensures each query is answered by the most relevant section of your docs.",installation
"{% for name, value in non_default_hyperparameters.items() %}- `{{ name }}`: {{ value }}
{% endfor %}{%- endif %}
#### All Hyperparameters
<details><summary>Click to expand</summary>",usage
"# Slide 3: Bayesian Q&A Layer
- Cosine-likelihood softmax → posterior",reference
"{{ data_collection_and_processing_section | default(""[More Information Needed]"", true)}}",other
"```bash
pip install -U sentence-transformers
```",installation
<details><summary>Click to see the direct usage in Transformers</summary>,usage
"*List how the model may foreseeably be misused and address what users ought not to do with the model.*
-->
{% if eval_metrics %}
## Evaluation",other
"# Slide 1: Architecture Overview
- User → RepoSage CLI → index/query modules",usage
"{% for name, value in non_default_hyperparameters.items() %}- `{{ name }}`: {{ value }}
{% endfor %}{%- endif %}
#### All Hyperparameters
<details><summary>Click to expand</summary>",usage
"<!-- If there is a paper or blog post introducing the model, the APA and Bibtex information for that should go in this section. -->",reference
"### Metrics
{% for metrics in eval_metrics %}
#### {{ metrics.description }}
{% if metrics.dataset_name %}
* Dataset{% if metrics.dataset_name is not string and metrics.dataset_name | length > 1 %}s{% endif %}: {% if metrics.dataset_name is string -%}
        `{{ metrics.dataset_name }}`
    {%- else -%}
        {%- for name in metrics.dataset_name -%}
            `{{ name }}`
            {%- if not loop.last -%}
                {%- if loop.index == metrics.dataset_name | length - 1 %} and {% else -%}, {% endif -%}
            {%- endif -%}
        {%- endfor -%}
    {%- endif -%}
{%- endif %}
* Evaluated with {% if metrics.class_name.startswith(""sentence_transformers."") %}[<code>{{ metrics.class_name.split(""."")[-1] }}</code>](https://sbert.net/docs/package_reference/cross_encoder/evaluation.html#sentence_transformers.cross_encoder.evaluation.{{ metrics.class_name.split(""."")[-1] }}){% else %}<code>{{ metrics.class_name }}</code>{% endif %}{% if metrics.config_code %} with these parameters:
{{ metrics.config_code }}{% endif %}",reference
"# Query
reposage query ""What is DeepSeek?"" --threshold 0.3",usage
"* After labeling paragraphs (e.g. `installation`, `usage`, `configuration`, `api_reference`, `troubleshooting`), train an H2O AutoML model to predict the category of any incoming question.
   * This routing step ensures each query is answered by the most relevant section of your docs.",installation
## Citation,other
Update this directory using maint_tools/vendor_array_api_extra.sh,reference
"# 2) Install dependencies
pip install --upgrade pip setuptools wheel
pip install -r requirements.txt",installation
"### Installation
```bash
git clone https://github.com/RohitRajpoot/reposage.git
cd reposage
pip install -r requirements.txt",installation
Update this directory using maint_tools/vendor_array_api_compat.sh,reference
"### Usage
# Build the index
reposage index",usage
"### Installation
```bash
git clone https://github.com/RohitRajpoot/reposage.git
cd reposage
pip install -r requirements.txt",installation
## ⚙️ Installation,installation
"# 2) Install dependencies
pip install --upgrade pip setuptools wheel
pip install -r requirements.txt",installation
First install the Sentence Transformers library:,installation
"* The entire stack is orchestrated in `app.py` (or Gradio) as a Streamlit application.
   * Deploy on Hugging Face Spaces for instant access.
   * Users type a question, see “Routed to category: X,” and receive a clear, sourced answer.",other
"# Build the index
reposage index",usage
#### Metrics,other
"{%- if eval_lines %}
### Training Logs
{% if hide_eval_lines %}<details><summary>Click to expand</summary>",usage
## 📖 Overview,other
"<!-- This section describes the source data (e.g. news text and headlines, social media posts, translated sentences, ...). -->",reference
<!-- Provide a quick summary of the dataset. -->,other
"{% for name, value in non_default_hyperparameters.items() %}- `{{ name }}`: {{ value }}
{% endfor %}{%- endif %}
#### All Hyperparameters
<details><summary>Click to expand</summary>",usage
"| Team Member | Role(s)                 | Responsibilities                             |
|-------------|-------------------------|----------------------------------------------|
| Rohit       | CEO / Product Lead,     | • Define product vision & roadmap<br>• Manage scope & stakeholder communication<br>• Write technical documentation |
| Kishan      | CTO / DevOps Lead,      | • Develop DeepSeek index & CLI scaffold<br>• Set up CI/CD pipelines<br>• Manage Hugging Face Spaces deployments   |
| Maitri      | Data Scientist / AI Lead| • Implement Bayesian Q&A layer<br>• Design & tune priors<br>• Oversee transformer fallback prompts          |",usage
"## Training Details
{% for dataset_type, dataset_list in [(""training"", train_datasets), (""evaluation"", eval_datasets)] %}{% if dataset_list %}
### {{ dataset_type.title() }} Dataset{{""s"" if dataset_list | length > 1 else """"}}
{% for dataset in dataset_list %}{% if dataset_list | length > 3 %}<details><summary>{{ dataset['name'] or 'Unnamed Dataset' }}</summary>
{% endif %}
#### {{ dataset['name'] or 'Unnamed Dataset' }}
{% if dataset['name'] %}
* Dataset: {% if 'id' in dataset %}[{{ dataset['name'] }}](https://huggingface.co/datasets/{{ dataset['id'] }}){% else %}{{ dataset['name'] }}{% endif %}
{%- if 'revision' in dataset and 'id' in dataset %} at [{{ dataset['revision'][:7] }}](https://huggingface.co/datasets/{{ dataset['id'] }}/tree/{{ dataset['revision'] }}){% endif %}{% endif %}
{% if dataset['size'] %}* Size: {{ ""{:,}"".format(dataset['size']) }} {{ dataset_type }} samples
{% endif %}* Columns: {% if dataset['columns'] | length == 1 %}{{ dataset['columns'][0] }}{% elif dataset['columns'] | length == 2 %}{{ dataset['columns'][0] }} and {{ dataset['columns'][1] }}{% else %}{{ dataset['columns'][:-1] | join(', ') }}, and {{ dataset['columns'][-1] }}{% endif %}
{% if dataset['stats_table'] %}* Approximate statistics based on the first {{ [dataset['size'], 1000] | min }} samples:
{{ dataset['stats_table'] }}{% endif %}{% if dataset['examples_table'] %}* Samples:
{{ dataset['examples_table'] }}{% endif %}* Loss: {% if dataset[""loss""][""fullname""].startswith(""sentence_transformers."") %}[<code>{{ dataset[""loss""][""fullname""].split(""."")[-1] }}</code>](https://sbert.net/docs/package_reference/cross_encoder/losses.html#{{ dataset[""loss""][""fullname""].split(""."")[-1].lower() }}){% else %}<code>{{ dataset[""loss""][""fullname""] }}</code>{% endif %}{% if ""config_code"" in dataset[""loss""] %} with these parameters:
{{ dataset[""loss""][""config_code""] }}{% endif %}
{% if dataset_list | length > 3 %}</details>
{% endif %}{% endfor %}{% endif %}{% endfor -%}",reference
<!-- This relates heavily to the Technical Specifications. Content here should link to that section when it is relevant to the training procedure. -->,other
"```bash
pip install -U sentence-transformers
```",installation
"# Query
reposage query ""What is DeepSeek?"" --threshold 0.3",usage
"{%- if eval_lines %}
### Training Logs
{% if hide_eval_lines %}<details><summary>Click to expand</summary>",usage
## ⚙️ Installation,installation
"```bash
pip install -U sentence-transformers
```",installation
<details><summary>Click to see the direct usage in Transformers</summary>,usage
"# 2) Install dependencies
pip install --upgrade pip setuptools wheel
pip install -r requirements.txt",installation
"# 3) Smoke-test CLI
python -m assist.main chat ""hello world""",usage
<details><summary>Click to see the direct usage in Transformers</summary>,usage
"### Metrics
{% for metrics in eval_metrics %}
#### {{ metrics.description }}
{% if metrics.dataset_name %}
* Dataset{% if metrics.dataset_name is not string and metrics.dataset_name | length > 1 %}s{% endif %}: {% if metrics.dataset_name is string -%}
        `{{ metrics.dataset_name }}`
    {%- else -%}
        {%- for name in metrics.dataset_name -%}
            `{{ name }}`
            {%- if not loop.last -%}
                {%- if loop.index == metrics.dataset_name | length - 1 %} and {% else -%}, {% endif -%}
            {%- endif -%}
        {%- endfor -%}
    {%- endif -%}
{%- endif %}
* Evaluated with {% if metrics.class_name.startswith(""sentence_transformers."") %}[<code>{{ metrics.class_name.split(""."")[-1] }}</code>](https://sbert.net/docs/package_reference/cross_encoder/evaluation.html#sentence_transformers.cross_encoder.evaluation.{{ metrics.class_name.split(""."")[-1] }}){% else %}<code>{{ metrics.class_name }}</code>{% endif %}{% if metrics.config_code %} with these parameters:
{{ metrics.config_code }}{% endif %}",reference
"* After labeling paragraphs (e.g. `installation`, `usage`, `configuration`, `api_reference`, `troubleshooting`), train an H2O AutoML model to predict the category of any incoming question.
   * This routing step ensures each query is answered by the most relevant section of your docs.",installation
"### Metrics
{% for metrics in eval_metrics %}
#### {{ metrics.description }}
{% if metrics.dataset_name %}
* Dataset{% if metrics.dataset_name is not string and metrics.dataset_name | length > 1 %}s{% endif %}: {% if metrics.dataset_name is string -%}
        `{{ metrics.dataset_name }}`
    {%- else -%}
        {%- for name in metrics.dataset_name -%}
            `{{ name }}`
            {%- if not loop.last -%}
                {%- if loop.index == metrics.dataset_name | length - 1 %} and {% else -%}, {% endif -%}
            {%- endif -%}
        {%- endfor -%}
    {%- endif -%}
{%- endif %}
* Evaluated with {% if metrics.class_name.startswith(""sentence_transformers."") %}[<code>{{ metrics.class_name.split(""."")[-1] }}</code>](https://sbert.net/docs/package_reference/cross_encoder/evaluation.html#sentence_transformers.cross_encoder.evaluation.{{ metrics.class_name.split(""."")[-1] }}){% else %}<code>{{ metrics.class_name }}</code>{% endif %}{% if metrics.config_code %} with these parameters:
{{ metrics.config_code }}{% endif %}",reference
"```bash
pip install -U sentence-transformers
```",installation
## Copyright,other
"### Metrics
{% for metrics in eval_metrics %}
#### {{ metrics.description }}
{% if metrics.dataset_name %}
* Dataset{% if metrics.dataset_name is not string and metrics.dataset_name | length > 1 %}s{% endif %}: {% if metrics.dataset_name is string -%}
        `{{ metrics.dataset_name }}`
    {%- else -%}
        {%- for name in metrics.dataset_name -%}
            `{{ name }}`
            {%- if not loop.last -%}
                {%- if loop.index == metrics.dataset_name | length - 1 %} and {% else -%}, {% endif -%}
            {%- endif -%}
        {%- endfor -%}
    {%- endif -%}
{%- endif %}
* Evaluated with {% if metrics.class_name.startswith(""sentence_transformers."") %}[<code>{{ metrics.class_name.split(""."")[-1] }}</code>](https://sbert.net/docs/package_reference/sentence_transformer/evaluation.html#sentence_transformers.evaluation.{{ metrics.class_name.split(""."")[-1] }}){% else %}<code>{{ metrics.class_name }}</code>{% endif %}{% if metrics.config_code %} with these parameters:
{{ metrics.config_code }}{% endif %}",reference
"## Training Details
{% for dataset_type, dataset_list in [(""training"", train_datasets), (""evaluation"", eval_datasets)] %}{% if dataset_list %}
### {{ dataset_type.title() }} Dataset{{""s"" if dataset_list | length > 1 else """"}}
{% for dataset in dataset_list %}{% if dataset_list | length > 3 %}<details><summary>{{ dataset['name'] or 'Unnamed Dataset' }}</summary>
{% endif %}
#### {{ dataset['name'] or 'Unnamed Dataset' }}
{% if dataset['name'] %}
* Dataset: {% if 'id' in dataset %}[{{ dataset['name'] }}](https://huggingface.co/datasets/{{ dataset['id'] }}){% else %}{{ dataset['name'] }}{% endif %}
{%- if 'revision' in dataset and 'id' in dataset %} at [{{ dataset['revision'][:7] }}](https://huggingface.co/datasets/{{ dataset['id'] }}/tree/{{ dataset['revision'] }}){% endif %}{% endif %}
{% if dataset['size'] %}* Size: {{ ""{:,}"".format(dataset['size']) }} {{ dataset_type }} samples
{% endif %}* Columns: {% if dataset['columns'] | length == 1 %}{{ dataset['columns'][0] }}{% elif dataset['columns'] | length == 2 %}{{ dataset['columns'][0] }} and {{ dataset['columns'][1] }}{% else %}{{ dataset['columns'][:-1] | join(', ') }}, and {{ dataset['columns'][-1] }}{% endif %}
{% if dataset['stats_table'] %}* Approximate statistics based on the first {{ [dataset['size'], 1000] | min }} samples:
{{ dataset['stats_table'] }}{% endif %}{% if dataset['examples_table'] %}* Samples:
{{ dataset['examples_table'] }}{% endif %}* Loss: {% if dataset[""loss""][""fullname""].startswith(""sentence_transformers."") %}[<code>{{ dataset[""loss""][""fullname""].split(""."")[-1] }}</code>](https://sbert.net/docs/package_reference/cross_encoder/losses.html#{{ dataset[""loss""][""fullname""].split(""."")[-1].lower() }}){% else %}<code>{{ dataset[""loss""][""fullname""] }}</code>{% endif %}{% if ""config_code"" in dataset[""loss""] %} with these parameters:
{{ dataset[""loss""][""config_code""] }}{% endif %}
{% if dataset_list | length > 3 %}</details>
{% endif %}{% endfor %}{% endif %}{% endfor -%}",reference
First install the Sentence Transformers library:,installation
"<!-- This section is meant to convey recommendations with respect to the bias, risk, and technical limitations. -->",other
"### Installation
```bash
git clone https://github.com/RohitRajpoot/reposage.git
cd reposage
pip install -r requirements.txt",installation
"**THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""
AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE
LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR
CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF
SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS
INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN
CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)
ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF
THE POSSIBILITY OF SUCH DAMAGE.**",other
"* After labeling paragraphs (e.g. `installation`, `usage`, `configuration`, `api_reference`, `troubleshooting`), train an H2O AutoML model to predict the category of any incoming question.
   * This routing step ensures each query is answered by the most relevant section of your docs.",installation
"```bash
python scripts/prepare_corpus.py \
  --input-dir reposage/ \
  --output data/classification.csv
```",other
"# 3) Smoke-test CLI
python -m assist.main chat ""hello world""",usage
"<!-- If there is a paper or blog post introducing the dataset, the APA and Bibtex information for that should go in this section. -->",reference
"Developed by: Kevin Sheppard (<kevin.sheppard@economics.ox.ac.uk>,
<kevin.k.sheppard@gmail.com>)
[http://www.kevinsheppard.com](http://www.kevinsheppard.com)",other
"### Metrics
{% for metrics in eval_metrics %}
#### {{ metrics.description }}
{% if metrics.dataset_name %}
* Dataset{% if metrics.dataset_name is not string and metrics.dataset_name | length > 1 %}s{% endif %}: {% if metrics.dataset_name is string -%}
        `{{ metrics.dataset_name }}`
    {%- else -%}
        {%- for name in metrics.dataset_name -%}
            `{{ name }}`
            {%- if not loop.last -%}
                {%- if loop.index == metrics.dataset_name | length - 1 %} and {% else -%}, {% endif -%}
            {%- endif -%}
        {%- endfor -%}
    {%- endif -%}
{%- endif %}
* Evaluated with {% if metrics.class_name.startswith(""sentence_transformers."") %}[<code>{{ metrics.class_name.split(""."")[-1] }}</code>](https://sbert.net/docs/package_reference/sentence_transformer/evaluation.html#sentence_transformers.evaluation.{{ metrics.class_name.split(""."")[-1] }}){% else %}<code>{{ metrics.class_name }}</code>{% endif %}{% if metrics.config_code %} with these parameters:
{{ metrics.config_code }}{% endif %}",reference
"<!-- If there is a paper or blog post introducing the model, the APA and Bibtex information for that should go in this section. -->",reference
First install the Sentence Transformers library:,installation
"<!-- If there is a paper or blog post introducing the dataset, the APA and Bibtex information for that should go in this section. -->",reference
"1. Fork and create a feature branch.
2. Add or update scripts/tests as needed.
3. Ensure style checks (`flake8`, `black`) pass.
4. Submit a pull request detailing your enhancements.",reference
"# Query
reposage query ""What is DeepSeek?"" --threshold 0.3",usage
"* `models/routing_model.zip`
* `models/faiss_index.bin`
* Embedding & Flan‑T5 models via your settings or environment variables.",other
"# Slide 4: Transformer Fallback
- Flan-T5 small for low-confidence queries",other
"* After labeling paragraphs (e.g. `installation`, `usage`, `configuration`, `api_reference`, `troubleshooting`), train an H2O AutoML model to predict the category of any incoming question.
   * This routing step ensures each query is answered by the most relevant section of your docs.",installation
"Redistribution and use in source and binary forms, with or without modification,
are permitted provided that the following conditions are met:",other
<details><summary>Click to expand</summary>,usage
<details><summary>Click to expand</summary>,usage
**BibTeX:**,other
"# Or call the REST API
curl -X POST https://<your-space>.hf.space/query \
     -d '{""question"":""What is DeepSeek?""}'",reference
"* After labeling paragraphs (e.g. `installation`, `usage`, `configuration`, `api_reference`, `troubleshooting`), train an H2O AutoML model to predict the category of any incoming question.
   * This routing step ensures each query is answered by the most relevant section of your docs.",installation
"<!-- If there is a paper or blog post introducing the model, the APA and Bibtex information for that should go in this section. -->",reference
First install the Sentence Transformers library:,installation
"*Lists the people who create the model card, providing recognition and accountability for the detailed work that goes into its construction.*
-->",other
"### Metrics
{% for metrics in eval_metrics %}
#### {{ metrics.description }}
{% if metrics.dataset_name %}
* Dataset{% if metrics.dataset_name is not string and metrics.dataset_name | length > 1 %}s{% endif %}: {% if metrics.dataset_name is string -%}
        `{{ metrics.dataset_name }}`
    {%- else -%}
        {%- for name in metrics.dataset_name -%}
            `{{ name }}`
            {%- if not loop.last -%}
                {%- if loop.index == metrics.dataset_name | length - 1 %} and {% else -%}, {% endif -%}
            {%- endif -%}
        {%- endfor -%}
    {%- endif -%}
{%- endif %}
* Evaluated with {% if metrics.class_name.startswith(""sentence_transformers."") %}[<code>{{ metrics.class_name.split(""."")[-1] }}</code>](https://sbert.net/docs/package_reference/cross_encoder/evaluation.html#sentence_transformers.cross_encoder.evaluation.{{ metrics.class_name.split(""."")[-1] }}){% else %}<code>{{ metrics.class_name }}</code>{% endif %}{% if metrics.config_code %} with these parameters:
{{ metrics.config_code }}{% endif %}",reference
"## Features
- **DeepSeek Index** of lecture slides, code, and readings  
- **Bayesian Posterior Scoring** for precise answer selection  
- **Transformer Fallback** via HF text2text when confidence is low  
- **CLI & REST API** for local and web deployment  
- **CI/CD** via GitHub Actions → Hugging Face Spaces",usage
"### Metrics
{% for metrics in eval_metrics %}
#### {{ metrics.description }}
{% if metrics.dataset_name %}
* Dataset{% if metrics.dataset_name is not string and metrics.dataset_name | length > 1 %}s{% endif %}: {% if metrics.dataset_name is string -%}
        `{{ metrics.dataset_name }}`
    {%- else -%}
        {%- for name in metrics.dataset_name -%}
            `{{ name }}`
            {%- if not loop.last -%}
                {%- if loop.index == metrics.dataset_name | length - 1 %} and {% else -%}, {% endif -%}
            {%- endif -%}
        {%- endfor -%}
    {%- endif -%}
{%- endif %}
* Evaluated with {% if metrics.class_name.startswith(""sentence_transformers."") %}[<code>{{ metrics.class_name.split(""."")[-1] }}</code>](https://sbert.net/docs/package_reference/sentence_transformer/evaluation.html#sentence_transformers.evaluation.{{ metrics.class_name.split(""."")[-1] }}){% else %}<code>{{ metrics.class_name }}</code>{% endif %}{% if metrics.config_code %} with these parameters:
{{ metrics.config_code }}{% endif %}",reference
See also [how to contribute to NLTK](https://www.nltk.org/contribute.html).,other
First install the Sentence Transformers library:,installation
"```bash
pip install -U sentence-transformers
```",installation
"<!-- This section describes the source data (e.g. news text and headlines, social media posts, translated sentences, ...). -->",reference
"# Slide 1: Architecture Overview
- User → RepoSage CLI → index/query modules",usage
"# Slide 1: Architecture Overview
- User → RepoSage CLI → index/query modules",usage
First install the Sentence Transformers library:,installation
"{% for name, value in non_default_hyperparameters.items() %}- `{{ name }}`: {{ value }}
{% endfor %}{%- endif %}
#### All Hyperparameters
<details><summary>Click to expand</summary>",usage
"### Usage
# Build the index
reposage index",usage
"### Metrics
{% for metrics in eval_metrics %}
#### {{ metrics.description }}
{% if metrics.dataset_name %}
* Dataset{% if metrics.dataset_name is not string and metrics.dataset_name | length > 1 %}s{% endif %}: {% if metrics.dataset_name is string -%}
        `{{ metrics.dataset_name }}`
    {%- else -%}
        {%- for name in metrics.dataset_name -%}
            `{{ name }}`
            {%- if not loop.last -%}
                {%- if loop.index == metrics.dataset_name | length - 1 %} and {% else -%}, {% endif -%}
            {%- endif -%}
        {%- endfor -%}
    {%- endif -%}
{%- endif %}
* Evaluated with {% if metrics.class_name.startswith(""sentence_transformers."") %}[<code>{{ metrics.class_name.split(""."")[-1] }}</code>](https://sbert.net/docs/package_reference/cross_encoder/evaluation.html#sentence_transformers.cross_encoder.evaluation.{{ metrics.class_name.split(""."")[-1] }}){% else %}<code>{{ metrics.class_name }}</code>{% endif %}{% if metrics.config_code %} with these parameters:
{{ metrics.config_code }}{% endif %}",reference
"| Team Member | Role(s)                 | Responsibilities                             |
|-------------|-------------------------|----------------------------------------------|
| Rohit       | CEO / Product Lead,     | • Define product vision & roadmap<br>• Manage scope & stakeholder communication<br>• Write technical documentation |
| Kishan      | CTO / DevOps Lead,      | • Develop DeepSeek index & CLI scaffold<br>• Set up CI/CD pipelines<br>• Manage Hugging Face Spaces deployments   |
| Maitri      | Data Scientist / AI Lead| • Implement Bayesian Q&A layer<br>• Design & tune priors<br>• Oversee transformer fallback prompts          |",usage
"{%- if eval_lines %}
### Training Logs
{% if hide_eval_lines %}<details><summary>Click to expand</summary>",usage
First install the Sentence Transformers library:,installation
Update this directory using maint_tools/vendor_array_api_compat.sh,reference
"*What are the known or foreseeable issues stemming from this model? You could also flag here known failure cases or weaknesses of the model.*
-->",other
"### Metrics
{% for metrics in eval_metrics %}
#### {{ metrics.description }}
{% if metrics.dataset_name %}
* Dataset{% if metrics.dataset_name is not string and metrics.dataset_name | length > 1 %}s{% endif %}: {% if metrics.dataset_name is string -%}
        `{{ metrics.dataset_name }}`
    {%- else -%}
        {%- for name in metrics.dataset_name -%}
            `{{ name }}`
            {%- if not loop.last -%}
                {%- if loop.index == metrics.dataset_name | length - 1 %} and {% else -%}, {% endif -%}
            {%- endif -%}
        {%- endfor -%}
    {%- endif -%}
{%- endif %}
* Evaluated with {% if metrics.class_name.startswith(""sentence_transformers."") %}[<code>{{ metrics.class_name.split(""."")[-1] }}</code>](https://sbert.net/docs/package_reference/cross_encoder/evaluation.html#sentence_transformers.cross_encoder.evaluation.{{ metrics.class_name.split(""."")[-1] }}){% else %}<code>{{ metrics.class_name }}</code>{% endif %}{% if metrics.config_code %} with these parameters:
{{ metrics.config_code }}{% endif %}",reference
"### Metrics
{% for metrics in eval_metrics %}
#### {{ metrics.description }}
{% if metrics.dataset_name %}
* Dataset{% if metrics.dataset_name is not string and metrics.dataset_name | length > 1 %}s{% endif %}: {% if metrics.dataset_name is string -%}
        `{{ metrics.dataset_name }}`
    {%- else -%}
        {%- for name in metrics.dataset_name -%}
            `{{ name }}`
            {%- if not loop.last -%}
                {%- if loop.index == metrics.dataset_name | length - 1 %} and {% else -%}, {% endif -%}
            {%- endif -%}
        {%- endfor -%}
    {%- endif -%}
{%- endif %}
* Evaluated with {% if metrics.class_name.startswith(""sentence_transformers."") %}[<code>{{ metrics.class_name.split(""."")[-1] }}</code>](https://sbert.net/docs/package_reference/cross_encoder/evaluation.html#sentence_transformers.cross_encoder.evaluation.{{ metrics.class_name.split(""."")[-1] }}){% else %}<code>{{ metrics.class_name }}</code>{% endif %}{% if metrics.config_code %} with these parameters:
{{ metrics.config_code }}{% endif %}",reference
"{% if all_hyperparameters %}
### Training Hyperparameters
{% if non_default_hyperparameters -%}
#### Non-Default Hyperparameters",other
"{% for name, value in non_default_hyperparameters.items() %}- `{{ name }}`: {{ value }}
{% endfor %}{%- endif %}
#### All Hyperparameters
<details><summary>Click to expand</summary>",usage
"{% for name, value in non_default_hyperparameters.items() %}- `{{ name }}`: {{ value }}
{% endfor %}{%- endif %}
#### All Hyperparameters
<details><summary>Click to expand</summary>",usage
<details><summary>Click to see the direct usage in Transformers</summary>,usage
#### Preprocessing [optional],other
<details><summary>Click to see the direct usage in Transformers</summary>,usage
Update this directory using maint_tools/vendor_array_api_compat.sh,reference
Update this directory using maint_tools/vendor_array_api_compat.sh,reference
"### Metrics
{% for metrics in eval_metrics %}
#### {{ metrics.description }}
{% if metrics.dataset_name %}
* Dataset{% if metrics.dataset_name is not string and metrics.dataset_name | length > 1 %}s{% endif %}: {% if metrics.dataset_name is string -%}
        `{{ metrics.dataset_name }}`
    {%- else -%}
        {%- for name in metrics.dataset_name -%}
            `{{ name }}`
            {%- if not loop.last -%}
                {%- if loop.index == metrics.dataset_name | length - 1 %} and {% else -%}, {% endif -%}
            {%- endif -%}
        {%- endfor -%}
    {%- endif -%}
{%- endif %}
* Evaluated with {% if metrics.class_name.startswith(""sentence_transformers."") %}[<code>{{ metrics.class_name.split(""."")[-1] }}</code>](https://sbert.net/docs/package_reference/sentence_transformer/evaluation.html#sentence_transformers.evaluation.{{ metrics.class_name.split(""."")[-1] }}){% else %}<code>{{ metrics.class_name }}</code>{% endif %}{% if metrics.config_code %} with these parameters:
{{ metrics.config_code }}{% endif %}",reference
### Local (macOS/Linux),other
"# 2) Install dependencies
pip install --upgrade pip setuptools wheel
pip install -r requirements.txt",installation
"In `app.py`, configure paths to:",reference
"In `app.py`, configure paths to:",reference
"# Slide 3: Bayesian Q&A Layer
- Cosine-likelihood softmax → posterior",reference
## 🔄 Hybrid Pipeline Overview,other
Update this directory using maint_tools/vendor_array_api_compat.sh,reference
"```bash
pip install -U sentence-transformers
```",installation
### Training Data,other
"* After labeling paragraphs (e.g. `installation`, `usage`, `configuration`, `api_reference`, `troubleshooting`), train an H2O AutoML model to predict the category of any incoming question.
   * This routing step ensures each query is answered by the most relevant section of your docs.",installation
First install the Sentence Transformers library:,installation
"{% for name, value in non_default_hyperparameters.items() %}- `{{ name }}`: {{ value }}
{% endfor %}{%- endif %}
#### All Hyperparameters
<details><summary>Click to expand</summary>",usage
"### Metrics
{% for metrics in eval_metrics %}
#### {{ metrics.description }}
{% if metrics.dataset_name %}
* Dataset{% if metrics.dataset_name is not string and metrics.dataset_name | length > 1 %}s{% endif %}: {% if metrics.dataset_name is string -%}
        `{{ metrics.dataset_name }}`
    {%- else -%}
        {%- for name in metrics.dataset_name -%}
            `{{ name }}`
            {%- if not loop.last -%}
                {%- if loop.index == metrics.dataset_name | length - 1 %} and {% else -%}, {% endif -%}
            {%- endif -%}
        {%- endfor -%}
    {%- endif -%}
{%- endif %}
* Evaluated with {% if metrics.class_name.startswith(""sentence_transformers."") %}[<code>{{ metrics.class_name.split(""."")[-1] }}</code>](https://sbert.net/docs/package_reference/sentence_transformer/evaluation.html#sentence_transformers.evaluation.{{ metrics.class_name.split(""."")[-1] }}){% else %}<code>{{ metrics.class_name }}</code>{% endif %}{% if metrics.config_code %} with these parameters:
{{ metrics.config_code }}{% endif %}",reference
"### Installation
```bash
git clone https://github.com/RohitRajpoot/reposage.git
cd reposage
pip install -r requirements.txt",installation
"In `app.py`, configure paths to:",reference
## ⚙️ Installation,installation
"{{ hardware_requirements | default(""[More Information Needed]"", true)}}",other
"# 2) Install dependencies
pip install --upgrade pip setuptools wheel
pip install -r requirements.txt",installation
"<!-- If relevant, include terms and calculations in this section that can help readers understand the model or model card. -->",other
"NLTK -- the Natural Language Toolkit -- is a suite of open source Python
modules, data sets, and tutorials supporting research and development in Natural
Language Processing. NLTK requires Python version 3.8, 3.9, 3.10, 3.11 or 3.12.",other
"{%- if eval_lines %}
### Training Logs
{% if hide_eval_lines %}<details><summary>Click to expand</summary>",usage
# RepoSage – AML3304 AI Product Engineering,other
"### Installation
```bash
git clone https://github.com/RohitRajpoot/reposage.git
cd reposage
pip install -r requirements.txt",installation
"<!-- If there is a paper or blog post introducing the model, the APA and Bibtex information for that should go in this section. -->",reference
"# Slide 3: Bayesian Q&A Layer
- Cosine-likelihood softmax → posterior",reference
"{% for name, value in non_default_hyperparameters.items() %}- `{{ name }}`: {{ value }}
{% endfor %}{%- endif %}
#### All Hyperparameters
<details><summary>Click to expand</summary>",usage
"## Training Details
{% for dataset_type, dataset_list in [(""training"", train_datasets), (""evaluation"", eval_datasets)] %}{% if dataset_list %}
### {{ dataset_type.title() }} Dataset{{""s"" if dataset_list | length > 1 else """"}}
{% for dataset in dataset_list %}{% if dataset_list | length > 3 %}<details><summary>{{ dataset['name'] or 'Unnamed Dataset' }}</summary>
{% endif %}
#### {{ dataset['name'] or 'Unnamed Dataset' }}
{% if dataset['name'] %}
* Dataset: {% if 'id' in dataset %}[{{ dataset['name'] }}](https://huggingface.co/datasets/{{ dataset['id'] }}){% else %}{{ dataset['name'] }}{% endif %}
{%- if 'revision' in dataset and 'id' in dataset %} at [{{ dataset['revision'][:7] }}](https://huggingface.co/datasets/{{ dataset['id'] }}/tree/{{ dataset['revision'] }}){% endif %}{% endif %}
{% if dataset['size'] %}* Size: {{ ""{:,}"".format(dataset['size']) }} {{ dataset_type }} samples
{% endif %}* Columns: {% if dataset['columns'] | length == 1 %}{{ dataset['columns'][0] }}{% elif dataset['columns'] | length == 2 %}{{ dataset['columns'][0] }} and {{ dataset['columns'][1] }}{% else %}{{ dataset['columns'][:-1] | join(', ') }}, and {{ dataset['columns'][-1] }}{% endif %}
{% if dataset['stats_table'] %}* Approximate statistics based on the first {{ [dataset['size'], 1000] | min }} samples:
{{ dataset['stats_table'] }}{% endif %}{% if dataset['examples_table'] %}* Samples:
{{ dataset['examples_table'] }}{% endif %}* Loss: {% if dataset[""loss""][""fullname""].startswith(""sentence_transformers."") %}[<code>{{ dataset[""loss""][""fullname""].split(""."")[-1] }}</code>](https://sbert.net/docs/package_reference/cross_encoder/losses.html#{{ dataset[""loss""][""fullname""].split(""."")[-1].lower() }}){% else %}<code>{{ dataset[""loss""][""fullname""] }}</code>{% endif %}{% if ""config_code"" in dataset[""loss""] %} with these parameters:
{{ dataset[""loss""][""config_code""] }}{% endif %}
{% if dataset_list | length > 3 %}</details>
{% endif %}{% endfor %}{% endif %}{% endfor -%}",reference
"{% for name, value in non_default_hyperparameters.items() %}- `{{ name }}`: {{ value }}
{% endfor %}{%- endif %}
#### All Hyperparameters
<details><summary>Click to expand</summary>",usage
"{% for name, value in non_default_hyperparameters.items() %}- `{{ name }}`: {{ value }}
{% endfor %}{%- endif %}
#### All Hyperparameters
<details><summary>Click to expand</summary>",usage
<details><summary>Click to expand</summary>,usage
"{{ model_card_contact | default(""[More Information Needed]"", true)}}",other
### 3. Build the FAISS Index,other
"# Slide 1: Architecture Overview
- User → RepoSage CLI → index/query modules",usage
Licensed under MIT. See [LICENSE](LICENSE) for details.,other
"<!-- If there is a paper or blog post introducing the dataset, the APA and Bibtex information for that should go in this section. -->",reference
"# Slide 3: Bayesian Q&A Layer
- Cosine-likelihood softmax → posterior",reference
"```bash
pip install -U sentence-transformers
```",installation
"{% for name, value in non_default_hyperparameters.items() %}- `{{ name }}`: {{ value }}
{% endfor %}{%- endif %}
#### All Hyperparameters
<details><summary>Click to expand</summary>",usage
Update this directory using maint_tools/vendor_array_api_extra.sh,reference
<details><summary>Click to see the direct usage in Transformers</summary>,usage
"### Metrics
{% for metrics in eval_metrics %}
#### {{ metrics.description }}
{% if metrics.dataset_name %}
* Dataset{% if metrics.dataset_name is not string and metrics.dataset_name | length > 1 %}s{% endif %}: {% if metrics.dataset_name is string -%}
        `{{ metrics.dataset_name }}`
    {%- else -%}
        {%- for name in metrics.dataset_name -%}
            `{{ name }}`
            {%- if not loop.last -%}
                {%- if loop.index == metrics.dataset_name | length - 1 %} and {% else -%}, {% endif -%}
            {%- endif -%}
        {%- endfor -%}
    {%- endif -%}
{%- endif %}
* Evaluated with {% if metrics.class_name.startswith(""sentence_transformers."") %}[<code>{{ metrics.class_name.split(""."")[-1] }}</code>](https://sbert.net/docs/package_reference/sentence_transformer/evaluation.html#sentence_transformers.evaluation.{{ metrics.class_name.split(""."")[-1] }}){% else %}<code>{{ metrics.class_name }}</code>{% endif %}{% if metrics.config_code %} with these parameters:
{{ metrics.config_code }}{% endif %}",reference
"In `app.py`, configure paths to:",reference
<!-- This section is for the model use without fine-tuning or plugging into a larger ecosystem/app. -->,other
"### Metrics
{% for metrics in eval_metrics %}
#### {{ metrics.description }}
{% if metrics.dataset_name %}
* Dataset{% if metrics.dataset_name is not string and metrics.dataset_name | length > 1 %}s{% endif %}: {% if metrics.dataset_name is string -%}
        `{{ metrics.dataset_name }}`
    {%- else -%}
        {%- for name in metrics.dataset_name -%}
            `{{ name }}`
            {%- if not loop.last -%}
                {%- if loop.index == metrics.dataset_name | length - 1 %} and {% else -%}, {% endif -%}
            {%- endif -%}
        {%- endfor -%}
    {%- endif -%}
{%- endif %}
* Evaluated with {% if metrics.class_name.startswith(""sentence_transformers."") %}[<code>{{ metrics.class_name.split(""."")[-1] }}</code>](https://sbert.net/docs/package_reference/sentence_transformer/evaluation.html#sentence_transformers.evaluation.{{ metrics.class_name.split(""."")[-1] }}){% else %}<code>{{ metrics.class_name }}</code>{% endif %}{% if metrics.config_code %} with these parameters:
{{ metrics.config_code }}{% endif %}",reference
<details><summary>Click to see the direct usage in Transformers</summary>,usage
## Donate,other
"<!-- This section describes the source data (e.g. news text and headlines, social media posts, translated sentences, ...). -->",reference
"## Training Details
{% for dataset_type, dataset_list in [(""training"", train_datasets), (""evaluation"", eval_datasets)] %}{% if dataset_list %}
### {{ dataset_type.title() }} Dataset{{""s"" if dataset_list | length > 1 else """"}}
{% for dataset in dataset_list %}{% if dataset_list | length > 3 %}<details><summary>{{ dataset['name'] or 'Unnamed Dataset' }}</summary>
{% endif %}
#### {{ dataset['name'] or 'Unnamed Dataset' }}
{% if dataset['name'] %}
* Dataset: {% if 'id' in dataset %}[{{ dataset['name'] }}](https://huggingface.co/datasets/{{ dataset['id'] }}){% else %}{{ dataset['name'] }}{% endif %}
{%- if 'revision' in dataset and 'id' in dataset %} at [{{ dataset['revision'][:7] }}](https://huggingface.co/datasets/{{ dataset['id'] }}/tree/{{ dataset['revision'] }}){% endif %}{% endif %}
{% if dataset['size'] %}* Size: {{ ""{:,}"".format(dataset['size']) }} {{ dataset_type }} samples
{% endif %}* Columns: {% if dataset['columns'] | length == 1 %}{{ dataset['columns'][0] }}{% elif dataset['columns'] | length == 2 %}{{ dataset['columns'][0] }} and {{ dataset['columns'][1] }}{% else %}{{ dataset['columns'][:-1] | join(', ') }}, and {{ dataset['columns'][-1] }}{% endif %}
{% if dataset['stats_table'] %}* Approximate statistics based on the first {{ [dataset['size'], 1000] | min }} samples:
{{ dataset['stats_table'] }}{% endif %}{% if dataset['examples_table'] %}* Samples:
{{ dataset['examples_table'] }}{% endif %}* Loss: {% if dataset[""loss""][""fullname""].startswith(""sentence_transformers."") %}[<code>{{ dataset[""loss""][""fullname""].split(""."")[-1] }}</code>](https://sbert.net/docs/package_reference/cross_encoder/losses.html#{{ dataset[""loss""][""fullname""].split(""."")[-1].lower() }}){% else %}<code>{{ dataset[""loss""][""fullname""] }}</code>{% endif %}{% if ""config_code"" in dataset[""loss""] %} with these parameters:
{{ dataset[""loss""][""config_code""] }}{% endif %}
{% if dataset_list | length > 3 %}</details>
{% endif %}{% endfor %}{% endif %}{% endfor -%}",reference
<details><summary>Click to expand</summary>,usage
"| Team Member | Role(s)                 | Responsibilities                             |
|-------------|-------------------------|----------------------------------------------|
| Rohit       | CEO / Product Lead,     | • Define product vision & roadmap<br>• Manage scope & stakeholder communication<br>• Write technical documentation |
| Kishan      | CTO / DevOps Lead,      | • Develop DeepSeek index & CLI scaffold<br>• Set up CI/CD pipelines<br>• Manage Hugging Face Spaces deployments   |
| Maitri      | Data Scientist / AI Lead| • Implement Bayesian Q&A layer<br>• Design & tune priors<br>• Oversee transformer fallback prompts          |",usage
"{% for name, value in non_default_hyperparameters.items() %}- `{{ name }}`: {{ value }}
{% endfor %}{%- endif %}
#### All Hyperparameters
<details><summary>Click to expand</summary>",usage
"```bash
pip install -U sentence-transformers
```",installation
### 2. Train the H2O Classifier,other
"<!--
## Model Card Authors",other
"<!-- If there is a paper or blog post introducing the dataset, the APA and Bibtex information for that should go in this section. -->",reference
"# Slide 3: Bayesian Q&A Layer
- Cosine-likelihood softmax → posterior",reference
"### Installation
```bash
git clone https://github.com/RohitRajpoot/reposage.git
cd reposage
pip install -r requirements.txt",installation
<!-- Motivation for the creation of this dataset. -->,other
"In `app.py`, configure paths to:",reference
"# 2) Install dependencies
pip install --upgrade pip setuptools wheel
pip install -r requirements.txt",installation
"### Installation
```bash
git clone https://github.com/RohitRajpoot/reposage.git
cd reposage
pip install -r requirements.txt",installation
<details><summary>Click to see the direct usage in Transformers</summary>,usage
<!-- This section describes suitable use cases for the dataset. -->,other
### Compute Infrastructure,other
"# Build the index
reposage index",usage
"* After labeling paragraphs (e.g. `installation`, `usage`, `configuration`, `api_reference`, `troubleshooting`), train an H2O AutoML model to predict the category of any incoming question.
   * This routing step ensures each query is answered by the most relevant section of your docs.",installation
"* After labeling paragraphs (e.g. `installation`, `usage`, `configuration`, `api_reference`, `troubleshooting`), train an H2O AutoML model to predict the category of any incoming question.
   * This routing step ensures each query is answered by the most relevant section of your docs.",installation
First install the Sentence Transformers library:,installation
<details><summary>Click to see the direct usage in Transformers</summary>,usage
"### Metrics
{% for metrics in eval_metrics %}
#### {{ metrics.description }}
{% if metrics.dataset_name %}
* Dataset{% if metrics.dataset_name is not string and metrics.dataset_name | length > 1 %}s{% endif %}: {% if metrics.dataset_name is string -%}
        `{{ metrics.dataset_name }}`
    {%- else -%}
        {%- for name in metrics.dataset_name -%}
            `{{ name }}`
            {%- if not loop.last -%}
                {%- if loop.index == metrics.dataset_name | length - 1 %} and {% else -%}, {% endif -%}
            {%- endif -%}
        {%- endfor -%}
    {%- endif -%}
{%- endif %}
* Evaluated with {% if metrics.class_name.startswith(""sentence_transformers."") %}[<code>{{ metrics.class_name.split(""."")[-1] }}</code>](https://sbert.net/docs/package_reference/cross_encoder/evaluation.html#sentence_transformers.cross_encoder.evaluation.{{ metrics.class_name.split(""."")[-1] }}){% else %}<code>{{ metrics.class_name }}</code>{% endif %}{% if metrics.config_code %} with these parameters:
{{ metrics.config_code }}{% endif %}",reference
"{% for name, value in non_default_hyperparameters.items() %}- `{{ name }}`: {{ value }}
{% endfor %}{%- endif %}
#### All Hyperparameters
<details><summary>Click to expand</summary>",usage
"If you publish work that uses NLTK, please cite the NLTK book, as follows:",other
Manually open `data/classification.csv` and assign each paragraph a category label.,other
"1. Fork and create a feature branch.
2. Add or update scripts/tests as needed.
3. Ensure style checks (`flake8`, `black`) pass.
4. Submit a pull request detailing your enhancements.",reference
"# Query
reposage query ""What is DeepSeek?"" --threshold 0.3",usage
"# Build the index
reposage index",usage
"### Installation
```bash
git clone https://github.com/RohitRajpoot/reposage.git
cd reposage
pip install -r requirements.txt",installation
"## Training Details
{% for dataset_type, dataset_list in [(""training"", train_datasets), (""evaluation"", eval_datasets)] %}{% if dataset_list %}
### {{ dataset_type.title() }} Dataset{{""s"" if dataset_list | length > 1 else """"}}
{% for dataset in dataset_list %}{% if dataset_list | length > 3 %}<details><summary>{{ dataset['name'] or 'Unnamed Dataset' }}</summary>
{% endif %}
#### {{ dataset['name'] or 'Unnamed Dataset' }}
{% if dataset['name'] %}
* Dataset: {% if 'id' in dataset %}[{{ dataset['name'] }}](https://huggingface.co/datasets/{{ dataset['id'] }}){% else %}{{ dataset['name'] }}{% endif %}
{%- if 'revision' in dataset and 'id' in dataset %} at [{{ dataset['revision'][:7] }}](https://huggingface.co/datasets/{{ dataset['id'] }}/tree/{{ dataset['revision'] }}){% endif %}{% endif %}
{% if dataset['size'] %}* Size: {{ ""{:,}"".format(dataset['size']) }} {{ dataset_type }} samples
{% endif %}* Columns: {% if dataset['columns'] | length == 1 %}{{ dataset['columns'][0] }}{% elif dataset['columns'] | length == 2 %}{{ dataset['columns'][0] }} and {{ dataset['columns'][1] }}{% else %}{{ dataset['columns'][:-1] | join(', ') }}, and {{ dataset['columns'][-1] }}{% endif %}
{% if dataset['stats_table'] %}* Approximate statistics based on the first {{ [dataset['size'], 1000] | min }} samples:
{{ dataset['stats_table'] }}{% endif %}{% if dataset['examples_table'] %}* Samples:
{{ dataset['examples_table'] }}{% endif %}* Loss: {% if dataset[""loss""][""fullname""].startswith(""sentence_transformers."") %}[<code>{{ dataset[""loss""][""fullname""].split(""."")[-1] }}</code>](https://sbert.net/docs/package_reference/sentence_transformer/losses.html#{{ dataset[""loss""][""fullname""].split(""."")[-1].lower() }}){% else %}<code>{{ dataset[""loss""][""fullname""] }}</code>{% endif %}{% if ""config_code"" in dataset[""loss""] %} with these parameters:
{{ dataset[""loss""][""config_code""] }}{% endif %}
{% if dataset_list | length > 3 %}</details>
{% endif %}{% endfor %}{% endif %}{% endfor -%}",reference
"```bash
pip install -U sentence-transformers
```",installation
"## Training Details
{% for dataset_type, dataset_list in [(""training"", train_datasets), (""evaluation"", eval_datasets)] %}{% if dataset_list %}
### {{ dataset_type.title() }} Dataset{{""s"" if dataset_list | length > 1 else """"}}
{% for dataset in dataset_list %}{% if dataset_list | length > 3 %}<details><summary>{{ dataset['name'] or 'Unnamed Dataset' }}</summary>
{% endif %}
#### {{ dataset['name'] or 'Unnamed Dataset' }}
{% if dataset['name'] %}
* Dataset: {% if 'id' in dataset %}[{{ dataset['name'] }}](https://huggingface.co/datasets/{{ dataset['id'] }}){% else %}{{ dataset['name'] }}{% endif %}
{%- if 'revision' in dataset and 'id' in dataset %} at [{{ dataset['revision'][:7] }}](https://huggingface.co/datasets/{{ dataset['id'] }}/tree/{{ dataset['revision'] }}){% endif %}{% endif %}
{% if dataset['size'] %}* Size: {{ ""{:,}"".format(dataset['size']) }} {{ dataset_type }} samples
{% endif %}* Columns: {% if dataset['columns'] | length == 1 %}{{ dataset['columns'][0] }}{% elif dataset['columns'] | length == 2 %}{{ dataset['columns'][0] }} and {{ dataset['columns'][1] }}{% else %}{{ dataset['columns'][:-1] | join(', ') }}, and {{ dataset['columns'][-1] }}{% endif %}
{% if dataset['stats_table'] %}* Approximate statistics based on the first {{ [dataset['size'], 1000] | min }} samples:
{{ dataset['stats_table'] }}{% endif %}{% if dataset['examples_table'] %}* Samples:
{{ dataset['examples_table'] }}{% endif %}* Loss: {% if dataset[""loss""][""fullname""].startswith(""sentence_transformers."") %}[<code>{{ dataset[""loss""][""fullname""].split(""."")[-1] }}</code>](https://sbert.net/docs/package_reference/cross_encoder/losses.html#{{ dataset[""loss""][""fullname""].split(""."")[-1].lower() }}){% else %}<code>{{ dataset[""loss""][""fullname""] }}</code>{% endif %}{% if ""config_code"" in dataset[""loss""] %} with these parameters:
{{ dataset[""loss""][""config_code""] }}{% endif %}
{% if dataset_list | length > 3 %}</details>
{% endif %}{% endfor %}{% endif %}{% endfor -%}",reference
## ⚙️ Installation,installation
"{{ testing_factors | default(""[More Information Needed]"", true)}}",other
"In `app.py`, configure paths to:",reference
"{{ more_information | default(""[More Information Needed]"", true)}}",other
## 🔧 Directory Structure,other
<details><summary>Click to expand</summary>,usage
"```bash
pip install -U sentence-transformers
```",installation
"{{ dataset_card_contact | default(""[More Information Needed]"", true)}}",other
"An MVP AI chatbot built in AML-3304 using Bayesian embeddings, a simple transformer block, and DeepSeek-R1 integration — all wired up with a GitHub-driven CI/CD pipeline to Hugging Face Spaces.",other
"## Training Details
{% for dataset_type, dataset_list in [(""training"", train_datasets), (""evaluation"", eval_datasets)] %}{% if dataset_list %}
### {{ dataset_type.title() }} Dataset{{""s"" if dataset_list | length > 1 else """"}}
{% for dataset in dataset_list %}{% if dataset_list | length > 3 %}<details><summary>{{ dataset['name'] or 'Unnamed Dataset' }}</summary>
{% endif %}
#### {{ dataset['name'] or 'Unnamed Dataset' }}
{% if dataset['name'] %}
* Dataset: {% if 'id' in dataset %}[{{ dataset['name'] }}](https://huggingface.co/datasets/{{ dataset['id'] }}){% else %}{{ dataset['name'] }}{% endif %}
{%- if 'revision' in dataset and 'id' in dataset %} at [{{ dataset['revision'][:7] }}](https://huggingface.co/datasets/{{ dataset['id'] }}/tree/{{ dataset['revision'] }}){% endif %}{% endif %}
{% if dataset['size'] %}* Size: {{ ""{:,}"".format(dataset['size']) }} {{ dataset_type }} samples
{% endif %}* Columns: {% if dataset['columns'] | length == 1 %}{{ dataset['columns'][0] }}{% elif dataset['columns'] | length == 2 %}{{ dataset['columns'][0] }} and {{ dataset['columns'][1] }}{% else %}{{ dataset['columns'][:-1] | join(', ') }}, and {{ dataset['columns'][-1] }}{% endif %}
{% if dataset['stats_table'] %}* Approximate statistics based on the first {{ [dataset['size'], 1000] | min }} samples:
{{ dataset['stats_table'] }}{% endif %}{% if dataset['examples_table'] %}* Samples:
{{ dataset['examples_table'] }}{% endif %}* Loss: {% if dataset[""loss""][""fullname""].startswith(""sentence_transformers."") %}[<code>{{ dataset[""loss""][""fullname""].split(""."")[-1] }}</code>](https://sbert.net/docs/package_reference/sentence_transformer/losses.html#{{ dataset[""loss""][""fullname""].split(""."")[-1].lower() }}){% else %}<code>{{ dataset[""loss""][""fullname""] }}</code>{% endif %}{% if ""config_code"" in dataset[""loss""] %} with these parameters:
{{ dataset[""loss""][""config_code""] }}{% endif %}
{% if dataset_list | length > 3 %}</details>
{% endif %}{% endfor %}{% endif %}{% endfor -%}",reference
"1. **Route** via the H2O classifier to select a subset of paragraphs.
     2. **Retrieve** top‑k relevant passages with FAISS.
     3. **Rescore** with a Bayesian posterior to boost high-confidence results.
     4. **Fallback**: if the posterior confidence is below threshold, generate an answer using a compact Flan‑T5 model.",reference
"# Query
reposage query ""What is DeepSeek?"" --threshold 0.3",usage
"* After labeling paragraphs (e.g. `installation`, `usage`, `configuration`, `api_reference`, `troubleshooting`), train an H2O AutoML model to predict the category of any incoming question.
   * This routing step ensures each query is answered by the most relevant section of your docs.",installation
"```bash
pip install -U sentence-transformers
```",installation
"| Team Member | Role(s)                 | Responsibilities                             |
|-------------|-------------------------|----------------------------------------------|
| Rohit       | CEO / Product Lead,     | • Define product vision & roadmap<br>• Manage scope & stakeholder communication<br>• Write technical documentation |
| Kishan      | CTO / DevOps Lead,      | • Develop DeepSeek index & CLI scaffold<br>• Set up CI/CD pipelines<br>• Manage Hugging Face Spaces deployments   |
| Maitri      | Data Scientist / AI Lead| • Implement Bayesian Q&A layer<br>• Design & tune priors<br>• Oversee transformer fallback prompts          |",usage
"### Installation
```bash
git clone https://github.com/RohitRajpoot/reposage.git
cd reposage
pip install -r requirements.txt",installation
"{% for name, value in non_default_hyperparameters.items() %}- `{{ name }}`: {{ value }}
{% endfor %}{%- endif %}
#### All Hyperparameters
<details><summary>Click to expand</summary>",usage
## ⚙️ Installation,installation
"### Installation
```bash
git clone https://github.com/RohitRajpoot/reposage.git
cd reposage
pip install -r requirements.txt",installation
First install the Sentence Transformers library:,installation
"# 2) Install dependencies
pip install --upgrade pip setuptools wheel
pip install -r requirements.txt",installation
Update this directory using maint_tools/vendor_array_api_compat.sh,reference
Copyright (C) 2001-2024 NLTK Project,other
"1. Fork and create a feature branch.
2. Add or update scripts/tests as needed.
3. Ensure style checks (`flake8`, `black`) pass.
4. Submit a pull request detailing your enhancements.",reference
"<!-- If there is a paper or blog post introducing the dataset, the APA and Bibtex information for that should go in this section. -->",reference
First install the Sentence Transformers library:,installation
"1. **Route** via the H2O classifier to select a subset of paragraphs.
     2. **Retrieve** top‑k relevant passages with FAISS.
     3. **Rescore** with a Bayesian posterior to boost high-confidence results.
     4. **Fallback**: if the posterior confidence is below threshold, generate an answer using a compact Flan‑T5 model.",reference
"### Metrics
{% for metrics in eval_metrics %}
#### {{ metrics.description }}
{% if metrics.dataset_name %}
* Dataset{% if metrics.dataset_name is not string and metrics.dataset_name | length > 1 %}s{% endif %}: {% if metrics.dataset_name is string -%}
        `{{ metrics.dataset_name }}`
    {%- else -%}
        {%- for name in metrics.dataset_name -%}
            `{{ name }}`
            {%- if not loop.last -%}
                {%- if loop.index == metrics.dataset_name | length - 1 %} and {% else -%}, {% endif -%}
            {%- endif -%}
        {%- endfor -%}
    {%- endif -%}
{%- endif %}
* Evaluated with {% if metrics.class_name.startswith(""sentence_transformers."") %}[<code>{{ metrics.class_name.split(""."")[-1] }}</code>](https://sbert.net/docs/package_reference/sentence_transformer/evaluation.html#sentence_transformers.evaluation.{{ metrics.class_name.split(""."")[-1] }}){% else %}<code>{{ metrics.class_name }}</code>{% endif %}{% if metrics.config_code %} with these parameters:
{{ metrics.config_code }}{% endif %}",reference
"Do you want to contribute to NLTK development? Great!
Please read [CONTRIBUTING.md](CONTRIBUTING.md) for more details.",other
"```bash
pip install -U sentence-transformers
```",installation
<details><summary>Click to expand</summary>,usage
"### Installation
```bash
git clone https://github.com/RohitRajpoot/reposage.git
cd reposage
pip install -r requirements.txt",installation
"```bash
pip install -U sentence-transformers
```",installation
## Usage,other
<details><summary>Click to expand</summary>,usage
## ⚙️ Installation,installation
First install the Sentence Transformers library:,installation
"### Prerequisites
- Python 3.8+  
- `faiss`, `sentence-transformers`, `transformers`, `openai-whisper`",other
## Contributing,other
<details><summary>Click to see the direct usage in Transformers</summary>,usage
## ⚙️ Installation,installation
#### Personal and Sensitive Information,other
"```bash
pip install -U sentence-transformers
```",installation
"# 2) Install dependencies
pip install --upgrade pip setuptools wheel
pip install -r requirements.txt",installation
3. **DeepSeek + Bayesian + Transformer Q\&A**,other
"## Training Details
{% for dataset_type, dataset_list in [(""training"", train_datasets), (""evaluation"", eval_datasets)] %}{% if dataset_list %}
### {{ dataset_type.title() }} Dataset{{""s"" if dataset_list | length > 1 else """"}}
{% for dataset in dataset_list %}{% if dataset_list | length > 3 %}<details><summary>{{ dataset['name'] or 'Unnamed Dataset' }}</summary>
{% endif %}
#### {{ dataset['name'] or 'Unnamed Dataset' }}
{% if dataset['name'] %}
* Dataset: {% if 'id' in dataset %}[{{ dataset['name'] }}](https://huggingface.co/datasets/{{ dataset['id'] }}){% else %}{{ dataset['name'] }}{% endif %}
{%- if 'revision' in dataset and 'id' in dataset %} at [{{ dataset['revision'][:7] }}](https://huggingface.co/datasets/{{ dataset['id'] }}/tree/{{ dataset['revision'] }}){% endif %}{% endif %}
{% if dataset['size'] %}* Size: {{ ""{:,}"".format(dataset['size']) }} {{ dataset_type }} samples
{% endif %}* Columns: {% if dataset['columns'] | length == 1 %}{{ dataset['columns'][0] }}{% elif dataset['columns'] | length == 2 %}{{ dataset['columns'][0] }} and {{ dataset['columns'][1] }}{% else %}{{ dataset['columns'][:-1] | join(', ') }}, and {{ dataset['columns'][-1] }}{% endif %}
{% if dataset['stats_table'] %}* Approximate statistics based on the first {{ [dataset['size'], 1000] | min }} samples:
{{ dataset['stats_table'] }}{% endif %}{% if dataset['examples_table'] %}* Samples:
{{ dataset['examples_table'] }}{% endif %}* Loss: {% if dataset[""loss""][""fullname""].startswith(""sentence_transformers."") %}[<code>{{ dataset[""loss""][""fullname""].split(""."")[-1] }}</code>](https://sbert.net/docs/package_reference/cross_encoder/losses.html#{{ dataset[""loss""][""fullname""].split(""."")[-1].lower() }}){% else %}<code>{{ dataset[""loss""][""fullname""] }}</code>{% endif %}{% if ""config_code"" in dataset[""loss""] %} with these parameters:
{{ dataset[""loss""][""config_code""] }}{% endif %}
{% if dataset_list | length > 3 %}</details>
{% endif %}{% endfor %}{% endif %}{% endfor -%}",reference
First install the Sentence Transformers library:,installation
Update this directory using maint_tools/vendor_array_api_compat.sh,reference
"{%- if eval_lines %}
### Training Logs
{% if hide_eval_lines %}<details><summary>Click to expand</summary>",usage
"## Bias, Risks, and Limitations",other
"<!-- If there is a paper or blog post introducing the dataset, the APA and Bibtex information for that should go in this section. -->",reference
First install the Sentence Transformers library:,installation
"# Slide 3: Bayesian Q&A Layer
- Cosine-likelihood softmax → posterior",reference
"# 2) Install dependencies
pip install --upgrade pip setuptools wheel
pip install -r requirements.txt",installation
"{%- if eval_lines %}
### Training Logs
{% if hide_eval_lines %}<details><summary>Click to expand</summary>",usage
"Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the ""Software""), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:",other
"# 2) Install dependencies
pip install --upgrade pip setuptools wheel
pip install -r requirements.txt",installation
## Training Details,other
"For documentation, please visit [nltk.org](https://www.nltk.org/).",other
"# Query
reposage query ""What is DeepSeek?"" --threshold 0.3",usage
<details><summary>Click to see the direct usage in Transformers</summary>,usage
First install the Sentence Transformers library:,installation
"### Installation
```bash
git clone https://github.com/RohitRajpoot/reposage.git
cd reposage
pip install -r requirements.txt",installation
"{%- if eval_lines %}
### Training Logs
{% if hide_eval_lines %}<details><summary>Click to expand</summary>",usage
Update this directory using maint_tools/vendor_array_api_extra.sh,reference
"| Team Member | Role(s)                 | Responsibilities                             |
|-------------|-------------------------|----------------------------------------------|
| Rohit       | CEO / Product Lead,     | • Define product vision & roadmap<br>• Manage scope & stakeholder communication<br>• Write technical documentation |
| Kishan      | CTO / DevOps Lead,      | • Develop DeepSeek index & CLI scaffold<br>• Set up CI/CD pipelines<br>• Manage Hugging Face Spaces deployments   |
| Maitri      | Data Scientist / AI Lead| • Implement Bayesian Q&A layer<br>• Design & tune priors<br>• Oversee transformer fallback prompts          |",usage
First install the Sentence Transformers library:,installation
## ⚙️ Installation,installation
### Authors of snowball arabic stemmer algorithm,other
"# Slide 1: Architecture Overview
- User → RepoSage CLI → index/query modules",usage
Update this directory using maint_tools/vendor_array_api_extra.sh,reference
"```bash
pip install -U sentence-transformers
```",installation
"# Query
reposage query ""What is DeepSeek?"" --threshold 0.3",usage
"# Build the index
reposage index",usage
"<!-- This section provides a description of the dataset fields, and additional information about the dataset structure such as criteria used to create the splits, relationships between data points, etc. -->",other
## ⚙️ Installation,installation
"### Installation
```bash
git clone https://github.com/RohitRajpoot/reposage.git
cd reposage
pip install -r requirements.txt",installation
<details><summary>Click to see the direct usage in Transformers</summary>,usage
"## Others whose work we've taken and included in NLTK, but who didn't directly contribute it:",other
"In `app.py`, configure paths to:",reference
"### Metrics
{% for metrics in eval_metrics %}
#### {{ metrics.description }}
{% if metrics.dataset_name %}
* Dataset{% if metrics.dataset_name is not string and metrics.dataset_name | length > 1 %}s{% endif %}: {% if metrics.dataset_name is string -%}
        `{{ metrics.dataset_name }}`
    {%- else -%}
        {%- for name in metrics.dataset_name -%}
            `{{ name }}`
            {%- if not loop.last -%}
                {%- if loop.index == metrics.dataset_name | length - 1 %} and {% else -%}, {% endif -%}
            {%- endif -%}
        {%- endfor -%}
    {%- endif -%}
{%- endif %}
* Evaluated with {% if metrics.class_name.startswith(""sentence_transformers."") %}[<code>{{ metrics.class_name.split(""."")[-1] }}</code>](https://sbert.net/docs/package_reference/cross_encoder/evaluation.html#sentence_transformers.cross_encoder.evaluation.{{ metrics.class_name.split(""."")[-1] }}){% else %}<code>{{ metrics.class_name }}</code>{% endif %}{% if metrics.config_code %} with these parameters:
{{ metrics.config_code }}{% endif %}",reference
<!-- This section describes the people or systems who originally created the data. It should also include self-reported demographic or identity information for the source data creators if this information is available. -->,other
"1. Fork and create a feature branch.
2. Add or update scripts/tests as needed.
3. Ensure style checks (`flake8`, `black`) pass.
4. Submit a pull request detailing your enhancements.",reference
<!-- This should link to a Dataset Card if possible. -->,other
"{{ model_examination | default(""[More Information Needed]"", true)}}",other
"# 2) Install dependencies
pip install --upgrade pip setuptools wheel
pip install -r requirements.txt",installation
"# Query
reposage query ""What is DeepSeek?"" --threshold 0.3",usage
"{%- if co2_eq_emissions %}
### Environmental Impact
Carbon emissions were measured using [CodeCarbon](https://github.com/mlco2/codecarbon).
- **Energy Consumed**: {{ ""%.3f""|format(co2_eq_emissions[""energy_consumed""]) }} kWh
- **Carbon Emitted**: {{ ""%.3f""|format(co2_eq_emissions[""emissions""] / 1000) }} kg of CO2
- **Hours Used**: {{ co2_eq_emissions[""hours_used""] }} hours",other
"```bash
pip install -U sentence-transformers
```",installation
"<!-- This section describes the annotation process such as annotation tools used in the process, the amount of data annotated, annotation guidelines provided to the annotators, interannotator statistics, annotation validation, etc. -->",other
"### Metrics
{% for metrics in eval_metrics %}
#### {{ metrics.description }}
{% if metrics.dataset_name %}
* Dataset{% if metrics.dataset_name is not string and metrics.dataset_name | length > 1 %}s{% endif %}: {% if metrics.dataset_name is string -%}
        `{{ metrics.dataset_name }}`
    {%- else -%}
        {%- for name in metrics.dataset_name -%}
            `{{ name }}`
            {%- if not loop.last -%}
                {%- if loop.index == metrics.dataset_name | length - 1 %} and {% else -%}, {% endif -%}
            {%- endif -%}
        {%- endfor -%}
    {%- endif -%}
{%- endif %}
* Evaluated with {% if metrics.class_name.startswith(""sentence_transformers."") %}[<code>{{ metrics.class_name.split(""."")[-1] }}</code>](https://sbert.net/docs/package_reference/sentence_transformer/evaluation.html#sentence_transformers.evaluation.{{ metrics.class_name.split(""."")[-1] }}){% else %}<code>{{ metrics.class_name }}</code>{% endif %}{% if metrics.config_code %} with these parameters:
{{ metrics.config_code }}{% endif %}",reference
"{{ direct_use | default(""[More Information Needed]"", true)}}",other
"* After labeling paragraphs (e.g. `installation`, `usage`, `configuration`, `api_reference`, `troubleshooting`), train an H2O AutoML model to predict the category of any incoming question.
   * This routing step ensures each query is answered by the most relevant section of your docs.",installation
"# 2) Install dependencies
pip install --upgrade pip setuptools wheel
pip install -r requirements.txt",installation
## ⚙️ Installation,installation
"# Query
reposage query ""What is DeepSeek?"" --threshold 0.3",usage
"1. **Route** via the H2O classifier to select a subset of paragraphs.
     2. **Retrieve** top‑k relevant passages with FAISS.
     3. **Rescore** with a Bayesian posterior to boost high-confidence results.
     4. **Fallback**: if the posterior confidence is below threshold, generate an answer using a compact Flan‑T5 model.",reference
**APA:**,other
<details><summary>Click to see the direct usage in Transformers</summary>,usage
# RepoSage: Hybrid AI-Centric Q\&A Pipeline,other
"</details>
-->",other
"# Query
reposage query ""What is DeepSeek?"" --threshold 0.3",usage
"# 2) Install dependencies
pip install --upgrade pip setuptools wheel
pip install -r requirements.txt",installation
"# Slide 3: Bayesian Q&A Layer
- Cosine-likelihood softmax → posterior",reference
"# 2) Install dependencies
pip install --upgrade pip setuptools wheel
pip install -r requirements.txt",installation
First install the Sentence Transformers library:,installation
<details><summary>Click to expand</summary>,usage
First install the Sentence Transformers library:,installation
"```bash
pip install -U sentence-transformers
```",installation
<details><summary>Click to see the direct usage in Transformers</summary>,usage
First install the Sentence Transformers library:,installation
"```bash
pip install -U sentence-transformers
```",installation
First install the Sentence Transformers library:,installation
## Model Card Authors [optional],other
<details><summary>Click to expand</summary>,usage
"# Slide 1: Architecture Overview
- User → RepoSage CLI → index/query modules",usage
"<!-- This section describes the source data (e.g. news text and headlines, social media posts, translated sentences, ...). -->",reference
<details><summary>Click to expand</summary>,usage
<details><summary>Click to see the direct usage in Transformers</summary>,usage
"```bash
pip install -U sentence-transformers
```",installation
"### Installation
```bash
git clone https://github.com/RohitRajpoot/reposage.git
cd reposage
pip install -r requirements.txt",installation
"# 2) Install dependencies
pip install --upgrade pip setuptools wheel
pip install -r requirements.txt",installation
## ⚙️ Installation,installation
"<!-- If there is a paper or blog post introducing the dataset, the APA and Bibtex information for that should go in this section. -->",reference
"# Slide 1: Architecture Overview
- User → RepoSage CLI → index/query modules",usage
"1. **Route** via the H2O classifier to select a subset of paragraphs.
     2. **Retrieve** top‑k relevant passages with FAISS.
     3. **Rescore** with a Bayesian posterior to boost high-confidence results.
     4. **Fallback**: if the posterior confidence is below threshold, generate an answer using a compact Flan‑T5 model.",reference
"{% for name, value in non_default_hyperparameters.items() %}- `{{ name }}`: {{ value }}
{% endfor %}{%- endif %}
#### All Hyperparameters
<details><summary>Click to expand</summary>",usage
"{% for name, value in non_default_hyperparameters.items() %}- `{{ name }}`: {{ value }}
{% endfor %}{%- endif %}
#### All Hyperparameters
<details><summary>Click to expand</summary>",usage
"# 3) Smoke-test CLI
python -m assist.main chat ""hello world""",usage
## ⚙️ Installation,installation
"* After labeling paragraphs (e.g. `installation`, `usage`, `configuration`, `api_reference`, `troubleshooting`), train an H2O AutoML model to predict the category of any incoming question.
   * This routing step ensures each query is answered by the most relevant section of your docs.",installation
## ⚙️ Installation,installation
"# Query
reposage query ""What is DeepSeek?"" --threshold 0.3",usage
"# Or call the REST API
curl -X POST https://<your-space>.hf.space/query \
     -d '{""question"":""What is DeepSeek?""}'",reference
"# Slide 1: Architecture Overview
- User → RepoSage CLI → index/query modules",usage
"```bash
pip install -U sentence-transformers
```",installation
"1. Fork and create a feature branch.
2. Add or update scripts/tests as needed.
3. Ensure style checks (`flake8`, `black`) pass.
4. Submit a pull request detailing your enhancements.",reference
"* After labeling paragraphs (e.g. `installation`, `usage`, `configuration`, `api_reference`, `troubleshooting`), train an H2O AutoML model to predict the category of any incoming question.
   * This routing step ensures each query is answered by the most relevant section of your docs.",installation
"# Slide 1: Architecture Overview
- User → RepoSage CLI → index/query modules",usage
"*Provides a way for people who have updates to the Model Card, suggestions, or questions, to contact the Model Card authors.*
-->",other
"<!-- If there is a paper or blog post introducing the model, the APA and Bibtex information for that should go in this section. -->",reference
"{% for name, value in non_default_hyperparameters.items() %}- `{{ name }}`: {{ value }}
{% endfor %}{%- endif %}
#### All Hyperparameters
<details><summary>Click to expand</summary>",usage
"```bash
pip install -U sentence-transformers
```",installation
"<!-- If there is a paper or blog post introducing the model, the APA and Bibtex information for that should go in this section. -->",reference
"1. Fork and create a feature branch.
2. Add or update scripts/tests as needed.
3. Ensure style checks (`flake8`, `black`) pass.
4. Submit a pull request detailing your enhancements.",reference
"### Metrics
{% for metrics in eval_metrics %}
#### {{ metrics.description }}
{% if metrics.dataset_name %}
* Dataset{% if metrics.dataset_name is not string and metrics.dataset_name | length > 1 %}s{% endif %}: {% if metrics.dataset_name is string -%}
        `{{ metrics.dataset_name }}`
    {%- else -%}
        {%- for name in metrics.dataset_name -%}
            `{{ name }}`
            {%- if not loop.last -%}
                {%- if loop.index == metrics.dataset_name | length - 1 %} and {% else -%}, {% endif -%}
            {%- endif -%}
        {%- endfor -%}
    {%- endif -%}
{%- endif %}
* Evaluated with {% if metrics.class_name.startswith(""sentence_transformers."") %}[<code>{{ metrics.class_name.split(""."")[-1] }}</code>](https://sbert.net/docs/package_reference/sentence_transformer/evaluation.html#sentence_transformers.evaluation.{{ metrics.class_name.split(""."")[-1] }}){% else %}<code>{{ metrics.class_name }}</code>{% endif %}{% if metrics.config_code %} with these parameters:
{{ metrics.config_code }}{% endif %}",reference
"# Or call the REST API
curl -X POST https://<your-space>.hf.space/query \
     -d '{""question"":""What is DeepSeek?""}'",reference
"{{ who_are_annotators_section | default(""[More Information Needed]"", true)}}",other
<details><summary>Click to see the direct usage in Transformers</summary>,usage
"<!-- This section describes the source data (e.g. news text and headlines, social media posts, translated sentences, ...). -->",reference
"<!-- This section describes the source data (e.g. news text and headlines, social media posts, translated sentences, ...). -->",reference
<details><summary>Click to see the direct usage in Transformers</summary>,usage
"1. **Route** via the H2O classifier to select a subset of paragraphs.
     2. **Retrieve** top‑k relevant passages with FAISS.
     3. **Rescore** with a Bayesian posterior to boost high-confidence results.
     4. **Fallback**: if the posterior confidence is below threshold, generate an answer using a compact Flan‑T5 model.",reference
"### Installation
```bash
git clone https://github.com/RohitRajpoot/reposage.git
cd reposage
pip install -r requirements.txt",installation
"### Installation
```bash
git clone https://github.com/RohitRajpoot/reposage.git
cd reposage
pip install -r requirements.txt",installation
"# Or call the REST API
curl -X POST https://<your-space>.hf.space/query \
     -d '{""question"":""What is DeepSeek?""}'",reference
"{%- if eval_lines %}
### Training Logs
{% if hide_eval_lines %}<details><summary>Click to expand</summary>",usage
"{%- if eval_lines %}
### Training Logs
{% if hide_eval_lines %}<details><summary>Click to expand</summary>",usage
"<!-- If there is a paper or blog post introducing the model, the APA and Bibtex information for that should go in this section. -->",reference
"<!-- If there is a paper or blog post introducing the dataset, the APA and Bibtex information for that should go in this section. -->",reference
"# Query
reposage query ""What is DeepSeek?"" --threshold 0.3",usage
"This is a [Cross Encoder](https://www.sbert.net/docs/cross_encoder/usage/usage.html) model{% if base_model %} finetuned from [{{ base_model }}](https://huggingface.co/{{ base_model }}){% else %} trained{% endif %}{% if train_datasets | selectattr(""name"") | list %} on {% if train_datasets | selectattr(""name"") | map(attribute=""name"") | join("", "") | length > 200 %}{{ train_datasets | length }}{% else %}the {% for dataset in (train_datasets | selectattr(""name"")) %}{% if dataset.id %}[{{ dataset.name if dataset.name else dataset.id }}](https://huggingface.co/datasets/{{ dataset.id }}){% else %}{{ dataset.name }}{% endif %}{% if not loop.last %}{% if loop.index == (train_datasets | selectattr(""name"") | list | length - 1) %} and {% else %}, {% endif %}{% endif %}{% endfor %}{% endif %} dataset{{""s"" if train_datasets | selectattr(""name"") | list | length > 1 else """"}}{% endif %} using the [sentence-transformers](https://www.SBERT.net) library. It computes scores for pairs of texts, which can be used for {{ task_name }}.",other
"## Training Details
{% for dataset_type, dataset_list in [(""training"", train_datasets), (""evaluation"", eval_datasets)] %}{% if dataset_list %}
### {{ dataset_type.title() }} Dataset{{""s"" if dataset_list | length > 1 else """"}}
{% for dataset in dataset_list %}{% if dataset_list | length > 3 %}<details><summary>{{ dataset['name'] or 'Unnamed Dataset' }}</summary>
{% endif %}
#### {{ dataset['name'] or 'Unnamed Dataset' }}
{% if dataset['name'] %}
* Dataset: {% if 'id' in dataset %}[{{ dataset['name'] }}](https://huggingface.co/datasets/{{ dataset['id'] }}){% else %}{{ dataset['name'] }}{% endif %}
{%- if 'revision' in dataset and 'id' in dataset %} at [{{ dataset['revision'][:7] }}](https://huggingface.co/datasets/{{ dataset['id'] }}/tree/{{ dataset['revision'] }}){% endif %}{% endif %}
{% if dataset['size'] %}* Size: {{ ""{:,}"".format(dataset['size']) }} {{ dataset_type }} samples
{% endif %}* Columns: {% if dataset['columns'] | length == 1 %}{{ dataset['columns'][0] }}{% elif dataset['columns'] | length == 2 %}{{ dataset['columns'][0] }} and {{ dataset['columns'][1] }}{% else %}{{ dataset['columns'][:-1] | join(', ') }}, and {{ dataset['columns'][-1] }}{% endif %}
{% if dataset['stats_table'] %}* Approximate statistics based on the first {{ [dataset['size'], 1000] | min }} samples:
{{ dataset['stats_table'] }}{% endif %}{% if dataset['examples_table'] %}* Samples:
{{ dataset['examples_table'] }}{% endif %}* Loss: {% if dataset[""loss""][""fullname""].startswith(""sentence_transformers."") %}[<code>{{ dataset[""loss""][""fullname""].split(""."")[-1] }}</code>](https://sbert.net/docs/package_reference/cross_encoder/losses.html#{{ dataset[""loss""][""fullname""].split(""."")[-1].lower() }}){% else %}<code>{{ dataset[""loss""][""fullname""] }}</code>{% endif %}{% if ""config_code"" in dataset[""loss""] %} with these parameters:
{{ dataset[""loss""][""config_code""] }}{% endif %}
{% if dataset_list | length > 3 %}</details>
{% endif %}{% endfor %}{% endif %}{% endfor -%}",reference
<details><summary>Click to see the direct usage in Transformers</summary>,usage
<details><summary>Click to see the direct usage in Transformers</summary>,usage
"<!-- This section describes the source data (e.g. news text and headlines, social media posts, translated sentences, ...). -->",reference
Update this directory using maint_tools/vendor_array_api_compat.sh,reference
"# Slide 3: Bayesian Q&A Layer
- Cosine-likelihood softmax → posterior",reference
"<!-- This section describes the source data (e.g. news text and headlines, social media posts, translated sentences, ...). -->",reference
"<!-- If the dataset contains annotations which are not part of the initial data collection, use this section to describe them. -->",other
"1. Redistributions of source code must retain the above copyright
   notice, this list of conditions and the following disclaimer.",other
"### Metrics
{% for metrics in eval_metrics %}
#### {{ metrics.description }}
{% if metrics.dataset_name %}
* Dataset{% if metrics.dataset_name is not string and metrics.dataset_name | length > 1 %}s{% endif %}: {% if metrics.dataset_name is string -%}
        `{{ metrics.dataset_name }}`
    {%- else -%}
        {%- for name in metrics.dataset_name -%}
            `{{ name }}`
            {%- if not loop.last -%}
                {%- if loop.index == metrics.dataset_name | length - 1 %} and {% else -%}, {% endif -%}
            {%- endif -%}
        {%- endfor -%}
    {%- endif -%}
{%- endif %}
* Evaluated with {% if metrics.class_name.startswith(""sentence_transformers."") %}[<code>{{ metrics.class_name.split(""."")[-1] }}</code>](https://sbert.net/docs/package_reference/cross_encoder/evaluation.html#sentence_transformers.cross_encoder.evaluation.{{ metrics.class_name.split(""."")[-1] }}){% else %}<code>{{ metrics.class_name }}</code>{% endif %}{% if metrics.config_code %} with these parameters:
{{ metrics.config_code }}{% endif %}",reference
"Try it out live:  
👉 https://huggingface.co/spaces/rohitrajpoot/reposage-chatbot",other
"In `app.py`, configure paths to:",reference
"---
# For reference on model card metadata, see the spec: https://github.com/huggingface/hub-docs/blob/main/modelcard.md?plain=1
# Doc / guide: https://huggingface.co/docs/hub/model-cards
{{ card_data }}
---",other
"{%- if eval_lines %}
### Training Logs
{% if hide_eval_lines %}<details><summary>Click to expand</summary>",usage
"{{ results | default(""[More Information Needed]"", true)}}",other
<details><summary>Click to see the direct usage in Transformers</summary>,usage
<details><summary>Click to see the direct usage in Transformers</summary>,usage
"# Slide 1: Architecture Overview
- User → RepoSage CLI → index/query modules",usage
First install the Sentence Transformers library:,installation
"# 2) Install dependencies
pip install --upgrade pip setuptools wheel
pip install -r requirements.txt",installation
<details><summary>Click to see the direct usage in Transformers</summary>,usage
## ⚙️ Installation,installation
"{%- if eval_lines %}
### Training Logs
{% if hide_eval_lines %}<details><summary>Click to expand</summary>",usage
### Model Sources,other
### 1. Prepare the Corpus,other
"```bash
pip install -U sentence-transformers
```",installation
"# Slide 3: Bayesian Q&A Layer
- Cosine-likelihood softmax → posterior",reference
<details><summary>Click to see the direct usage in Transformers</summary>,usage
<details><summary>Click to see the direct usage in Transformers</summary>,usage
"{{ dataset_structure | default(""[More Information Needed]"", true)}}",other
Update this directory using maint_tools/vendor_array_api_extra.sh,reference
## Citation [optional],other
"# Slide 3: Bayesian Q&A Layer
- Cosine-likelihood softmax → posterior",reference
"# 2) Install dependencies
pip install --upgrade pip setuptools wheel
pip install -r requirements.txt",installation
"### Metrics
{% for metrics in eval_metrics %}
#### {{ metrics.description }}
{% if metrics.dataset_name %}
* Dataset{% if metrics.dataset_name is not string and metrics.dataset_name | length > 1 %}s{% endif %}: {% if metrics.dataset_name is string -%}
        `{{ metrics.dataset_name }}`
    {%- else -%}
        {%- for name in metrics.dataset_name -%}
            `{{ name }}`
            {%- if not loop.last -%}
                {%- if loop.index == metrics.dataset_name | length - 1 %} and {% else -%}, {% endif -%}
            {%- endif -%}
        {%- endfor -%}
    {%- endif -%}
{%- endif %}
* Evaluated with {% if metrics.class_name.startswith(""sentence_transformers."") %}[<code>{{ metrics.class_name.split(""."")[-1] }}</code>](https://sbert.net/docs/package_reference/cross_encoder/evaluation.html#sentence_transformers.cross_encoder.evaluation.{{ metrics.class_name.split(""."")[-1] }}){% else %}<code>{{ metrics.class_name }}</code>{% endif %}{% if metrics.config_code %} with these parameters:
{{ metrics.config_code }}{% endif %}",reference
"Many parts of this module have been derived from original sources, 
often the algorithm's designer. Component licenses are located with 
the component code.",other
"# Or call the REST API
curl -X POST https://<your-space>.hf.space/query \
     -d '{""question"":""What is DeepSeek?""}'",reference
"<!-- Total emissions (in grams of CO2eq) and additional considerations, such as electricity usage, go here. Edit the suggested text below accordingly -->",other
"# Or call the REST API
curl -X POST https://<your-space>.hf.space/query \
     -d '{""question"":""What is DeepSeek?""}'",reference
First install the Sentence Transformers library:,installation
<details><summary>Click to see the direct usage in Transformers</summary>,usage
"### Installation
```bash
git clone https://github.com/RohitRajpoot/reposage.git
cd reposage
pip install -r requirements.txt",installation
"## Training Details
{% for dataset_type, dataset_list in [(""training"", train_datasets), (""evaluation"", eval_datasets)] %}{% if dataset_list %}
### {{ dataset_type.title() }} Dataset{{""s"" if dataset_list | length > 1 else """"}}
{% for dataset in dataset_list %}{% if dataset_list | length > 3 %}<details><summary>{{ dataset['name'] or 'Unnamed Dataset' }}</summary>
{% endif %}
#### {{ dataset['name'] or 'Unnamed Dataset' }}
{% if dataset['name'] %}
* Dataset: {% if 'id' in dataset %}[{{ dataset['name'] }}](https://huggingface.co/datasets/{{ dataset['id'] }}){% else %}{{ dataset['name'] }}{% endif %}
{%- if 'revision' in dataset and 'id' in dataset %} at [{{ dataset['revision'][:7] }}](https://huggingface.co/datasets/{{ dataset['id'] }}/tree/{{ dataset['revision'] }}){% endif %}{% endif %}
{% if dataset['size'] %}* Size: {{ ""{:,}"".format(dataset['size']) }} {{ dataset_type }} samples
{% endif %}* Columns: {% if dataset['columns'] | length == 1 %}{{ dataset['columns'][0] }}{% elif dataset['columns'] | length == 2 %}{{ dataset['columns'][0] }} and {{ dataset['columns'][1] }}{% else %}{{ dataset['columns'][:-1] | join(', ') }}, and {{ dataset['columns'][-1] }}{% endif %}
{% if dataset['stats_table'] %}* Approximate statistics based on the first {{ [dataset['size'], 1000] | min }} samples:
{{ dataset['stats_table'] }}{% endif %}{% if dataset['examples_table'] %}* Samples:
{{ dataset['examples_table'] }}{% endif %}* Loss: {% if dataset[""loss""][""fullname""].startswith(""sentence_transformers."") %}[<code>{{ dataset[""loss""][""fullname""].split(""."")[-1] }}</code>](https://sbert.net/docs/package_reference/sentence_transformer/losses.html#{{ dataset[""loss""][""fullname""].split(""."")[-1].lower() }}){% else %}<code>{{ dataset[""loss""][""fullname""] }}</code>{% endif %}{% if ""config_code"" in dataset[""loss""] %} with these parameters:
{{ dataset[""loss""][""config_code""] }}{% endif %}
{% if dataset_list | length > 3 %}</details>
{% endif %}{% endfor %}{% endif %}{% endfor -%}",reference
"{{ preprocessing | default(""[More Information Needed]"", true)}}",other
"### Metrics
{% for metrics in eval_metrics %}
#### {{ metrics.description }}
{% if metrics.dataset_name %}
* Dataset{% if metrics.dataset_name is not string and metrics.dataset_name | length > 1 %}s{% endif %}: {% if metrics.dataset_name is string -%}
        `{{ metrics.dataset_name }}`
    {%- else -%}
        {%- for name in metrics.dataset_name -%}
            `{{ name }}`
            {%- if not loop.last -%}
                {%- if loop.index == metrics.dataset_name | length - 1 %} and {% else -%}, {% endif -%}
            {%- endif -%}
        {%- endfor -%}
    {%- endif -%}
{%- endif %}
* Evaluated with {% if metrics.class_name.startswith(""sentence_transformers."") %}[<code>{{ metrics.class_name.split(""."")[-1] }}</code>](https://sbert.net/docs/package_reference/cross_encoder/evaluation.html#sentence_transformers.cross_encoder.evaluation.{{ metrics.class_name.split(""."")[-1] }}){% else %}<code>{{ metrics.class_name }}</code>{% endif %}{% if metrics.config_code %} with these parameters:
{{ metrics.config_code }}{% endif %}",reference
#### Annotation process,other
"For license information, see [LICENSE.txt](LICENSE.txt).",other
"# Slide 3: Bayesian Q&A Layer
- Cosine-likelihood softmax → posterior",reference
"# Build the index
reposage index",usage
"# Slide 1: Architecture Overview
- User → RepoSage CLI → index/query modules",usage
First install the Sentence Transformers library:,installation
"{%- if eval_lines %}
### Training Logs
{% if hide_eval_lines %}<details><summary>Click to expand</summary>",usage
"1. **Route** via the H2O classifier to select a subset of paragraphs.
     2. **Retrieve** top‑k relevant passages with FAISS.
     3. **Rescore** with a Bayesian posterior to boost high-confidence results.
     4. **Fallback**: if the posterior confidence is below threshold, generate an answer using a compact Flan‑T5 model.",reference
"## Training Details
{% for dataset_type, dataset_list in [(""training"", train_datasets), (""evaluation"", eval_datasets)] %}{% if dataset_list %}
### {{ dataset_type.title() }} Dataset{{""s"" if dataset_list | length > 1 else """"}}
{% for dataset in dataset_list %}{% if dataset_list | length > 3 %}<details><summary>{{ dataset['name'] or 'Unnamed Dataset' }}</summary>
{% endif %}
#### {{ dataset['name'] or 'Unnamed Dataset' }}
{% if dataset['name'] %}
* Dataset: {% if 'id' in dataset %}[{{ dataset['name'] }}](https://huggingface.co/datasets/{{ dataset['id'] }}){% else %}{{ dataset['name'] }}{% endif %}
{%- if 'revision' in dataset and 'id' in dataset %} at [{{ dataset['revision'][:7] }}](https://huggingface.co/datasets/{{ dataset['id'] }}/tree/{{ dataset['revision'] }}){% endif %}{% endif %}
{% if dataset['size'] %}* Size: {{ ""{:,}"".format(dataset['size']) }} {{ dataset_type }} samples
{% endif %}* Columns: {% if dataset['columns'] | length == 1 %}{{ dataset['columns'][0] }}{% elif dataset['columns'] | length == 2 %}{{ dataset['columns'][0] }} and {{ dataset['columns'][1] }}{% else %}{{ dataset['columns'][:-1] | join(', ') }}, and {{ dataset['columns'][-1] }}{% endif %}
{% if dataset['stats_table'] %}* Approximate statistics based on the first {{ [dataset['size'], 1000] | min }} samples:
{{ dataset['stats_table'] }}{% endif %}{% if dataset['examples_table'] %}* Samples:
{{ dataset['examples_table'] }}{% endif %}* Loss: {% if dataset[""loss""][""fullname""].startswith(""sentence_transformers."") %}[<code>{{ dataset[""loss""][""fullname""].split(""."")[-1] }}</code>](https://sbert.net/docs/package_reference/sentence_transformer/losses.html#{{ dataset[""loss""][""fullname""].split(""."")[-1].lower() }}){% else %}<code>{{ dataset[""loss""][""fullname""] }}</code>{% endif %}{% if ""config_code"" in dataset[""loss""] %} with these parameters:
{{ dataset[""loss""][""config_code""] }}{% endif %}
{% if dataset_list | length > 3 %}</details>
{% endif %}{% endfor %}{% endif %}{% endfor -%}",reference
## ⚙️ Installation,installation
<!-- This section describes the people or systems who created the annotations. -->,other
## 📜 License,other
"1. Fork and create a feature branch.
2. Add or update scripts/tests as needed.
3. Ensure style checks (`flake8`, `black`) pass.
4. Submit a pull request detailing your enhancements.",reference
"{%- if eval_lines %}
### Training Logs
{% if hide_eval_lines %}<details><summary>Click to expand</summary>",usage
"```bash
pip install -U sentence-transformers
```",installation
"*Clearly define terms in order to be accessible across audiences.*
-->",other
"### Installation
```bash
git clone https://github.com/RohitRajpoot/reposage.git
cd reposage
pip install -r requirements.txt",installation
"### Metrics
{% for metrics in eval_metrics %}
#### {{ metrics.description }}
{% if metrics.dataset_name %}
* Dataset{% if metrics.dataset_name is not string and metrics.dataset_name | length > 1 %}s{% endif %}: {% if metrics.dataset_name is string -%}
        `{{ metrics.dataset_name }}`
    {%- else -%}
        {%- for name in metrics.dataset_name -%}
            `{{ name }}`
            {%- if not loop.last -%}
                {%- if loop.index == metrics.dataset_name | length - 1 %} and {% else -%}, {% endif -%}
            {%- endif -%}
        {%- endfor -%}
    {%- endif -%}
{%- endif %}
* Evaluated with {% if metrics.class_name.startswith(""sentence_transformers."") %}[<code>{{ metrics.class_name.split(""."")[-1] }}</code>](https://sbert.net/docs/package_reference/cross_encoder/evaluation.html#sentence_transformers.cross_encoder.evaluation.{{ metrics.class_name.split(""."")[-1] }}){% else %}<code>{{ metrics.class_name }}</code>{% endif %}{% if metrics.config_code %} with these parameters:
{{ metrics.config_code }}{% endif %}",reference
"| Team Member | Role(s)                 | Responsibilities                             |
|-------------|-------------------------|----------------------------------------------|
| Rohit       | CEO / Product Lead,     | • Define product vision & roadmap<br>• Manage scope & stakeholder communication<br>• Write technical documentation |
| Kishan      | CTO / DevOps Lead,      | • Develop DeepSeek index & CLI scaffold<br>• Set up CI/CD pipelines<br>• Manage Hugging Face Spaces deployments   |
| Maitri      | Data Scientist / AI Lead| • Implement Bayesian Q&A layer<br>• Design & tune priors<br>• Oversee transformer fallback prompts          |",usage
First install the Sentence Transformers library:,installation
Update this directory using maint_tools/vendor_array_api_compat.sh,reference
"- **Repository:** {{ repo | default(""[More Information Needed]"", true)}}
- **Paper [optional]:** {{ paper | default(""[More Information Needed]"", true)}}
- **Demo [optional]:** {{ demo | default(""[More Information Needed]"", true)}}",other
## 🚀 Getting Started,other
## More Information [optional],other
<details><summary>Click to expand</summary>,usage
"{%- if eval_lines %}
### Training Logs
{% if hide_eval_lines %}<details><summary>Click to expand</summary>",usage
First install the Sentence Transformers library:,installation
"# Dataset Card for {{ pretty_name | default(""Dataset Name"", true) }}",other
"<!-- If there is a paper or blog post introducing the dataset, the APA and Bibtex information for that should go in this section. -->",reference
"```bash
python scripts/build_index.py \
  --data data/classification.csv \
  --index-out models/faiss_index.bin
```",other
"# Slide 1: Architecture Overview
- User → RepoSage CLI → index/query modules",usage
"* After labeling paragraphs (e.g. `installation`, `usage`, `configuration`, `api_reference`, `troubleshooting`), train an H2O AutoML model to predict the category of any incoming question.
   * This routing step ensures each query is answered by the most relevant section of your docs.",installation
Update this directory using maint_tools/vendor_array_api_compat.sh,reference
First install the Sentence Transformers library:,installation
"{%- if eval_lines %}
### Training Logs
{% if hide_eval_lines %}<details><summary>Click to expand</summary>",usage
First install the Sentence Transformers library:,installation
"## Features
- **DeepSeek Index** of lecture slides, code, and readings  
- **Bayesian Posterior Scoring** for precise answer selection  
- **Transformer Fallback** via HF text2text when confidence is low  
- **CLI & REST API** for local and web deployment  
- **CI/CD** via GitHub Actions → Hugging Face Spaces",usage
"### Metrics
{% for metrics in eval_metrics %}
#### {{ metrics.description }}
{% if metrics.dataset_name %}
* Dataset{% if metrics.dataset_name is not string and metrics.dataset_name | length > 1 %}s{% endif %}: {% if metrics.dataset_name is string -%}
        `{{ metrics.dataset_name }}`
    {%- else -%}
        {%- for name in metrics.dataset_name -%}
            `{{ name }}`
            {%- if not loop.last -%}
                {%- if loop.index == metrics.dataset_name | length - 1 %} and {% else -%}, {% endif -%}
            {%- endif -%}
        {%- endfor -%}
    {%- endif -%}
{%- endif %}
* Evaluated with {% if metrics.class_name.startswith(""sentence_transformers."") %}[<code>{{ metrics.class_name.split(""."")[-1] }}</code>](https://sbert.net/docs/package_reference/cross_encoder/evaluation.html#sentence_transformers.cross_encoder.evaluation.{{ metrics.class_name.split(""."")[-1] }}){% else %}<code>{{ metrics.class_name }}</code>{% endif %}{% if metrics.config_code %} with these parameters:
{{ metrics.config_code }}{% endif %}",reference
### Curation Rationale,other
"# Slide 3: Bayesian Q&A Layer
- Cosine-likelihood softmax → posterior",reference
"# Query
reposage query ""What is DeepSeek?"" --threshold 0.3",usage
"# 3) Smoke-test CLI
python -m assist.main chat ""hello world""",usage
First install the Sentence Transformers library:,installation
"{%- if eval_lines %}
### Training Logs
{% if hide_eval_lines %}<details><summary>Click to expand</summary>",usage
"## Training Details
{% for dataset_type, dataset_list in [(""training"", train_datasets), (""evaluation"", eval_datasets)] %}{% if dataset_list %}
### {{ dataset_type.title() }} Dataset{{""s"" if dataset_list | length > 1 else """"}}
{% for dataset in dataset_list %}{% if dataset_list | length > 3 %}<details><summary>{{ dataset['name'] or 'Unnamed Dataset' }}</summary>
{% endif %}
#### {{ dataset['name'] or 'Unnamed Dataset' }}
{% if dataset['name'] %}
* Dataset: {% if 'id' in dataset %}[{{ dataset['name'] }}](https://huggingface.co/datasets/{{ dataset['id'] }}){% else %}{{ dataset['name'] }}{% endif %}
{%- if 'revision' in dataset and 'id' in dataset %} at [{{ dataset['revision'][:7] }}](https://huggingface.co/datasets/{{ dataset['id'] }}/tree/{{ dataset['revision'] }}){% endif %}{% endif %}
{% if dataset['size'] %}* Size: {{ ""{:,}"".format(dataset['size']) }} {{ dataset_type }} samples
{% endif %}* Columns: {% if dataset['columns'] | length == 1 %}{{ dataset['columns'][0] }}{% elif dataset['columns'] | length == 2 %}{{ dataset['columns'][0] }} and {{ dataset['columns'][1] }}{% else %}{{ dataset['columns'][:-1] | join(', ') }}, and {{ dataset['columns'][-1] }}{% endif %}
{% if dataset['stats_table'] %}* Approximate statistics based on the first {{ [dataset['size'], 1000] | min }} samples:
{{ dataset['stats_table'] }}{% endif %}{% if dataset['examples_table'] %}* Samples:
{{ dataset['examples_table'] }}{% endif %}* Loss: {% if dataset[""loss""][""fullname""].startswith(""sentence_transformers."") %}[<code>{{ dataset[""loss""][""fullname""].split(""."")[-1] }}</code>](https://sbert.net/docs/package_reference/cross_encoder/losses.html#{{ dataset[""loss""][""fullname""].split(""."")[-1].lower() }}){% else %}<code>{{ dataset[""loss""][""fullname""] }}</code>{% endif %}{% if ""config_code"" in dataset[""loss""] %} with these parameters:
{{ dataset[""loss""][""config_code""] }}{% endif %}
{% if dataset_list | length > 3 %}</details>
{% endif %}{% endfor %}{% endif %}{% endfor -%}",reference
"## Training Details
{% for dataset_type, dataset_list in [(""training"", train_datasets), (""evaluation"", eval_datasets)] %}{% if dataset_list %}
### {{ dataset_type.title() }} Dataset{{""s"" if dataset_list | length > 1 else """"}}
{% for dataset in dataset_list %}{% if dataset_list | length > 3 %}<details><summary>{{ dataset['name'] or 'Unnamed Dataset' }}</summary>
{% endif %}
#### {{ dataset['name'] or 'Unnamed Dataset' }}
{% if dataset['name'] %}
* Dataset: {% if 'id' in dataset %}[{{ dataset['name'] }}](https://huggingface.co/datasets/{{ dataset['id'] }}){% else %}{{ dataset['name'] }}{% endif %}
{%- if 'revision' in dataset and 'id' in dataset %} at [{{ dataset['revision'][:7] }}](https://huggingface.co/datasets/{{ dataset['id'] }}/tree/{{ dataset['revision'] }}){% endif %}{% endif %}
{% if dataset['size'] %}* Size: {{ ""{:,}"".format(dataset['size']) }} {{ dataset_type }} samples
{% endif %}* Columns: {% if dataset['columns'] | length == 1 %}{{ dataset['columns'][0] }}{% elif dataset['columns'] | length == 2 %}{{ dataset['columns'][0] }} and {{ dataset['columns'][1] }}{% else %}{{ dataset['columns'][:-1] | join(', ') }}, and {{ dataset['columns'][-1] }}{% endif %}
{% if dataset['stats_table'] %}* Approximate statistics based on the first {{ [dataset['size'], 1000] | min }} samples:
{{ dataset['stats_table'] }}{% endif %}{% if dataset['examples_table'] %}* Samples:
{{ dataset['examples_table'] }}{% endif %}* Loss: {% if dataset[""loss""][""fullname""].startswith(""sentence_transformers."") %}[<code>{{ dataset[""loss""][""fullname""].split(""."")[-1] }}</code>](https://sbert.net/docs/package_reference/cross_encoder/losses.html#{{ dataset[""loss""][""fullname""].split(""."")[-1].lower() }}){% else %}<code>{{ dataset[""loss""][""fullname""] }}</code>{% endif %}{% if ""config_code"" in dataset[""loss""] %} with these parameters:
{{ dataset[""loss""][""config_code""] }}{% endif %}
{% if dataset_list | length > 3 %}</details>
{% endif %}{% endfor %}{% endif %}{% endfor -%}",reference
"{{ out_of_scope_use | default(""[More Information Needed]"", true)}}",other
### 4. Hook into Streamlit App,other
"# 2) Install dependencies
pip install --upgrade pip setuptools wheel
pip install -r requirements.txt",installation
"# Query
reposage query ""What is DeepSeek?"" --threshold 0.3",usage
## ⚙️ Installation,installation
<!-- This section describes the evaluation protocols and provides the results. -->,other
"<!-- This section describes the source data (e.g. news text and headlines, social media posts, translated sentences, ...). -->",reference
First install the Sentence Transformers library:,installation
"| Team Member | Role(s)                 | Responsibilities                             |
|-------------|-------------------------|----------------------------------------------|
| Rohit       | CEO / Product Lead,     | • Define product vision & roadmap<br>• Manage scope & stakeholder communication<br>• Write technical documentation |
| Kishan      | CTO / DevOps Lead,      | • Develop DeepSeek index & CLI scaffold<br>• Set up CI/CD pipelines<br>• Manage Hugging Face Spaces deployments   |
| Maitri      | Data Scientist / AI Lead| • Implement Bayesian Q&A layer<br>• Design & tune priors<br>• Oversee transformer fallback prompts          |",usage
"1. Fork and create a feature branch.
2. Add or update scripts/tests as needed.
3. Ensure style checks (`flake8`, `black`) pass.
4. Submit a pull request detailing your enhancements.",reference
MIT License,other
"<!-- If there is a paper or blog post introducing the dataset, the APA and Bibtex information for that should go in this section. -->",reference
"```bash
pip install -U sentence-transformers
```",installation
"*Provides a way for people who have updates to the Model Card, suggestions, or questions, to contact the Model Card authors.*
-->",other
Update this directory using maint_tools/vendor_array_api_compat.sh,reference
First install the Sentence Transformers library:,installation
"# 2) Install dependencies
pip install --upgrade pip setuptools wheel
pip install -r requirements.txt",installation
"{%- if co2_eq_emissions %}
### Environmental Impact
Carbon emissions were measured using [CodeCarbon](https://github.com/mlco2/codecarbon).
- **Energy Consumed**: {{ ""%.3f""|format(co2_eq_emissions[""energy_consumed""]) }} kWh
- **Carbon Emitted**: {{ ""%.3f""|format(co2_eq_emissions[""emissions""] / 1000) }} kg of CO2
- **Hours Used**: {{ co2_eq_emissions[""hours_used""] }} hours",other
"# 3-Clause BSD License
**Copyright (c) 2019 Kevin Sheppard. All rights reserved.**",other
First install the Sentence Transformers library:,installation
"# Build the index
reposage index",usage
"{% endif -%}
{{ eval_lines }}{% if explain_bold_in_eval %}
* The bold row denotes the saved checkpoint.{% endif %}
{%- if hide_eval_lines %}
</details>{% endif %}
{% endif %}",other
"## Training Details
{% for dataset_type, dataset_list in [(""training"", train_datasets), (""evaluation"", eval_datasets)] %}{% if dataset_list %}
### {{ dataset_type.title() }} Dataset{{""s"" if dataset_list | length > 1 else """"}}
{% for dataset in dataset_list %}{% if dataset_list | length > 3 %}<details><summary>{{ dataset['name'] or 'Unnamed Dataset' }}</summary>
{% endif %}
#### {{ dataset['name'] or 'Unnamed Dataset' }}
{% if dataset['name'] %}
* Dataset: {% if 'id' in dataset %}[{{ dataset['name'] }}](https://huggingface.co/datasets/{{ dataset['id'] }}){% else %}{{ dataset['name'] }}{% endif %}
{%- if 'revision' in dataset and 'id' in dataset %} at [{{ dataset['revision'][:7] }}](https://huggingface.co/datasets/{{ dataset['id'] }}/tree/{{ dataset['revision'] }}){% endif %}{% endif %}
{% if dataset['size'] %}* Size: {{ ""{:,}"".format(dataset['size']) }} {{ dataset_type }} samples
{% endif %}* Columns: {% if dataset['columns'] | length == 1 %}{{ dataset['columns'][0] }}{% elif dataset['columns'] | length == 2 %}{{ dataset['columns'][0] }} and {{ dataset['columns'][1] }}{% else %}{{ dataset['columns'][:-1] | join(', ') }}, and {{ dataset['columns'][-1] }}{% endif %}
{% if dataset['stats_table'] %}* Approximate statistics based on the first {{ [dataset['size'], 1000] | min }} samples:
{{ dataset['stats_table'] }}{% endif %}{% if dataset['examples_table'] %}* Samples:
{{ dataset['examples_table'] }}{% endif %}* Loss: {% if dataset[""loss""][""fullname""].startswith(""sentence_transformers."") %}[<code>{{ dataset[""loss""][""fullname""].split(""."")[-1] }}</code>](https://sbert.net/docs/package_reference/cross_encoder/losses.html#{{ dataset[""loss""][""fullname""].split(""."")[-1].lower() }}){% else %}<code>{{ dataset[""loss""][""fullname""] }}</code>{% endif %}{% if ""config_code"" in dataset[""loss""] %} with these parameters:
{{ dataset[""loss""][""config_code""] }}{% endif %}
{% if dataset_list | length > 3 %}</details>
{% endif %}{% endfor %}{% endif %}{% endfor -%}",reference
"# 2) Install dependencies
pip install --upgrade pip setuptools wheel
pip install -r requirements.txt",installation
"```bash
pip install -U sentence-transformers
```",installation
First install the Sentence Transformers library:,installation
"1. **Route** via the H2O classifier to select a subset of paragraphs.
     2. **Retrieve** top‑k relevant passages with FAISS.
     3. **Rescore** with a Bayesian posterior to boost high-confidence results.
     4. **Fallback**: if the posterior confidence is below threshold, generate an answer using a compact Flan‑T5 model.",reference
"### Installation
```bash
git clone https://github.com/RohitRajpoot/reposage.git
cd reposage
pip install -r requirements.txt",installation
