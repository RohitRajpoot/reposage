text,category
"## Day 1
- Decision: Workspace ready, boards & sites created.
- Next: Build embedding notebook tomorrow.",other
# RepoSage: Hybrid AI-Centric Q\&A Pipeline,other
"This repository contains the core components of **RepoSage**, a hybrid AI-powered study assistant that routes student queries to the right documentation sections and delivers precise answers using a blend of classification, retrieval, Bayesian scoring, and generative fallback.",other
---,other
## üîÑ Hybrid Pipeline Overview,other
1. **Corpus Preparation**,other
"* Recursively scan **all** project markdowns (`reposage/README.md`, other `*.md`).
   * Split each file into paragraphs and export to `data/classification.csv` with an empty `category` column for manual labeling.",other
2. **H2O Classifier (Routing Layer)**,other
"* After labeling paragraphs (e.g. `installation`, `usage`, `configuration`, `api_reference`, `troubleshooting`), train an H2O AutoML model to predict the category of any incoming question.
   * This routing step ensures each query is answered by the most relevant section of your docs.",installation
3. **DeepSeek + Bayesian + Transformer Q\&A**,other
"* Build a FAISS index over all paragraphs (using sentence-transformer embeddings).
   * On query:",other
"1. **Route** via the H2O classifier to select a subset of paragraphs.
     2. **Retrieve** top‚Äëk relevant passages with FAISS.
     3. **Rescore** with a Bayesian posterior to boost high-confidence results.
     4. **Fallback**: if the posterior confidence is below threshold, generate an answer using a compact Flan‚ÄëT5 model.",api_reference
4. **Streamlit UI / Hugging Face Space**,other
"* The entire stack is orchestrated in `app.py` (or Gradio) as a Streamlit application.
   * Deploy on Hugging Face Spaces for instant access.
   * Users type a question, see ‚ÄúRouted to category:¬†X,‚Äù and receive a clear, sourced answer.",other
---,other
## üöÄ Getting Started,other
### 1. Prepare the Corpus,other
"```bash
python scripts/prepare_corpus.py \
  --input-dir reposage/ \
  --output data/classification.csv
```",other
Manually open `data/classification.csv` and assign each paragraph a category label.,other
### 2. Train the H2O Classifier,other
"```bash
python scripts/train_classifier.py \
  --data data/classification.csv \
  --model-out models/routing_model.zip
```",other
### 3. Build the FAISS Index,other
"```bash
python scripts/build_index.py \
  --data data/classification.csv \
  --index-out models/faiss_index.bin
```",other
### 4. Hook into Streamlit App,other
"In `app.py`, configure paths to:",configuration
"* `models/routing_model.zip`
* `models/faiss_index.bin`
* Embedding & Flan‚ÄëT5 models via your settings or environment variables.",other
Run locally:,other
"```bash
streamlit run app.py
```",other
---,other
## üîß Directory Structure,other
"```
‚îú‚îÄ data/
‚îÇ  ‚îî‚îÄ classification.csv       # Paragraphs + category column
‚îú‚îÄ models/
‚îÇ  ‚îú‚îÄ routing_model.zip       # Trained H2O classifier
‚îÇ  ‚îî‚îÄ faiss_index.bin         # FAISS index of embeddings
‚îú‚îÄ scripts/
‚îÇ  ‚îú‚îÄ prepare_corpus.py       # Splits & exports paragraphs
‚îÇ  ‚îú‚îÄ train_classifier.py     # H2O AutoML training
‚îÇ  ‚îî‚îÄ build_index.py          # Embedding + FAISS index builder
‚îú‚îÄ app.py                     # Streamlit (or Gradio) Q&A app
‚îú‚îÄ requirements.txt           # Python dependencies
‚îî‚îÄ README.md                  # (This file)
```",other
---,other
## üõ†Ô∏è Next Steps,other
"1. Label paragraphs in `data/classification.csv`.
2. Train or fine-tune the H2O classifier (`scripts/train_classifier.py`).
3. Integrate the routing model into your Streamlit UI so that every query is first classified, then answered from the right subset.",other
---,other
## ü§ù Contributing,other
"1. Fork and create a feature branch.
2. Add or update scripts/tests as needed.
3. Ensure style checks (`flake8`, `black`) pass.
4. Submit a pull request detailing your enhancements.",api_reference
---,other
## üìú License,other
Licensed under MIT. See [LICENSE](LICENSE) for details.,other
"---
title: RepoSage Chatbot
emoji: ü§ñ
colorFrom: indigo
colorTo: blue
sdk: streamlit
sdk_version: ""1.46.0""
app_file: app.py
pinned: true
---
# RepoSage‚Ñ¢ Chatbot",other
"An MVP AI chatbot built in AML-3304 using Bayesian embeddings, a simple transformer block, and DeepSeek-R1 integration ‚Äî all wired up with a GitHub-driven CI/CD pipeline to Hugging Face Spaces.",other
---,other
## üöÄ Live Demo,other
"Try it out live:  
üëâ https://huggingface.co/spaces/rohitrajpoot/reposage-chatbot",other
---,other
## üìñ Overview,other
"**What it is:**  
- A command-line & web demo (via Streamlit) that shows:
  1. **Embedding Q&A**: nearest‚Äêneighbor lookup in a trained token embedding (`assist/chat.py`)  
  2. **Bayesian Q&A**: frequency‚Äêbased ‚Äúco-occurrence‚Äù embedding lookup (`assist/bayes_chat.py`)  
  3. **Transformer Demo**: single‚Äêblock transformer next‚Äêtoken prediction (`assist/transformer_demo.py`)  
  4. **DeepSeek-R1**: calls to a 1.3B-parameter model for generative Q&A (wrapped to skip gracefully in Colab)",other
"**Why it matters:**  
- Demonstrates core GPT ‚Äúatoms‚Äù (token ‚Üí embedding ‚Üí attention ‚Üí generation)  
- Shows an end-to-end MLOps flow: local dev ‚Üí GitHub Actions ‚Üí Docker ‚Üí Hugging Face Spaces",other
---,other
## ‚öôÔ∏è Installation,installation
### Local (macOS/Linux),other
"```bash
git clone https://github.com/rohitrajpoot/reposage.git
cd reposage",other
"# 1) Create & activate venv
python3 -m venv .venv
source .venv/bin/activate",other
"# 2) Install dependencies
pip install --upgrade pip setuptools wheel
pip install -r requirements.txt",installation
"# 3) Smoke-test CLI
python -m assist.main chat ""hello world""",usage
"# 4) Run Streamlit demo
streamlit run app.py",other
"# Build the index
reposage index",usage
"# Query
reposage query ""What is DeepSeek?"" --threshold 0.3",usage
"# Or call the REST API
curl -X POST https://<your-space>.hf.space/query \
     -d '{""question"":""What is DeepSeek?""}'",api_reference
# RepoSage ‚Äì AML3304 AI Product Engineering,other
"## Overview
RepoSage is an AI-centric study assistant that uses semantic search (DeepSeek), a Bayesian Q&A layer, and a transformer fallback to answer student queries with traceability and transparency.",other
"## Features
- **DeepSeek Index** of lecture slides, code, and readings  
- **Bayesian Posterior Scoring** for precise answer selection  
- **Transformer Fallback** via HF text2text when confidence is low  
- **CLI & REST API** for local and web deployment  
- **CI/CD** via GitHub Actions ‚Üí Hugging Face Spaces",usage
## Getting Started,other
"### Prerequisites
- Python 3.8+  
- `faiss`, `sentence-transformers`, `transformers`, `openai-whisper`",other
"### Installation
```bash
git clone https://github.com/RohitRajpoot/reposage.git
cd reposage
pip install -r requirements.txt",installation
"### Usage
# Build the index
reposage index",usage
"# Query
reposage query ""What is DeepSeek?"" --threshold 0.3",usage
"```markdown
# Prompt Design Strategy",other
"## Goals
- Guide the transformer fallback to produce concise, context-aware answers.
- Maintain consistency with our domain (AML3304 materials).",other
"## Approach
1. **System prompt**:  
   > You are RepoSage, an AI tutor. Use the indexed materials to answer student questions.",other
2. **User prompt template**:,other
# Role Breakdown,other
"| Team Member | Role(s)                 | Responsibilities                             |
|-------------|-------------------------|----------------------------------------------|
| Rohit       | CEO / Product Lead,     | ‚Ä¢ Define product vision & roadmap<br>‚Ä¢ Manage scope & stakeholder communication<br>‚Ä¢ Write technical documentation |
| Kishan      | CTO / DevOps Lead,      | ‚Ä¢ Develop DeepSeek index & CLI scaffold<br>‚Ä¢ Set up CI/CD pipelines<br>‚Ä¢ Manage Hugging Face Spaces deployments   |
| Maitri      | Data Scientist / AI Lead| ‚Ä¢ Implement Bayesian Q&A layer<br>‚Ä¢ Design & tune priors<br>‚Ä¢ Oversee transformer fallback prompts          |",usage
"---
# For reference on model card metadata, see the spec: https://github.com/huggingface/hub-docs/blob/main/modelcard.md?plain=1
# Doc / guide: https://huggingface.co/docs/hub/model-cards
{{ card_data }}
---",other
"# {{ model_name if model_name else ""Sentence Transformer model"" }}",other
"This is a [sentence-transformers](https://www.SBERT.net) model{% if base_model %} finetuned from [{{ base_model }}](https://huggingface.co/{{ base_model }}){% else %} trained{% endif %}{% if train_datasets | selectattr(""name"") | list %} on {% if train_datasets | selectattr(""name"") | map(attribute=""name"") | join("", "") | length > 200 %}{{ train_datasets | length }}{% else %}the {% for dataset in (train_datasets | selectattr(""name"")) %}{% if dataset.id %}[{{ dataset.name if dataset.name else dataset.id }}](https://huggingface.co/datasets/{{ dataset.id }}){% else %}{{ dataset.name }}{% endif %}{% if not loop.last %}{% if loop.index == (train_datasets | selectattr(""name"") | list | length - 1) %} and {% else %}, {% endif %}{% endif %}{% endfor %}{% endif %} dataset{{""s"" if train_datasets | selectattr(""name"") | list | length > 1 else """"}}{% endif %}. It maps sentences & paragraphs to a {{ output_dimensionality }}-dimensional dense vector space and can be used for {{ task_name }}.",other
## Model Details,other
"### Model Description
- **Model Type:** Sentence Transformer
{% if base_model -%}
    {%- if base_model_revision -%}
    - **Base model:** [{{ base_model }}](https://huggingface.co/{{ base_model }}) <!-- at revision {{ base_model_revision }} -->
    {%- else -%}
    - **Base model:** [{{ base_model }}](https://huggingface.co/{{ base_model }})
    {%- endif -%}
{%- else -%}
    <!-- - **Base model:** [Unknown](https://huggingface.co/unknown) -->
{%- endif %}
- **Maximum Sequence Length:** {{ model_max_length }} tokens
- **Output Dimensionality:** {{ output_dimensionality }} dimensions
- **Similarity Function:** {{ similarity_fn_name }}
{% if train_datasets | selectattr(""name"") | list -%}
    - **Training Dataset{{""s"" if train_datasets | selectattr(""name"") | list | length > 1 else """"}}:**
    {%- for dataset in (train_datasets | selectattr(""name"")) %}
        {%- if dataset.id %}
    - [{{ dataset.name if dataset.name else dataset.id }}](https://huggingface.co/datasets/{{ dataset.id }})
        {%- else %}
    - {{ dataset.name }}
        {%- endif %}
    {%- endfor %}
{%- else -%}
    <!-- - **Training Dataset:** Unknown -->
{%- endif %}
{% if language -%}
    - **Language{{""s"" if language is not string and language | length > 1 else """"}}:**
    {%- if language is string %} {{ language }}
    {%- else %} {% for lang in language -%}
            {{ lang }}{{ "", "" if not loop.last else """" }}
        {%- endfor %}
    {%- endif %}
{%- else -%}
    <!-- - **Language:** Unknown -->
{%- endif %}
{% if license -%}
    - **License:** {{ license }}
{%- else -%}
    <!-- - **License:** Unknown -->
{%- endif %}",other
### Model Sources,other
"- **Documentation:** [Sentence Transformers Documentation](https://sbert.net)
- **Repository:** [Sentence Transformers on GitHub](https://github.com/UKPLab/sentence-transformers)
- **Hugging Face:** [Sentence Transformers on Hugging Face](https://huggingface.co/models?library=sentence-transformers)",other
### Full Model Architecture,other
"```
{{ model_string }}
```",other
## Usage,other
### Direct Usage (Sentence Transformers),other
First install the Sentence Transformers library:,installation
"```bash
pip install -U sentence-transformers
```",installation
"Then you can load this model and run inference.
```python
from sentence_transformers import SentenceTransformer",other
"# Download from the {{ hf_emoji }} Hub
model = SentenceTransformer(""{{ model_id | default('sentence_transformers_model_id', true) }}"")
# Run inference
sentences = [
{%- for text in (predict_example or [""The weather is lovely today."", ""It's so sunny outside!"", ""He drove to the stadium.""]) %}
    {{ ""%r"" | format(text) }},
{%- endfor %}
]
embeddings = model.encode(sentences)
print(embeddings.shape)
# [{{ (predict_example or [""The weather is lovely today."", ""It's so sunny outside!"", ""He drove to the stadium.""]) | length}}, {{ output_dimensionality | default(1024, true) }}]",other
"# Get the similarity scores for the embeddings
similarities = model.similarity(embeddings, embeddings)
print(similarities.shape)
# [{{ (predict_example or [""The weather is lovely today."", ""It's so sunny outside!"", ""He drove to the stadium.""]) | length}}, {{ (predict_example or [""The weather is lovely today."", ""It's so sunny outside!"", ""He drove to the stadium.""]) | length}}]
```",other
"<!--
### Direct Usage (Transformers)",other
<details><summary>Click to see the direct usage in Transformers</summary>,usage
"</details>
-->",other
"<!--
### Downstream Usage (Sentence Transformers)",other
You can finetune this model on your own dataset.,other
<details><summary>Click to expand</summary>,usage
"</details>
-->",other
"<!--
### Out-of-Scope Use",other
"*List how the model may foreseeably be misused and address what users ought not to do with the model.*
-->
{% if eval_metrics %}
## Evaluation",other
"### Metrics
{% for metrics in eval_metrics %}
#### {{ metrics.description }}
{% if metrics.dataset_name %}
* Dataset{% if metrics.dataset_name is not string and metrics.dataset_name | length > 1 %}s{% endif %}: {% if metrics.dataset_name is string -%}
        `{{ metrics.dataset_name }}`
    {%- else -%}
        {%- for name in metrics.dataset_name -%}
            `{{ name }}`
            {%- if not loop.last -%}
                {%- if loop.index == metrics.dataset_name | length - 1 %} and {% else -%}, {% endif -%}
            {%- endif -%}
        {%- endfor -%}
    {%- endif -%}
{%- endif %}
* Evaluated with {% if metrics.class_name.startswith(""sentence_transformers."") %}[<code>{{ metrics.class_name.split(""."")[-1] }}</code>](https://sbert.net/docs/package_reference/sentence_transformer/evaluation.html#sentence_transformers.evaluation.{{ metrics.class_name.split(""."")[-1] }}){% else %}<code>{{ metrics.class_name }}</code>{% endif %}{% if metrics.config_code %} with these parameters:
{{ metrics.config_code }}{% endif %}",configuration
"{{ metrics.table }}
{%- endfor %}{% endif %}
<!--
## Bias, Risks and Limitations",other
"*What are the known or foreseeable issues stemming from this model? You could also flag here known failure cases or weaknesses of the model.*
-->",other
"<!--
### Recommendations",other
"*What are recommendations with respect to the foreseeable issues? For example, filtering explicit content.*
-->",other
"## Training Details
{% for dataset_type, dataset_list in [(""training"", train_datasets), (""evaluation"", eval_datasets)] %}{% if dataset_list %}
### {{ dataset_type.title() }} Dataset{{""s"" if dataset_list | length > 1 else """"}}
{% for dataset in dataset_list %}{% if dataset_list | length > 3 %}<details><summary>{{ dataset['name'] or 'Unnamed Dataset' }}</summary>
{% endif %}
#### {{ dataset['name'] or 'Unnamed Dataset' }}
{% if dataset['name'] %}
* Dataset: {% if 'id' in dataset %}[{{ dataset['name'] }}](https://huggingface.co/datasets/{{ dataset['id'] }}){% else %}{{ dataset['name'] }}{% endif %}
{%- if 'revision' in dataset and 'id' in dataset %} at [{{ dataset['revision'][:7] }}](https://huggingface.co/datasets/{{ dataset['id'] }}/tree/{{ dataset['revision'] }}){% endif %}{% endif %}
{% if dataset['size'] %}* Size: {{ ""{:,}"".format(dataset['size']) }} {{ dataset_type }} samples
{% endif %}* Columns: {% if dataset['columns'] | length == 1 %}{{ dataset['columns'][0] }}{% elif dataset['columns'] | length == 2 %}{{ dataset['columns'][0] }} and {{ dataset['columns'][1] }}{% else %}{{ dataset['columns'][:-1] | join(', ') }}, and {{ dataset['columns'][-1] }}{% endif %}
{% if dataset['stats_table'] %}* Approximate statistics based on the first {{ [dataset['size'], 1000] | min }} samples:
{{ dataset['stats_table'] }}{% endif %}{% if dataset['examples_table'] %}* Samples:
{{ dataset['examples_table'] }}{% endif %}* Loss: {% if dataset[""loss""][""fullname""].startswith(""sentence_transformers."") %}[<code>{{ dataset[""loss""][""fullname""].split(""."")[-1] }}</code>](https://sbert.net/docs/package_reference/sentence_transformer/losses.html#{{ dataset[""loss""][""fullname""].split(""."")[-1].lower() }}){% else %}<code>{{ dataset[""loss""][""fullname""] }}</code>{% endif %}{% if ""config_code"" in dataset[""loss""] %} with these parameters:
{{ dataset[""loss""][""config_code""] }}{% endif %}
{% if dataset_list | length > 3 %}</details>
{% endif %}{% endfor %}{% endif %}{% endfor -%}",configuration
"{% if all_hyperparameters %}
### Training Hyperparameters
{% if non_default_hyperparameters -%}
#### Non-Default Hyperparameters",other
"{% for name, value in non_default_hyperparameters.items() %}- `{{ name }}`: {{ value }}
{% endfor %}{%- endif %}
#### All Hyperparameters
<details><summary>Click to expand</summary>",usage
"{% for name, value in all_hyperparameters.items() %}- `{{ name }}`: {{ value }}
{% endfor %}
</details>
{% endif %}",other
"{%- if eval_lines %}
### Training Logs
{% if hide_eval_lines %}<details><summary>Click to expand</summary>",usage
"{% endif -%}
{{ eval_lines }}{% if explain_bold_in_eval %}
* The bold row denotes the saved checkpoint.{% endif %}
{%- if hide_eval_lines %}
</details>{% endif %}
{% endif %}",other
"{%- if co2_eq_emissions %}
### Environmental Impact
Carbon emissions were measured using [CodeCarbon](https://github.com/mlco2/codecarbon).
- **Energy Consumed**: {{ ""%.3f""|format(co2_eq_emissions[""energy_consumed""]) }} kWh
- **Carbon Emitted**: {{ ""%.3f""|format(co2_eq_emissions[""emissions""] / 1000) }} kg of CO2
- **Hours Used**: {{ co2_eq_emissions[""hours_used""] }} hours",other
"### Training Hardware
- **On Cloud**: {{ ""Yes"" if co2_eq_emissions[""on_cloud""] else ""No"" }}
- **GPU Model**: {{ co2_eq_emissions[""hardware_used""] or ""No GPU used"" }}
- **CPU Model**: {{ co2_eq_emissions[""cpu_model""] }}
- **RAM Size**: {{ ""%.2f""|format(co2_eq_emissions[""ram_total_size""]) }} GB
{% endif %}
### Framework Versions
- Python: {{ version[""python""] }}
- Sentence Transformers: {{ version[""sentence_transformers""] }}
- Transformers: {{ version[""transformers""] }}
- PyTorch: {{ version[""torch""] }}
- Accelerate: {{ version[""accelerate""] }}
- Datasets: {{ version[""datasets""] }}
- Tokenizers: {{ version[""tokenizers""] }}",other
## Citation,other
"### BibTeX
{% for loss_name, citation in citations.items() %}
#### {{ loss_name }}
```bibtex
{{ citation | trim }}
```
{% endfor %}
<!--
## Glossary",other
"*Clearly define terms in order to be accessible across audiences.*
-->",other
"<!--
## Model Card Authors",other
"*Lists the people who create the model card, providing recognition and accountability for the detailed work that goes into its construction.*
-->",other
"<!--
## Model Card Contact",other
"*Provides a way for people who have updates to the Model Card, suggestions, or questions, to contact the Model Card authors.*
-->",other
"---
# For reference on model card metadata, see the spec: https://github.com/huggingface/hub-docs/blob/main/modelcard.md?plain=1
# Doc / guide: https://huggingface.co/docs/hub/model-cards
{{ card_data }}
---",other
"# {{ model_name if model_name else ""Cross Encoder model"" }}",other
"This is a [Cross Encoder](https://www.sbert.net/docs/cross_encoder/usage/usage.html) model{% if base_model %} finetuned from [{{ base_model }}](https://huggingface.co/{{ base_model }}){% else %} trained{% endif %}{% if train_datasets | selectattr(""name"") | list %} on {% if train_datasets | selectattr(""name"") | map(attribute=""name"") | join("", "") | length > 200 %}{{ train_datasets | length }}{% else %}the {% for dataset in (train_datasets | selectattr(""name"")) %}{% if dataset.id %}[{{ dataset.name if dataset.name else dataset.id }}](https://huggingface.co/datasets/{{ dataset.id }}){% else %}{{ dataset.name }}{% endif %}{% if not loop.last %}{% if loop.index == (train_datasets | selectattr(""name"") | list | length - 1) %} and {% else %}, {% endif %}{% endif %}{% endfor %}{% endif %} dataset{{""s"" if train_datasets | selectattr(""name"") | list | length > 1 else """"}}{% endif %} using the [sentence-transformers](https://www.SBERT.net) library. It computes scores for pairs of texts, which can be used for {{ task_name }}.",other
## Model Details,other
"### Model Description
- **Model Type:** Cross Encoder
{% if base_model -%}
    {%- if base_model_revision -%}
    - **Base model:** [{{ base_model }}](https://huggingface.co/{{ base_model }}) <!-- at revision {{ base_model_revision }} -->
    {%- else -%}
    - **Base model:** [{{ base_model }}](https://huggingface.co/{{ base_model }})
    {%- endif -%}
{%- else -%}
    <!-- - **Base model:** [Unknown](https://huggingface.co/unknown) -->
{%- endif %}
- **Maximum Sequence Length:** {{ model_max_length }} tokens
- **Number of Output Labels:** {{ model_num_labels }} label{{ ""s"" if model_num_labels > 1 else """" }}
{% if train_datasets | selectattr(""name"") | list -%}
    - **Training Dataset{{""s"" if train_datasets | selectattr(""name"") | list | length > 1 else """"}}:**
    {%- for dataset in (train_datasets | selectattr(""name"")) %}
        {%- if dataset.id %}
    - [{{ dataset.name if dataset.name else dataset.id }}](https://huggingface.co/datasets/{{ dataset.id }})
        {%- else %}
    - {{ dataset.name }}
        {%- endif %}
    {%- endfor %}
{%- else -%}
    <!-- - **Training Dataset:** Unknown -->
{%- endif %}
{% if language -%}
    - **Language{{""s"" if language is not string and language | length > 1 else """"}}:**
    {%- if language is string %} {{ language }}
    {%- else %} {% for lang in language -%}
            {{ lang }}{{ "", "" if not loop.last else """" }}
        {%- endfor %}
    {%- endif %}
{%- else -%}
    <!-- - **Language:** Unknown -->
{%- endif %}
{% if license -%}
    - **License:** {{ license }}
{%- else -%}
    <!-- - **License:** Unknown -->
{%- endif %}",other
### Model Sources,other
"- **Documentation:** [Sentence Transformers Documentation](https://sbert.net)
- **Documentation:** [Cross Encoder Documentation](https://www.sbert.net/docs/cross_encoder/usage/usage.html)
- **Repository:** [Sentence Transformers on GitHub](https://github.com/UKPLab/sentence-transformers)
- **Hugging Face:** [Cross Encoders on Hugging Face](https://huggingface.co/models?library=sentence-transformers&other=cross-encoder)",other
## Usage,other
### Direct Usage (Sentence Transformers),other
First install the Sentence Transformers library:,installation
"```bash
pip install -U sentence-transformers
```",installation
"Then you can load this model and run inference.
```python
from sentence_transformers import CrossEncoder",other
"# Download from the {{ hf_emoji }} Hub
model = CrossEncoder(""{{ model_id | default('cross_encoder_model_id', true) }}"")
# Get scores for pairs of texts
pairs = [
{%- for text in (predict_example or [[""How many calories in an egg"", ""There are on average between 55 and 80 calories in an egg depending on its size.""], [""How many calories in an egg"", ""Egg whites are very low in calories, have no fat, no cholesterol, and are loaded with protein.""], [""How many calories in an egg"", ""Most of the calories in an egg come from the yellow yolk in the center.""]]) %}
    {{ ""%r"" | format(text) }},
{%- endfor %}
]
scores = model.predict(pairs)
print(scores.shape)
# ({{ (predict_example or [""dummy"", ""dummy"", ""dummy""]) | length }},{{ ("" %d"" | format(model_num_labels)) if model_num_labels > 1 else """" }}){% if model_num_labels == 1 %}",other
"# Or rank different texts based on similarity to a single text
ranks = model.rank(
    {{ ""%r"" | format(predict_example[0][0] if predict_example else ""How many calories in an egg"") }},
    [
{%- for pair in (predict_example or [[""How many calories in an egg"", ""There are on average between 55 and 80 calories in an egg depending on its size.""], [""How many calories in an egg"", ""Egg whites are very low in calories, have no fat, no cholesterol, and are loaded with protein.""], [""How many calories in an egg"", ""Most of the calories in an egg come from the yellow yolk in the center.""]]) %}
        {{ ""%r"" | format(pair[1]) }},
{%- endfor %}
    ]
)
# [{'corpus_id': ..., 'score': ...}, {'corpus_id': ..., 'score': ...}, ...]{% endif %}
```",other
"<!--
### Direct Usage (Transformers)",other
<details><summary>Click to see the direct usage in Transformers</summary>,usage
"</details>
-->",other
"<!--
### Downstream Usage (Sentence Transformers)",other
You can finetune this model on your own dataset.,other
<details><summary>Click to expand</summary>,usage
"</details>
-->",other
"<!--
### Out-of-Scope Use",other
"*List how the model may foreseeably be misused and address what users ought not to do with the model.*
-->
{% if eval_metrics %}
## Evaluation",other
"### Metrics
{% for metrics in eval_metrics %}
#### {{ metrics.description }}
{% if metrics.dataset_name %}
* Dataset{% if metrics.dataset_name is not string and metrics.dataset_name | length > 1 %}s{% endif %}: {% if metrics.dataset_name is string -%}
        `{{ metrics.dataset_name }}`
    {%- else -%}
        {%- for name in metrics.dataset_name -%}
            `{{ name }}`
            {%- if not loop.last -%}
                {%- if loop.index == metrics.dataset_name | length - 1 %} and {% else -%}, {% endif -%}
            {%- endif -%}
        {%- endfor -%}
    {%- endif -%}
{%- endif %}
* Evaluated with {% if metrics.class_name.startswith(""sentence_transformers."") %}[<code>{{ metrics.class_name.split(""."")[-1] }}</code>](https://sbert.net/docs/package_reference/cross_encoder/evaluation.html#sentence_transformers.cross_encoder.evaluation.{{ metrics.class_name.split(""."")[-1] }}){% else %}<code>{{ metrics.class_name }}</code>{% endif %}{% if metrics.config_code %} with these parameters:
{{ metrics.config_code }}{% endif %}",configuration
"{{ metrics.table }}
{%- endfor %}{% endif %}
<!--
## Bias, Risks and Limitations",other
"*What are the known or foreseeable issues stemming from this model? You could also flag here known failure cases or weaknesses of the model.*
-->",other
"<!--
### Recommendations",other
"*What are recommendations with respect to the foreseeable issues? For example, filtering explicit content.*
-->",other
"## Training Details
{% for dataset_type, dataset_list in [(""training"", train_datasets), (""evaluation"", eval_datasets)] %}{% if dataset_list %}
### {{ dataset_type.title() }} Dataset{{""s"" if dataset_list | length > 1 else """"}}
{% for dataset in dataset_list %}{% if dataset_list | length > 3 %}<details><summary>{{ dataset['name'] or 'Unnamed Dataset' }}</summary>
{% endif %}
#### {{ dataset['name'] or 'Unnamed Dataset' }}
{% if dataset['name'] %}
* Dataset: {% if 'id' in dataset %}[{{ dataset['name'] }}](https://huggingface.co/datasets/{{ dataset['id'] }}){% else %}{{ dataset['name'] }}{% endif %}
{%- if 'revision' in dataset and 'id' in dataset %} at [{{ dataset['revision'][:7] }}](https://huggingface.co/datasets/{{ dataset['id'] }}/tree/{{ dataset['revision'] }}){% endif %}{% endif %}
{% if dataset['size'] %}* Size: {{ ""{:,}"".format(dataset['size']) }} {{ dataset_type }} samples
{% endif %}* Columns: {% if dataset['columns'] | length == 1 %}{{ dataset['columns'][0] }}{% elif dataset['columns'] | length == 2 %}{{ dataset['columns'][0] }} and {{ dataset['columns'][1] }}{% else %}{{ dataset['columns'][:-1] | join(', ') }}, and {{ dataset['columns'][-1] }}{% endif %}
{% if dataset['stats_table'] %}* Approximate statistics based on the first {{ [dataset['size'], 1000] | min }} samples:
{{ dataset['stats_table'] }}{% endif %}{% if dataset['examples_table'] %}* Samples:
{{ dataset['examples_table'] }}{% endif %}* Loss: {% if dataset[""loss""][""fullname""].startswith(""sentence_transformers."") %}[<code>{{ dataset[""loss""][""fullname""].split(""."")[-1] }}</code>](https://sbert.net/docs/package_reference/cross_encoder/losses.html#{{ dataset[""loss""][""fullname""].split(""."")[-1].lower() }}){% else %}<code>{{ dataset[""loss""][""fullname""] }}</code>{% endif %}{% if ""config_code"" in dataset[""loss""] %} with these parameters:
{{ dataset[""loss""][""config_code""] }}{% endif %}
{% if dataset_list | length > 3 %}</details>
{% endif %}{% endfor %}{% endif %}{% endfor -%}",configuration
"{% if all_hyperparameters %}
### Training Hyperparameters
{% if non_default_hyperparameters -%}
#### Non-Default Hyperparameters",other
"{% for name, value in non_default_hyperparameters.items() %}- `{{ name }}`: {{ value }}
{% endfor %}{%- endif %}
#### All Hyperparameters
<details><summary>Click to expand</summary>",usage
"{% for name, value in all_hyperparameters.items() %}- `{{ name }}`: {{ value }}
{% endfor %}
</details>
{% endif %}",other
"{%- if eval_lines %}
### Training Logs
{% if hide_eval_lines %}<details><summary>Click to expand</summary>",usage
"{% endif -%}
{{ eval_lines }}{% if explain_bold_in_eval %}
* The bold row denotes the saved checkpoint.{% endif %}
{%- if hide_eval_lines %}
</details>{% endif %}
{% endif %}",other
"{%- if co2_eq_emissions %}
### Environmental Impact
Carbon emissions were measured using [CodeCarbon](https://github.com/mlco2/codecarbon).
- **Energy Consumed**: {{ ""%.3f""|format(co2_eq_emissions[""energy_consumed""]) }} kWh
- **Carbon Emitted**: {{ ""%.3f""|format(co2_eq_emissions[""emissions""] / 1000) }} kg of CO2
- **Hours Used**: {{ co2_eq_emissions[""hours_used""] }} hours",other
"### Training Hardware
- **On Cloud**: {{ ""Yes"" if co2_eq_emissions[""on_cloud""] else ""No"" }}
- **GPU Model**: {{ co2_eq_emissions[""hardware_used""] or ""No GPU used"" }}
- **CPU Model**: {{ co2_eq_emissions[""cpu_model""] }}
- **RAM Size**: {{ ""%.2f""|format(co2_eq_emissions[""ram_total_size""]) }} GB
{% endif %}
### Framework Versions
- Python: {{ version[""python""] }}
- Sentence Transformers: {{ version[""sentence_transformers""] }}
- Transformers: {{ version[""transformers""] }}
- PyTorch: {{ version[""torch""] }}
- Accelerate: {{ version[""accelerate""] }}
- Datasets: {{ version[""datasets""] }}
- Tokenizers: {{ version[""tokenizers""] }}",other
## Citation,other
"### BibTeX
{% for loss_name, citation in citations.items() %}
#### {{ loss_name }}
```bibtex
{{ citation | trim }}
```
{% endfor %}
<!--
## Glossary",other
"*Clearly define terms in order to be accessible across audiences.*
-->",other
"<!--
## Model Card Authors",other
"*Lists the people who create the model card, providing recognition and accountability for the detailed work that goes into its construction.*
-->",other
"<!--
## Model Card Contact",other
"*Provides a way for people who have updates to the Model Card, suggestions, or questions, to contact the Model Card authors.*
-->",other
BSD 3-Clause License,other
"Copyright (c) 2013-2024, Kim Davies and contributors.
All rights reserved.",other
"Redistribution and use in source and binary forms, with or without
modification, are permitted provided that the following conditions are
met:",other
"1. Redistributions of source code must retain the above copyright
   notice, this list of conditions and the following disclaimer.",other
"2. Redistributions in binary form must reproduce the above copyright
   notice, this list of conditions and the following disclaimer in the
   documentation and/or other materials provided with the distribution.",other
"3. Neither the name of the copyright holder nor the names of its
   contributors may be used to endorse or promote products derived from
   this software without specific prior written permission.",other
"THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS
""AS IS"" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT
LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR
A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT
HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED
TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.",other
# Natural Language Toolkit (NLTK) Authors,other
## Original Authors,other
"- Steven Bird <stevenbird1@gmail.com>
- Edward Loper <edloper@gmail.com>
- Ewan Klein <ewan@inf.ed.ac.uk>",other
## Contributors,other
"- Tom Aarsen
- Rami Al-Rfou'
- Mark Amery
- Greg Aumann
- Ivan Barria
- Ingolf Becker
- Yonatan Becker
- Paul Bedaride
- Steven Bethard
- Robert Berwick
- Dan Blanchard
- Nathan Bodenstab
- Alexander B√∂hm
- Francis Bond
- Paul Bone
- Jordan Boyd-Graber
- Daniel Blanchard
- Phil Blunsom
- Lars Buitinck
- Cristian Capdevila
- Steve Cassidy
- Chen-Fu Chiang
- Dmitry Chichkov
- Jinyoung Choi
- Andrew Clausen
- Lucas Champollion
- Graham Christensen
- Trevor Cohn
- David Coles
- Tom Conroy <https://github.com/tconroy>
- Claude Coulombe
- Lucas Cooper
- Robin Cooper
- Chris Crowner
- James Curran
- Arthur Darcet
- Dariel Dato-on
- Selina Dennis
- Leon Derczynski
- Alexis Dimitriadis
- Nikhil Dinesh
- Liang Dong
- David Doukhan
- Rebecca Dridan
- Pablo Duboue
- Long Duong
- Christian Federmann
- Campion Fellin
- Michelle Fullwood
- Dan Garrette
- Maciej Gawinecki
- Jean Mark Gawron
- Sumukh Ghodke
- Yoav Goldberg
- Michael Wayne Goodman
- Dougal Graham
- Brent Gray
- Simon Greenhill
- Clark Grubb
- Eduardo Pereira Habkost
- Masato Hagiwara
- Lauri Hallila
- Michael Hansen
- Yurie Hara
- Will Hardy
- Tyler Hartley
- Peter Hawkins
- Saimadhav Heblikar
- Fredrik Hedman
- Helder
- Michael Heilman
- Ofer Helman
- Christopher Hench
- Bruce Hill
- Amy Holland
- Kristy Hollingshead
- Marcus Huderle
- Baden Hughes
- Nancy Ide
- Rebecca Ingram
- Edward Ivanovic
- Thomas Jakobsen
- Nick Johnson
- Eric Kafe
- Piotr Kasprzyk
- Angelos Katharopoulos
- Sudharshan Kaushik
- Chris Koenig
- Mikhail Korobov
- Denis Krusko
- Ilia Kurenkov
- Stefano Lattarini
- Pierre-Fran√ßois Laquerre
- Stefano Lattarini
- Haejoong Lee
- Jackson Lee
- Max Leonov
- Chris Liechti
- Hyuckin David Lim
- Tom Lippincott
- Peter Ljungl√∂f
- Alex Louden
- David Luke≈°
- Joseph Lynch
- Nitin Madnani
- Felipe Madrigal
- Bj√∏rn M√¶land
- Dean Malmgren
- Christopher Maloof
- Rob Malouf
- Iker Manterola
- Carl de Marcken
- Mitch Marcus
- Torsten Marek
- Robert Marshall
- Marius Mather
- Duncan McGreggor
- David McClosky
- Xinfan Meng
- Dmitrijs Milajevs
- Matt Miller
- Margaret Mitchell
- Tomonori Nagano
- Jason Narad
- Shari A‚Äôaidil Nasruddin
- Lance Nathan
- Morten Neergaard
- David Nemeskey
- Eric Nichols
- Joel Nothman
- Alireza Nourian
- Alexander Oleynikov
- Pierpaolo Pantone
- Ted Pedersen
- Jacob Perkins
- Alberto Planas
- Ondrej Platek
- Alessandro Presta
- Qi Liu
- Martin Thorsen Ranang
- Michael Recachinas
- Brandon Rhodes
- Joshua Ritterman
- Will Roberts
- Stuart Robinson
- Carlos Rodriguez
- Lorenzo Rubio
- Alex Rudnick
- Jussi Salmela
- Geoffrey Sampson
- Kepa Sarasola
- Kevin Scannell
- Nathan Schneider
- Rico Sennrich
- Thomas Skardal
- Eric Smith
- Lynn Soe
- Rob Speer
- Peter Spiller
- Richard Sproat
- Ceri Stagg
- Peter Stahl
- Oliver Steele
- Thomas Stieglmaier
- Jan Strunk
- Liling Tan
- Claire Taylor
- Louis Tiao
- Steven Tomcavage
- Tiago Tresoldi
- Marcus Uneson
- Yu Usami
- Petro Verkhogliad
- Peter Wang
- Zhe Wang
- Charlotte Wilson
- Chuck Wooters
- Steven Xu
- Beracah Yankama
- Lei Ye (Âè∂Á£ä)
- Patrick Ye
- Geraldine Sim Wei Ying
- Jason Yoder
- Thomas Zieglier
- 0ssifrage
- ducki13
- kiwipi
- lade
- isnowfy
- onesandzeros
- pquentin
- wvanlint
- √Ålvaro Justen <https://github.com/turicas>
- bjut-hz
- Sergio Oller
- Izam Mohammed <https://github.com/izam-mohammed>
- Will Monroe
- Elijah Rippeth
- Emil Manukyan
- Casper Lehmann-Str√∏m
- Andrew Giel
- Tanin Na Nakorn
- Linghao Zhang
- Colin Carroll
- Heguang Miao
- Hannah Aizenman (story645)
- George Berry
- Adam Nelson
- J Richard Snape
- Alex Constantin <alex@keyworder.ch>
- Tsolak Ghukasyan
- Prasasto Adi
- Safwan Kamarrudin
- Arthur Tilley
- Vilhjalmur Thorsteinsson
- Jaehoon Hwang <https://github.com/jaehoonhwang>
- Chintan Shah <https://github.com/chintanshah24>
- sbagan
- Zicheng Xu
- Albert Au Yeung <https://github.com/albertauyeung>
- Shenjian Zhao
- Deng Wang <https://github.com/lmatt-bit>
- Ali Abdullah
- Stoytcho Stoytchev
- Lakhdar Benzahia
- Kheireddine Abainia <https://github.com/xprogramer>
- Yibin Lin <https://github.com/yibinlin>
- Artiem Krinitsyn
- Bj√∂rn Mattsson
- Oleg Chislov
- Pavan Gururaj Joshi <https://github.com/PavanGJ>
- Ethan Hill <https://github.com/hill1303>
- Vivek Lakshmanan
- Somnath Rakshit <https://github.com/somnathrakshit>
- Anlan Du
- Pulkit Maloo <https://github.com/pulkitmaloo>
- Brandon M. Burroughs <https://github.com/brandonmburroughs>
- John Stewart <https://github.com/free-variation>
- Iaroslav Tymchenko <https://github.com/myproblemchild>
- Ale≈° Tamchyna
- Tim Gianitsos <https://github.com/timgianitsos>
- Philippe Partarrieu <https://github.com/ppartarr>
- Andrew Owen Martin
- Adrian Ellis <https://github.com/adrianjellis>
- Nat Quayle Nelson <https://github.com/nqnstudios>
- Yanpeng Zhao <https://github.com/zhaoyanpeng>
- Matan Rak <https://github.com/matanrak>
- Nick Ulle <https://github.com/nick-ulle>
- Uday Krishna <https://github.com/udaykrishna>
- Osman Zubair <https://github.com/okz12>
- Viresh Gupta <https://github.com/virresh>
- Ond≈ôej C√≠fka <https://github.com/cifkao>
- Iris X. Zhou <https://github.com/irisxzhou>
- Devashish Lal <https://github.com/BLaZeKiLL>
- Gerhard Kremer <https://github.com/GerhardKa>
- Nicolas Darr <https://github.com/ndarr>
- Herv√© Nicol <https://github.com/hervenicol>
- Alexandre H. T. Dias <https://github.com/alexandredias3d>
- Daksh Shah <https://github.com/Daksh>
- Jacob Weightman <https://github.com/jacobdweightman>
- Bonifacio de Oliveira <https://github.com/Bonifacio2>
- Armins Bagrats Stepanjans <https://github.com/ab-10>
- Vassilis Palassopoulos <https://github.com/palasso>
- Ram Rachum <https://github.com/cool-RR>
- Or Sharir <https://github.com/orsharir>
- Denali Molitor <https://github.com/dmmolitor>
- Jacob Moorman <https://github.com/jdmoorman>
- Cory Nezin <https://github.com/corynezin>
- Matt Chaput
- Danny Sepler <https://github.com/dannysepler>
- Akshita Bhagia <https://github.com/AkshitaB>
- Pratap Yadav <https://github.com/prtpydv>
- Hiroki Teranishi <https://github.com/chantera>
- Ruben Cartuyvels <https://github.com/rubencart>
- Dalton Pearson <https://github.com/daltonpearson>
- Robby Horvath <https://github.com/robbyhorvath>
- Gavish Poddar <https://github.com/gavishpoddar>
- Saibo Geng <https://github.com/Saibo-creator>
- Ahmet Yildirim <https://github.com/RnDevelover>
- Yuta Nakamura <https://github.com/yutanakamura-tky>
- Adam Hawley <https://github.com/adamjhawley>
- Panagiotis Simakis <https://github.com/sp1thas>
- Richard Wang <https://github.com/richarddwang>
- Alexandre Perez-Lebel <https://github.com/aperezlebel>
- Fernando Carranza <https://github.com/fernandocar86>
- Martin Kondratzky <https://github.com/martinkondra>
- Heungson Lee <https://github.com/heungson>
- M.K. Pawelkiewicz <https://github.com/hamiltonianflow>
- Steven Thomas Smith <https://github.com/essandess>
- Jan Lennartz <https://github.com/Madnex>
- Tim Sockel <https://github.com/TiMauzi>
- Akihiro Yamazaki <https://github.com/zakkie>
- Ron Urbach <https://github.com/sharpblade4>
- Vivek Kalyan <https://github.com/vivekkalyan>
- Tom Strange https://github.com/strangetom",other
"## Others whose work we've taken and included in NLTK, but who didn't directly contribute it:",other
### Contributors to the Porter Stemmer,other
#NAME?,other
### Authors of snowball arabic stemmer algorithm,other
#NAME?,other
"# Natural Language Toolkit (NLTK)
[![PyPI](https://img.shields.io/pypi/v/nltk.svg)](https://pypi.python.org/pypi/nltk)
![CI](https://github.com/nltk/nltk/actions/workflows/ci.yaml/badge.svg?branch=develop)",other
"NLTK -- the Natural Language Toolkit -- is a suite of open source Python
modules, data sets, and tutorials supporting research and development in Natural
Language Processing. NLTK requires Python version 3.8, 3.9, 3.10, 3.11 or 3.12.",other
"For documentation, please visit [nltk.org](https://www.nltk.org/).",other
## Contributing,other
"Do you want to contribute to NLTK development? Great!
Please read [CONTRIBUTING.md](CONTRIBUTING.md) for more details.",other
See also [how to contribute to NLTK](https://www.nltk.org/contribute.html).,other
## Donate,other
"Have you found the toolkit helpful?  Please support NLTK development by donating
to the project via PayPal, using the link on the NLTK homepage.",other
## Citing,other
"If you publish work that uses NLTK, please cite the NLTK book, as follows:",other
"Bird, Steven, Edward Loper and Ewan Klein (2009).
    Natural Language Processing with Python.  O'Reilly Media Inc.",other
## Copyright,other
Copyright (C) 2001-2024 NLTK Project,other
"For license information, see [LICENSE.txt](LICENSE.txt).",other
[AUTHORS.md](AUTHORS.md) contains a list of everyone who has contributed to NLTK.,other
### Redistributing,other
"- NLTK source code is distributed under the Apache 2.0 License.
- NLTK documentation is distributed under the Creative Commons
  Attribution-Noncommercial-No Derivative Works 3.0 United States license.
- NLTK corpora are provided under the terms given in the README file for each
  corpus; all are redistributable and available for non-commercial use.
- NLTK may be freely redistributed, subject to the provisions of these licenses.",other
"---
# For reference on dataset card metadata, see the spec: https://github.com/huggingface/hub-docs/blob/main/datasetcard.md?plain=1
# Doc / guide: https://huggingface.co/docs/hub/datasets-cards
{{ card_data }}
---",other
"# Dataset Card for {{ pretty_name | default(""Dataset Name"", true) }}",other
<!-- Provide a quick summary of the dataset. -->,other
"{{ dataset_summary | default("""", true) }}",other
## Dataset Details,other
### Dataset Description,other
<!-- Provide a longer summary of what this dataset is. -->,other
"{{ dataset_description | default("""", true) }}",other
"- **Curated by:** {{ curators | default(""[More Information Needed]"", true)}}
- **Funded by [optional]:** {{ funded_by | default(""[More Information Needed]"", true)}}
- **Shared by [optional]:** {{ shared_by | default(""[More Information Needed]"", true)}}
- **Language(s) (NLP):** {{ language | default(""[More Information Needed]"", true)}}
- **License:** {{ license | default(""[More Information Needed]"", true)}}",other
### Dataset Sources [optional],other
<!-- Provide the basic links for the dataset. -->,other
"- **Repository:** {{ repo | default(""[More Information Needed]"", true)}}
- **Paper [optional]:** {{ paper | default(""[More Information Needed]"", true)}}
- **Demo [optional]:** {{ demo | default(""[More Information Needed]"", true)}}",other
## Uses,other
<!-- Address questions around how the dataset is intended to be used. -->,other
### Direct Use,other
<!-- This section describes suitable use cases for the dataset. -->,other
"{{ direct_use | default(""[More Information Needed]"", true)}}",other
### Out-of-Scope Use,other
"<!-- This section addresses misuse, malicious use, and uses that the dataset will not work well for. -->",other
"{{ out_of_scope_use | default(""[More Information Needed]"", true)}}",other
## Dataset Structure,other
"<!-- This section provides a description of the dataset fields, and additional information about the dataset structure such as criteria used to create the splits, relationships between data points, etc. -->",other
"{{ dataset_structure | default(""[More Information Needed]"", true)}}",other
## Dataset Creation,other
### Curation Rationale,other
<!-- Motivation for the creation of this dataset. -->,other
"{{ curation_rationale_section | default(""[More Information Needed]"", true)}}",other
### Source Data,other
"<!-- This section describes the source data (e.g. news text and headlines, social media posts, translated sentences, ...). -->",api_reference
#### Data Collection and Processing,other
"<!-- This section describes the data collection and processing process such as data selection criteria, filtering and normalization methods, tools and libraries used, etc. -->",other
"{{ data_collection_and_processing_section | default(""[More Information Needed]"", true)}}",other
#### Who are the source data producers?,other
<!-- This section describes the people or systems who originally created the data. It should also include self-reported demographic or identity information for the source data creators if this information is available. -->,other
"{{ source_data_producers_section | default(""[More Information Needed]"", true)}}",other
### Annotations [optional],other
"<!-- If the dataset contains annotations which are not part of the initial data collection, use this section to describe them. -->",other
#### Annotation process,other
"<!-- This section describes the annotation process such as annotation tools used in the process, the amount of data annotated, annotation guidelines provided to the annotators, interannotator statistics, annotation validation, etc. -->",other
"{{ annotation_process_section | default(""[More Information Needed]"", true)}}",other
#### Who are the annotators?,other
<!-- This section describes the people or systems who created the annotations. -->,other
"{{ who_are_annotators_section | default(""[More Information Needed]"", true)}}",other
#### Personal and Sensitive Information,other
"<!-- State whether the dataset contains data that might be considered personal, sensitive, or private (e.g., data that reveals addresses, uniquely identifiable names or aliases, racial or ethnic origins, sexual orientations, religious beliefs, political opinions, financial or health data, etc.). If efforts were made to anonymize the data, describe the anonymization process. -->",other
"{{ personal_and_sensitive_information | default(""[More Information Needed]"", true)}}",other
"## Bias, Risks, and Limitations",other
<!-- This section is meant to convey both technical and sociotechnical limitations. -->,other
"{{ bias_risks_limitations | default(""[More Information Needed]"", true)}}",other
### Recommendations,other
"<!-- This section is meant to convey recommendations with respect to the bias, risk, and technical limitations. -->",other
"{{ bias_recommendations | default(""Users should be made aware of the risks, biases and limitations of the dataset. More information needed for further recommendations."", true)}}",other
## Citation [optional],other
"<!-- If there is a paper or blog post introducing the dataset, the APA and Bibtex information for that should go in this section. -->",api_reference
**BibTeX:**,other
"{{ citation_bibtex | default(""[More Information Needed]"", true)}}",other
**APA:**,other
"{{ citation_apa | default(""[More Information Needed]"", true)}}",other
## Glossary [optional],other
"<!-- If relevant, include terms and calculations in this section that can help readers understand the dataset or dataset card. -->",other
"{{ glossary | default(""[More Information Needed]"", true)}}",other
## More Information [optional],other
"{{ more_information | default(""[More Information Needed]"", true)}}",other
## Dataset Card Authors [optional],other
"{{ dataset_card_authors | default(""[More Information Needed]"", true)}}",other
## Dataset Card Contact,other
"{{ dataset_card_contact | default(""[More Information Needed]"", true)}}",other
"---
# For reference on model card metadata, see the spec: https://github.com/huggingface/hub-docs/blob/main/modelcard.md?plain=1
# Doc / guide: https://huggingface.co/docs/hub/model-cards
{{ card_data }}
---",other
"# Model Card for {{ model_id | default(""Model ID"", true) }}",other
<!-- Provide a quick summary of what the model is/does. -->,other
"{{ model_summary | default("""", true) }}",other
## Model Details,other
### Model Description,other
<!-- Provide a longer summary of what this model is. -->,other
"{{ model_description | default("""", true) }}",other
"- **Developed by:** {{ developers | default(""[More Information Needed]"", true)}}
- **Funded by [optional]:** {{ funded_by | default(""[More Information Needed]"", true)}}
- **Shared by [optional]:** {{ shared_by | default(""[More Information Needed]"", true)}}
- **Model type:** {{ model_type | default(""[More Information Needed]"", true)}}
- **Language(s) (NLP):** {{ language | default(""[More Information Needed]"", true)}}
- **License:** {{ license | default(""[More Information Needed]"", true)}}
- **Finetuned from model [optional]:** {{ base_model | default(""[More Information Needed]"", true)}}",other
### Model Sources [optional],other
<!-- Provide the basic links for the model. -->,other
"- **Repository:** {{ repo | default(""[More Information Needed]"", true)}}
- **Paper [optional]:** {{ paper | default(""[More Information Needed]"", true)}}
- **Demo [optional]:** {{ demo | default(""[More Information Needed]"", true)}}",other
## Uses,other
"<!-- Address questions around how the model is intended to be used, including the foreseeable users of the model and those affected by the model. -->",other
### Direct Use,other
<!-- This section is for the model use without fine-tuning or plugging into a larger ecosystem/app. -->,other
"{{ direct_use | default(""[More Information Needed]"", true)}}",other
### Downstream Use [optional],other
"<!-- This section is for the model use when fine-tuned for a task, or when plugged into a larger ecosystem/app -->",other
"{{ downstream_use | default(""[More Information Needed]"", true)}}",other
### Out-of-Scope Use,other
"<!-- This section addresses misuse, malicious use, and uses that the model will not work well for. -->",other
"{{ out_of_scope_use | default(""[More Information Needed]"", true)}}",other
"## Bias, Risks, and Limitations",other
<!-- This section is meant to convey both technical and sociotechnical limitations. -->,other
"{{ bias_risks_limitations | default(""[More Information Needed]"", true)}}",other
### Recommendations,other
"<!-- This section is meant to convey recommendations with respect to the bias, risk, and technical limitations. -->",other
"{{ bias_recommendations | default(""Users (both direct and downstream) should be made aware of the risks, biases and limitations of the model. More information needed for further recommendations."", true)}}",other
## How to Get Started with the Model,other
Use the code below to get started with the model.,other
"{{ get_started_code | default(""[More Information Needed]"", true)}}",other
## Training Details,other
### Training Data,other
"<!-- This should link to a Dataset Card, perhaps with a short stub of information on what the training data is all about as well as documentation related to data pre-processing or additional filtering. -->",other
"{{ training_data | default(""[More Information Needed]"", true)}}",other
### Training Procedure,other
<!-- This relates heavily to the Technical Specifications. Content here should link to that section when it is relevant to the training procedure. -->,other
#### Preprocessing [optional],other
"{{ preprocessing | default(""[More Information Needed]"", true)}}",other
#### Training Hyperparameters,other
"- **Training regime:** {{ training_regime | default(""[More Information Needed]"", true)}} <!--fp32, fp16 mixed precision, bf16 mixed precision, bf16 non-mixed precision, fp16 non-mixed precision, fp8 mixed precision -->",other
"#### Speeds, Sizes, Times [optional]",other
"<!-- This section provides information about throughput, start/end time, checkpoint size if relevant, etc. -->",other
"{{ speeds_sizes_times | default(""[More Information Needed]"", true)}}",other
## Evaluation,other
<!-- This section describes the evaluation protocols and provides the results. -->,other
"### Testing Data, Factors & Metrics",other
#### Testing Data,other
<!-- This should link to a Dataset Card if possible. -->,other
"{{ testing_data | default(""[More Information Needed]"", true)}}",other
#### Factors,other
"<!-- These are the things the evaluation is disaggregating by, e.g., subpopulations or domains. -->",other
"{{ testing_factors | default(""[More Information Needed]"", true)}}",other
#### Metrics,other
"<!-- These are the evaluation metrics being used, ideally with a description of why. -->",other
"{{ testing_metrics | default(""[More Information Needed]"", true)}}",other
### Results,other
"{{ results | default(""[More Information Needed]"", true)}}",other
#### Summary,other
"{{ results_summary | default("""", true) }}",other
## Model Examination [optional],other
<!-- Relevant interpretability work for the model goes here -->,other
"{{ model_examination | default(""[More Information Needed]"", true)}}",other
## Environmental Impact,other
"<!-- Total emissions (in grams of CO2eq) and additional considerations, such as electricity usage, go here. Edit the suggested text below accordingly -->",other
Carbon emissions can be estimated using the [Machine Learning Impact calculator](https://mlco2.github.io/impact#compute) presented in [Lacoste et al. (2019)](https://arxiv.org/abs/1910.09700).,other
"- **Hardware Type:** {{ hardware_type | default(""[More Information Needed]"", true)}}
- **Hours used:** {{ hours_used | default(""[More Information Needed]"", true)}}
- **Cloud Provider:** {{ cloud_provider | default(""[More Information Needed]"", true)}}
- **Compute Region:** {{ cloud_region | default(""[More Information Needed]"", true)}}
- **Carbon Emitted:** {{ co2_emitted | default(""[More Information Needed]"", true)}}",other
## Technical Specifications [optional],other
### Model Architecture and Objective,other
"{{ model_specs | default(""[More Information Needed]"", true)}}",other
### Compute Infrastructure,other
"{{ compute_infrastructure | default(""[More Information Needed]"", true)}}",other
#### Hardware,other
"{{ hardware_requirements | default(""[More Information Needed]"", true)}}",other
#### Software,other
"{{ software | default(""[More Information Needed]"", true)}}",other
## Citation [optional],other
"<!-- If there is a paper or blog post introducing the model, the APA and Bibtex information for that should go in this section. -->",api_reference
**BibTeX:**,other
"{{ citation_bibtex | default(""[More Information Needed]"", true)}}",other
**APA:**,other
"{{ citation_apa | default(""[More Information Needed]"", true)}}",other
## Glossary [optional],other
"<!-- If relevant, include terms and calculations in this section that can help readers understand the model or model card. -->",other
"{{ glossary | default(""[More Information Needed]"", true)}}",other
## More Information [optional],other
"{{ more_information | default(""[More Information Needed]"", true)}}",other
## Model Card Authors [optional],other
"{{ model_card_authors | default(""[More Information Needed]"", true)}}",other
## Model Card Contact,other
"{{ model_card_contact | default(""[More Information Needed]"", true)}}",other
"# JupyterChart
This directory contains the JavaScript portion of the Altair `JupyterChart`. The `JupyterChart` is based on the [AnyWidget](https://anywidget.dev/) project.",other
"**This software is dual-licensed under the The University of Illinois/NCSA
Open Source License (NCSA) and The 3-Clause BSD License**",other
"# NCSA Open Source License
**Copyright (c) 2019 Kevin Sheppard. All rights reserved.**",other
"Developed by: Kevin Sheppard (<kevin.sheppard@economics.ox.ac.uk>,
<kevin.k.sheppard@gmail.com>)
[http://www.kevinsheppard.com](http://www.kevinsheppard.com)",other
"Permission is hereby granted, free of charge, to any person obtaining a copy of
this software and associated documentation files (the ""Software""), to deal with
the Software without restriction, including without limitation the rights to
use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies
of the Software, and to permit persons to whom the Software is furnished to do
so, subject to the following conditions:",other
"Redistributions of source code must retain the above copyright notice, this
list of conditions and the following disclaimers.",other
"Redistributions in binary form must reproduce the above copyright notice, this
list of conditions and the following disclaimers in the documentation and/or
other materials provided with the distribution.",other
"Neither the names of Kevin Sheppard, nor the names of any contributors may be
used to endorse or promote products derived from this Software without specific
prior written permission.",other
"**THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
CONTRIBUTORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS WITH
THE SOFTWARE.**",other
"# 3-Clause BSD License
**Copyright (c) 2019 Kevin Sheppard. All rights reserved.**",other
"Redistribution and use in source and binary forms, with or without
modification, are permitted provided that the following conditions are met:",other
"1. Redistributions of source code must retain the above copyright notice,
   this list of conditions and the following disclaimer.",other
"2. Redistributions in binary form must reproduce the above copyright notice,
   this list of conditions and the following disclaimer in the documentation
   and/or other materials provided with the distribution.",other
"3. Neither the name of the copyright holder nor the names of its contributors
   may be used to endorse or promote products derived from this software
   without specific prior written permission.",other
"**THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""
AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE
LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR
CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF
SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS
INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN
CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)
ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF
THE POSSIBILITY OF SUCH DAMAGE.**",other
# Components,other
"Many parts of this module have been derived from original sources, 
often the algorithm's designer. Component licenses are located with 
the component code.",other
Update this directory using maint_tools/vendor_array_api_compat.sh,api_reference
Update this directory using maint_tools/vendor_array_api_extra.sh,api_reference
"If you add a file to this directory, you **MUST** update
`torch/CMakeLists.txt` and add the file as a dependency to
the `add_custom_command` call.",other
"Copyright (C) 2010-2019 Max-Planck-Society
All rights reserved.",other
"Redistribution and use in source and binary forms, with or without modification,
are permitted provided that the following conditions are met:",other
"* Redistributions of source code must retain the above copyright notice, this
  list of conditions and the following disclaimer.
* Redistributions in binary form must reproduce the above copyright notice, this
  list of conditions and the following disclaimer in the documentation and/or
  other materials provided with the distribution.
* Neither the name of the copyright holder nor the names of its contributors may
  be used to endorse or promote products derived from this software without
  specific prior written permission.",other
"THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS"" AND
ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED
WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE
DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR
ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES
(INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;
LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON
ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
(INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.",other
MIT License,other
"Copyright (c) 2024, Marco Gorelli",other
"Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the ""Software""), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:",other
"The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.",other
"THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
SOFTWARE.",other
"<!---
  Licensed to the Apache Software Foundation (ASF) under one
  or more contributor license agreements.  See the NOTICE file
  distributed with this work for additional information
  regarding copyright ownership.  The ASF licenses this file
  to you under the Apache License, Version 2.0 (the
  ""License""); you may not use this file except in compliance
  with the License.  You may obtain a copy of the License at",other
http://www.apache.org/licenses/LICENSE-2.0,other
"Unless required by applicable law or agreed to in writing,
  software distributed under the License is distributed on an
  ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
  KIND, either express or implied.  See the License for the
  specific language governing permissions and limitations
  under the License.
-->",other
"The ORC and JSON files come from the `examples` directory in the Apache ORC
source tree:
https://github.com/apache/orc/tree/main/examples",other
"# Slide 1: RepoSage for AML3304
- Simulates AI-centric product pipeline
- DeepSeek + Bayesian Q&A + Transformer fallback",other
"# Slide 2: Project Goals
- Traceable, transparent AI engineering
- Real-world deployment to Hugging Face Spaces
- Continuous integration & sprint metrics",other
"# Slide 3: Team Roles
- PM, Dev, AI Lead, DevOps",other
"# Slide 1: Architecture Overview
- User ‚Üí RepoSage CLI ‚Üí index/query modules",usage
"# Slide 2: DeepSeek Module
- Embedding engine: all-MiniLM-L6-v2
- FAISS vector store",other
"# Slide 3: Bayesian Q&A Layer
- Cosine-likelihood softmax ‚Üí posterior",api_reference
"# Slide 4: Transformer Fallback
- Flan-T5 small for low-confidence queries",other
"# Slide 5: Deployment & CI/CD
- GitHub Actions ‚Üí HF Spaces deploy",other
